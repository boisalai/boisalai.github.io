"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[9041],{83265:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>d,toc:()=>l});var r=t(85893),i=t(11151);const s={sidebar_label:"Advanced Retrieval for AI with Chroma",sidebar_position:8},a="Advanced Retrieval for AI with Chroma",d={id:"courses/deeplearning-ai/p08-advanced-rag",title:"Advanced Retrieval for AI with Chroma",description:"2024-01-05",source:"@site/docs/courses/deeplearning-ai/p08-advanced-rag.md",sourceDirName:"courses/deeplearning-ai",slug:"/courses/deeplearning-ai/p08-advanced-rag",permalink:"/docs/courses/deeplearning-ai/p08-advanced-rag",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/deeplearning-ai/p08-advanced-rag.md",tags:[],version:"current",sidebarPosition:8,frontMatter:{sidebar_label:"Advanced Retrieval for AI with Chroma",sidebar_position:8},sidebar:"tutorialSidebar",previous:{title:"LangChain Chat with Your Data",permalink:"/docs/courses/deeplearning-ai/p07-chat-with-your-data/"},next:{title:"Retrieval Augmented Generation for Production with LangChain & LlamaIndex",permalink:"/docs/courses/activeloop/rag-for-production/"}},o={},l=[{value:"Overview of embeddings-based retrieval",id:"overview-of-embeddings-based-retrieval",level:2},{value:"Pitfalls of retrieval - when simple vector search fails!",id:"pitfalls-of-retrieval---when-simple-vector-search-fails",level:2},{value:"Relevancy and Distraction",id:"relevancy-and-distraction",level:3},{value:"Query Expansion",id:"query-expansion",level:2},{value:"Expansion with generated answers",id:"expansion-with-generated-answers",level:3},{value:"Expansion with multiple queries",id:"expansion-with-multiple-queries",level:3},{value:"Cross-encoder re-ranking",id:"cross-encoder-re-ranking",level:2},{value:"Re-ranking the long tail",id:"re-ranking-the-long-tail",level:3},{value:"Re-ranking with Query Expansion",id:"re-ranking-with-query-expansion",level:3},{value:"Embedding Adaptors",id:"embedding-adaptors",level:2},{value:"Creating a dataset",id:"creating-a-dataset",level:3},{value:"Setting up the model",id:"setting-up-the-model",level:3},{value:"Other techniques",id:"other-techniques",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"advanced-retrieval-for-ai-with-chroma",children:"Advanced Retrieval for AI with Chroma"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"2024-01-05"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"overview-of-embeddings-based-retrieval",children:"Overview of embeddings-based retrieval"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from helper_utils import word_wrap\n\nfrom pypdf import PdfReader\n\nreader = PdfReader("microsoft_annual_report_2022.pdf")\npdf_texts = [p.extract_text().strip() for p in reader.pages]\n\n# Filter the empty strings\npdf_texts = [text for text in pdf_texts if text]\n\n# Show the first page\nprint(word_wrap(pdf_texts[0]))\n'})}),"\n",(0,r.jsxs)(n.p,{children:["The Microsoft Annual Report 2022 is available ",(0,r.jsx)(n.a,{href:"https://www.annreports.com/microsoft/microsoft-ar-2022.pdf",children:"here"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.text_splitter import (\n  RecursiveCharacterTextSplitter,\n  SentenceTransformersTokenTextSplitter\n)\n\ncharacter_splitter = RecursiveCharacterTextSplitter(\n    separators=["\\n\\n", "\\n", ". ", " ", ""],\n    chunk_size=1000,\n    chunk_overlap=0\n)\ncharacter_split_texts = character_splitter.split_text(\'\\n\\n\'.join(pdf_texts))\n\nprint(word_wrap(character_split_texts[10]))\nprint(f"\\nTotal chunks: {len(character_split_texts)}")\n# Total chunks: 347.\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'token_splitter = SentenceTransformersTokenTextSplitter(\n  chunk_overlap=0, tokens_per_chunk=256)\n\ntoken_split_texts = []\nfor text in character_split_texts:\n    token_split_texts += token_splitter.split_text(text)\n\nprint(word_wrap(token_split_texts[10]))\nprint(f"\\nTotal chunks: {len(token_split_texts)}")\n# Total chunks: 349.\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import chromadb\nfrom chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n\nembedding_function = SentenceTransformerEmbeddingFunction()\nprint(embedding_function([token_split_texts[10]]))\n"})}),"\n",(0,r.jsxs)(n.p,{children:["See ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1908.10084",children:"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"})," for more details."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'chroma_client = chromadb.Client()\nchroma_collection = chroma_client.create_collection(\n  "microsoft_annual_report_2022", embedding_function=embedding_function)\n\nids = [str(i) for i in range(len(token_split_texts))]\n\nchroma_collection.add(ids=ids, documents=token_split_texts)\nchroma_collection.count()\n# 349\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"query = \"What was the total revenue?\"\n\nresults = chroma_collection.query(query_texts=[query], n_results=5)\nretrieved_documents = results['documents'][0]\n\nfor document in retrieved_documents:\n    print(word_wrap(document))\n    print('\\n')\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import os\nimport openai\nfrom openai import OpenAI\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nopenai.api_key = os.environ[\'OPENAI_API_KEY\']\n\nopenai_client = OpenAI()\n\ndef rag(query, retrieved_documents, model="gpt-3.5-turbo"):\n    information = "\\n\\n".join(retrieved_documents)\n\n    messages = [\n        {\n            "role": "system",\n            "content": "You are a helpful expert financial research assistant. Your users are asking questions about information contained in an annual report."\n            "You will be shown the user\'s question, and the relevant information from the annual report. Answer the user\'s question using only this information."\n        },\n        {"role": "user", "content": f"Question: {query}. \\n Information: {information}"}\n    ]\n    \n    response = openai_client.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n    content = response.choices[0].message.content\n    return content\n\noutput = rag(query=query, retrieved_documents=retrieved_documents)\nprint(word_wrap(output))\n# The total revenue for the year ended June 30, 2022 was $198,270 million.\n'})}),"\n",(0,r.jsx)(n.h2,{id:"pitfalls-of-retrieval---when-simple-vector-search-fails",children:"Pitfalls of retrieval - when simple vector search fails!"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from helper_utils import load_chroma, word_wrap\nfrom chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n\nembedding_function = SentenceTransformerEmbeddingFunction()\n\nchroma_collection = load_chroma(filename='microsoft_annual_report_2022.pdf', \n    collection_name='microsoft_annual_report_2022', \n    embedding_function=embedding_function)\nchroma_collection.count()\n# 349\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import umap\nimport numpy as np\nfrom tqdm import tqdm\n\nembeddings = chroma_collection.get(include=['embeddings'])['embeddings']\numap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)\n\ndef project_embeddings(embeddings, umap_transform):\n    umap_embeddings = np.empty((len(embeddings),2))\n    for i, embedding in enumerate(tqdm(embeddings)): \n        umap_embeddings[i] = umap_transform.transform([embedding])\n    return umap_embeddings\n\nprojected_dataset_embeddings = project_embeddings(embeddings, umap_transform)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can\nbe used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction.\nSee ",(0,r.jsx)(n.a,{href:"https://umap-learn.readthedocs.io/en/latest/",children:"UMAP"})," for more details."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import matplotlib.pyplot as plt\n\nplt.figure()\nplt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10)\nplt.gca().set_aspect('equal', 'datalim')\nplt.title('Projected Embeddings')\nplt.axis('off')\n"})}),"\n",(0,r.jsx)(n.h3,{id:"relevancy-and-distraction",children:"Relevancy and Distraction"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"query = \"What is the total revenue?\"\n\nresults = chroma_collection.query(query_texts=query, n_results=5, include=['documents', 'embeddings'])\n\nretrieved_documents = results['documents'][0]\n\nfor document in results['documents'][0]:\n    print(word_wrap(document))\n    print('')\n\nquery_embedding = embedding_function([query])[0]\nretrieved_embeddings = results['embeddings'][0]\n\nprojected_query_embedding = project_embeddings([query_embedding], umap_transform)\nprojected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)  \n\n# Plot the projected query and retrieved documents in the embedding space\nplt.figure()\nplt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\nplt.scatter(projected_query_embedding[:, 0], projected_query_embedding[:, 1], s=150, marker='X', color='r')\nplt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\n\nplt.gca().set_aspect('equal', 'datalim')\nplt.title(f'{query}')\nplt.axis('off')\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"query = \"What is the strategy around artificial intelligence (AI) ?\"\nresults = chroma_collection.query(query_texts=query, n_results=5, \n    include=['documents', 'embeddings'])\n\nretrieved_documents = results['documents'][0]\n\nfor document in results['documents'][0]:\n    print(word_wrap(document))\n    print('')\n\nquery_embedding = embedding_function([query])[0]\nretrieved_embeddings = results['embeddings'][0]\n\nprojected_query_embedding = project_embeddings([query_embedding], umap_transform)\nprojected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)\n\n# Plot the projected query and retrieved documents in the embedding space\nplt.figure()\nplt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\nplt.scatter(projected_query_embedding[:, 0], projected_query_embedding[:, 1], s=150, marker='X', color='r')\nplt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\n\nplt.gca().set_aspect('equal', 'datalim')\nplt.title(f'{query}')\nplt.axis('off')\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"query = \"What has been the investment in research and development?\"\nresults = chroma_collection.query(query_texts=query, n_results=5, \n    include=['documents', 'embeddings'])\n\nretrieved_documents = results['documents'][0]\n\nfor document in results['documents'][0]:\n    print(word_wrap(document))\n    print('')\n\nquery_embedding = embedding_function([query])[0]\nretrieved_embeddings = results['embeddings'][0]\n\nprojected_query_embedding = project_embeddings([query_embedding], umap_transform)\nprojected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)\n\n# Plot the projected query and retrieved documents in the embedding space\nplt.figure()\nplt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], \n    s=10, color='gray')\nplt.scatter(projected_query_embedding[:, 0], projected_query_embedding[:, 1], \n    s=150, marker='X', color='r')\nplt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], \n    s=100, facecolors='none', edgecolors='g')\n\nplt.gca().set_aspect('equal', 'datalim')\nplt.title(f'{query}')\nplt.axis('off')\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"query = \"What has Michael Jordan done for us lately?\"\nresults = chroma_collection.query(query_texts=query, n_results=5, \n    include=['documents', 'embeddings'])\n\nretrieved_documents = results['documents'][0]\n\nfor document in results['documents'][0]:\n    print(word_wrap(document))\n    print('')\n\nquery_embedding = embedding_function([query])[0]\nretrieved_embeddings = results['embeddings'][0]\n\nprojected_query_embedding = project_embeddings([query_embedding], umap_transform)\nprojected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)\n\n# Plot the projected query and retrieved documents in the embedding space\nplt.figure()\nplt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\nplt.scatter(projected_query_embedding[:, 0], projected_query_embedding[:, 1], s=150, marker='X', color='r')\nplt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\n\nplt.gca().set_aspect('equal', 'datalim')\nplt.title(f'{query}')\nplt.axis('off')\n"})}),"\n",(0,r.jsx)(n.h2,{id:"query-expansion",children:"Query Expansion"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"expansion",src:t(29061).Z+"",width:"1256",height:"1310"})}),"\n",(0,r.jsx)(n.p,{children:"Loading and Embedding a Document Collection:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Uses a custom utility (",(0,r.jsx)(n.code,{children:"load_chroma"}),") to load a PDF document (Microsoft's 2022 annual report) into a ",(0,r.jsx)(n.code,{children:"Chroma"})," collection."]}),"\n",(0,r.jsx)(n.li,{children:"Embeds the documents using a Sentence Transformer model, which converts text into numerical vectors that capture semantic meaning."}),"\n",(0,r.jsx)(n.li,{children:"Counts the number of documents in the collection."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from helper_utils import load_chroma, word_wrap, project_embeddings\nfrom chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n\nembedding_function = SentenceTransformerEmbeddingFunction()\n\nchroma_collection = load_chroma(filename='microsoft_annual_report_2022.pdf', \n    collection_name='microsoft_annual_report_2022', \n    embedding_function=embedding_function)\nchroma_collection.count()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Setting Up OpenAI Client:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Loads environment variables, including the OpenAI API key, from a ",(0,r.jsx)(n.code,{children:".env"})," file."]}),"\n",(0,r.jsx)(n.li,{children:"Initializes the OpenAI client for interacting with the GPT-3.5 API."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\nimport openai\nfrom openai import OpenAI\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nopenai_client = OpenAI()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Dimensionality Reduction with UMAP:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Retrieves embeddings from the loaded Chroma collection."}),"\n",(0,r.jsx)(n.li,{children:"Applies UMAP (Uniform Manifold Approximation and Projection) to reduce the dimensionality of the embeddings while preserving their structure. This is typically done to visualize high-dimensional data in 2D or 3D."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import umap\n\nembeddings = chroma_collection.get(include=['embeddings'])['embeddings']\numap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)\nprojected_dataset_embeddings = project_embeddings(embeddings, umap_transform)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"expansion-with-generated-answers",children:"Expansion with generated answers"}),"\n",(0,r.jsxs)(n.p,{children:["Query expansion is a widely used technique to improve the recall of search systems. In this paper, we\npropose an approach to query expansion that leverages the generative abilities of Large Language Models\n(LLMs). See ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2305.03653",children:"Query Expansion by Prompting Large Language Models"})," for more details."]}),"\n",(0,r.jsx)(n.p,{children:"Query Expansion Using GPT-3.5:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Describes a method to augment a search query using GPT-3.5's language generation capabilities."}),"\n",(0,r.jsx)(n.li,{children:"The function sends a prompt to GPT-3.5, asking it to act as a financial research assistant and provide an example answer to a given question."}),"\n",(0,r.jsx)(n.li,{children:"This augmented query (original query + generated answer) is expected to improve the search results in a document retrieval system."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def augment_query_generated(query, model="gpt-3.5-turbo"):\n    messages = [\n        {\n            "role": "system",\n            "content": "You are a helpful expert financial research assistant. Provide an example answer to the given question, that might be found in a document like an annual report. "\n        },\n        {"role": "user", "content": query}\n    ] \n\n    response = openai_client.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n    content = response.choices[0].message.content\n    return content\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'original_query = "Was there significant turnover in the executive team?"\nhypothetical_answer = augment_query_generated(original_query)\n\njoint_query = f"{original_query} {hypothetical_answer}"\nprint(word_wrap(joint_query))\n'})}),"\n",(0,r.jsx)(n.p,{children:"Searching the Document Collection:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Defines the original query about turnover in the executive team."}),"\n",(0,r.jsx)(n.li,{children:"Augments the query using the previously defined function."}),"\n",(0,r.jsx)(n.li,{children:"Performs a search on the Chroma collection with the augmented query and retrieves relevant documents and embeddings."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"results = chroma_collection.query(query_texts=joint_query, n_results=5, \n    include=['documents', 'embeddings'])\nretrieved_documents = results['documents'][0]\n\nfor doc in retrieved_documents:\n    print(word_wrap(doc))\n    print('')\n"})}),"\n",(0,r.jsx)(n.p,{children:"Visualization of Retrieved Documents in Embedding Space:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Computes UMAP projections for the original query, the augmented query, and the retrieved document embeddings."}),"\n",(0,r.jsx)(n.li,{children:"Uses Matplotlib to visualize these projections. The original and augmented queries are marked distinctly, and the retrieved documents are highlighted. This visual representation helps in understanding how closely the retrieved documents are related to the query in the semantic space."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"retrieved_embeddings = results['embeddings'][0]\noriginal_query_embedding = embedding_function([original_query])\naugmented_query_embedding = embedding_function([joint_query])\n\nprojected_original_query_embedding = project_embeddings(original_query_embedding, umap_transform)\nprojected_augmented_query_embedding = project_embeddings(augmented_query_embedding, umap_transform)\nprojected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import matplotlib.pyplot as plt\n\n# Plot the projected query and retrieved documents in the embedding space\nplt.figure()\nplt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\nplt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\nplt.scatter(projected_original_query_embedding[:, 0], projected_original_query_embedding[:, 1], s=150, marker='X', color='r')\nplt.scatter(projected_augmented_query_embedding[:, 0], projected_augmented_query_embedding[:, 1], s=150, marker='X', color='orange')\n\nplt.gca().set_aspect('equal', 'datalim')\nplt.title(f'{original_query}')\nplt.axis('off')\n"})}),"\n",(0,r.jsx)(n.h3,{id:"expansion-with-multiple-queries",children:"Expansion with multiple queries"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"multiple",src:t(71997).Z+"",width:"1278",height:"1316"})}),"\n",(0,r.jsx)(n.p,{children:"This code is an extension of the previous example, focusing on augmenting a single query into multiple related\nqueries to improve document retrieval from a collection. The goal is to generate a variety of questions related\nto the original query to explore different aspects of the topic and potentially retrieve a broader\nrange of relevant documents."}),"\n",(0,r.jsx)(n.p,{children:"Augmenting the Original Query into Multiple Related Queries:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Defines a function ",(0,r.jsx)(n.code,{children:"augment_multiple_query"})," to generate additional related questions based on the original query."]}),"\n",(0,r.jsx)(n.li,{children:"Sends a prompt to the GPT-3.5 API, instructing it to act as a financial research assistant and suggest up to five additional related questions."}),"\n",(0,r.jsx)(n.li,{children:"The generated questions are short, cover various aspects of the topic, and are related to the original query."}),"\n",(0,r.jsx)(n.li,{children:"The responses from the model are split into individual questions and returned."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def augment_multiple_query(query, model="gpt-3.5-turbo"):\n    messages = [\n        {\n            "role": "system",\n            "content": "You are a helpful expert financial research assistant. Your users are asking questions about an annual report. "\n            "Suggest up to five additional related questions to help them find the information they need, for the provided question. "\n            "Suggest only short questions without compound sentences. Suggest a variety of questions that cover different aspects of the topic."\n            "Make sure they are complete questions, and that they are related to the original question."\n            "Output one question per line. Do not number the questions."\n        },\n        {"role": "user", "content": query}\n    ]\n\n    response = openai_client.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n    content = response.choices[0].message.content\n    content = content.split("\\n")\n    return content\n'})}),"\n",(0,r.jsx)(n.p,{children:"Retrieving Documents for Each Query:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Sets the original query and uses the ",(0,r.jsx)(n.code,{children:"augment_multiple_query"})," function to generate additional related queries."]}),"\n",(0,r.jsx)(n.li,{children:"Prints each augmented query."}),"\n",(0,r.jsxs)(n.li,{children:["Combines the original and augmented queries and performs a document search for each using the ",(0,r.jsx)(n.code,{children:"chroma_collection.query"})," method."]}),"\n",(0,r.jsx)(n.li,{children:"Deduplicates the retrieved documents to ensure each unique document is only represented once."}),"\n",(0,r.jsx)(n.li,{children:"Iterates over the queries and their respective retrieved documents, printing them out for review."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'original_query = "What were the most important factors that contributed to increases in revenue?"\naugmented_queries = augment_multiple_query(original_query)\n\nfor query in augmented_queries:\n    print(query)\n# What were the most important factors that contributed to decreases in revenue?\n# What were the sources of revenue for the company?\n# How were sales and revenue distributed across different product lines or segments?\n# Were there any changes in pricing strategies that impacted revenue?\n# Did the company acquire any new customers or enter new markets that drove revenue growth?\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"queries = [original_query] + augmented_queries\nresults = chroma_collection.query(query_texts=queries, n_results=5, include=['documents', 'embeddings'])\n\nretrieved_documents = results['documents']\n\n# Deduplicate the retrieved documents\nunique_documents = set()\nfor documents in retrieved_documents:\n    for document in documents:\n        unique_documents.add(document)\n\nfor i, documents in enumerate(retrieved_documents):\n    print(f\"Query: {queries[i]}\")\n    print('')\n    print(\"Results:\")\n    for doc in documents:\n        print(word_wrap(doc))\n        print('')\n    print('-'*100)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Embedding Queries for Visualization:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Computes embeddings for the original and augmented queries using the earlier defined embedding function (likely a Sentence Transformer)."}),"\n",(0,r.jsx)(n.li,{children:"Applies UMAP to project the high-dimensional embeddings into a lower-dimensional space for visualization."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"original_query_embedding = embedding_function([original_query])\naugmented_query_embeddings = embedding_function(augmented_queries)\n\nproject_original_query = project_embeddings(original_query_embedding, umap_transform)\nproject_augmented_queries = project_embeddings(augmented_query_embeddings, umap_transform)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Embedding and Preparing Retrieved Documents for Visualization:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Extracts and flattens the embeddings of the retrieved documents."}),"\n",(0,r.jsx)(n.li,{children:"Projects these embeddings into the lower-dimensional space using UMAP."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"result_embeddings = results['embeddings']\nresult_embeddings = [item for sublist in result_embeddings for item in sublist]\nprojected_result_embeddings = project_embeddings(result_embeddings, umap_transform)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Visualization:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Uses Matplotlib to create a scatter plot visualizing the projected embeddings."}),"\n",(0,r.jsx)(n.li,{children:"Plots the entire dataset embeddings as a grey background to provide context."}),"\n",(0,r.jsx)(n.li,{children:"Marks the projected augmented queries with orange crosses and the projected original query with a red cross."}),"\n",(0,r.jsx)(n.li,{children:"Highlights the projected embeddings of the retrieved documents with green circles."}),"\n",(0,r.jsx)(n.li,{children:"Sets up the plot aesthetics and displays the title without an axis for a cleaner look."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import matplotlib.pyplot as plt\n\nplt.figure()\nplt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\nplt.scatter(project_augmented_queries[:, 0], project_augmented_queries[:, 1], s=150, marker='X', color='orange')\nplt.scatter(projected_result_embeddings[:, 0], projected_result_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\nplt.scatter(project_original_query[:, 0], project_original_query[:, 1], s=150, marker='X', color='r')\n\nplt.gca().set_aspect('equal', 'datalim')\nplt.title(f'{original_query}')\nplt.axis('off')\n"})}),"\n",(0,r.jsx)(n.p,{children:"In summary, this code aims to enhance the search and retrieval process by generating multiple related queries\nfrom a single original query. It then retrieves documents for each query, visualizes the relationship between\nqueries and documents in an embedding space, and aims to provide a more comprehensive understanding of the topic\nat hand. This approach is useful in scenarios where the original query might be too broad or vague, and\na more nuanced exploration of the topic is beneficial."}),"\n",(0,r.jsx)(n.h2,{id:"cross-encoder-re-ranking",children:"Cross-encoder re-ranking"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"reranking",src:t(29537).Z+"",width:"1318",height:"1404"})}),"\n",(0,r.jsx)(n.p,{children:'The image depicts a flowchart explaining a "ReRanking" process in the context of querying a vector database (Vector DB). Here are the steps described:'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Query"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"An initial query is sent to the vector database."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vector DB"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The vector database is queried, and a request for additional results is made."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Query Results"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The query results are returned, typically in an ordered list, symbolized here by documents numbered from 1 to n."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ReRank"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A re-ranking process is applied to the initial result set to rearrange the order based on relevance."}),"\n",(0,r.jsx)(n.li,{children:"The aim is to ensure the most relevant results get the highest rank."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Select the Top Ranking Results"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"After re-ranking, the top-ranking results are selected."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LLM"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"It is implied that the selected results are then processed or further analyzed by a Large Language Model (LLM),\nalthough the image does not specify exactly the role of the LLM in this process."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This diagram illustrates a common step in modern information retrieval systems, where re-ranking is used to improve the\nrelevance of results before presenting them to the user or submitting them for further natural language processing analysis."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from helper_utils import load_chroma, word_wrap, project_embeddings\nfrom chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\nimport numpy as np\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"embedding_function = SentenceTransformerEmbeddingFunction()\n\nchroma_collection = load_chroma(filename='microsoft_annual_report_2022.pdf', \n    collection_name='microsoft_annual_report_2022', \n    embedding_function=embedding_function)\nchroma_collection.count()\n# 349\n"})}),"\n",(0,r.jsx)(n.h3,{id:"re-ranking-the-long-tail",children:"Re-ranking the long tail"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"query = \"What has been the investment in research and development?\"\nresults = chroma_collection.query(query_texts=query, n_results=10, \n    include=['documents', 'embeddings'])\n\nretrieved_documents = results['documents'][0]\n\nfor document in results['documents'][0]:\n    print(word_wrap(document))\n    print('')\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"cross-encoder",src:t(47046).Z+"",width:"1360",height:"1374"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"cross-reranking",src:t(57287).Z+"",width:"1204",height:"1156"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from sentence_transformers import CrossEncoder\n\ncross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\npairs = [[query, doc] for doc in retrieved_documents]\nscores = cross_encoder.predict(pairs)\nprint(\"Scores:\")\nfor score in scores:\n    print(score)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-txt",children:"Scores:\n0.98693466\n2.644579\n-0.26802942\n-10.73159\n-7.7066045\n-5.6469955\n-4.297035\n-10.933233\n-7.0384283\n-7.3246956\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'print("New Ordering:")\nfor o in np.argsort(scores)[::-1]:\n    print(o+1)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-txt",children:"New Ordering:\n2\n1\n3\n7\n6\n9\n10\n5\n4\n8\n"})}),"\n",(0,r.jsx)(n.h3,{id:"re-ranking-with-query-expansion",children:"Re-ranking with Query Expansion"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'original_query = "What were the most important factors that contributed to increases in revenue?"\ngenerated_queries = [\n    "What were the major drivers of revenue growth?",\n    "Were there any new product launches that contributed to the increase in revenue?",\n    "Did any changes in pricing or promotions impact the revenue growth?",\n    "What were the key market trends that facilitated the increase in revenue?",\n    "Did any acquisitions or partnerships contribute to the revenue growth?"\n]\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"queries = [original_query] + generated_queries\n\nresults = chroma_collection.query(query_texts=queries, n_results=10, \n    include=['documents', 'embeddings'])\nretrieved_documents = results['documents']\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Deduplicate the retrieved documents\nunique_documents = set()\nfor documents in retrieved_documents:\n    for document in documents:\n        unique_documents.add(document)\n\nunique_documents = list(unique_documents)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"pairs = []\nfor doc in unique_documents:\n    pairs.append([original_query, doc])\n\nscores = cross_encoder.predict(pairs)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'print("Scores:")\nfor score in scores:\n    print(score)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'print("New Ordering:")\nfor o in np.argsort(scores)[::-1]:\n    print(o)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"embedding-adaptors",children:"Embedding Adaptors"}),"\n",(0,r.jsx)(n.p,{children:"The text describes a complete workflow for improving the relevance of query results using embedding adaptors. The process includes generating queries, retrieving and evaluating documents, as well as training an embedding adaptor to enhance the relevance of query results. The primary aim is to interact with a language model, manipulate embeddings, train a machine learning model, and visualize the outcomes."}),"\n",(0,r.jsx)(n.p,{children:"Training a model, in this case, an embedding adaptor, is crucial for several reasons. Firstly, it allows adjusting the query embeddings to better match the embeddings of relevant documents. This can significantly improve the accuracy of document retrieval by making the queries more representative of the types of documents the user wishes to retrieve. Secondly, adapting embeddings can help overcome the limitations of pre-trained embeddings that might not be perfectly aligned with the specifics or the domain of the targeted dataset. Lastly, visualizing the adapted embeddings and comparing them with the original embeddings provides a deeper understanding of the effectiveness of the adaptation and how it modifies the embedding space to better suit the query's needs."}),"\n",(0,r.jsx)(n.p,{children:"The code below is a complete workflow for generating queries, retrieving and evaluating documents,\nand training an embedding adaptor to improve the relevance of query results. It includes steps for\ninteracting with a language model, handling embeddings, training a machine learning model,\nand visualizing the results."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"embedding-adaptor",src:t(18483).Z+"",width:"1142",height:"860"})}),"\n",(0,r.jsx)(n.p,{children:"Loading and counting documents:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The code loads a collection of documents from a PDF file (Microsoft's 2022 annual report) into a ",(0,r.jsx)(n.code,{children:"chroma_collection"})," object."]}),"\n",(0,r.jsx)(n.li,{children:"It counts the number of documents in the collection."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from helper_utils import load_chroma, word_wrap, project_embeddings\nfrom chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\nimport numpy as np\nimport umap\nfrom tqdm import tqdm\n\nimport torch\n\nembedding_function = SentenceTransformerEmbeddingFunction()\n\nchroma_collection = load_chroma(filename='microsoft_annual_report_2022.pdf', \n    collection_name='microsoft_annual_report_2022', \n    embedding_function=embedding_function)\nchroma_collection.count()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Embedding Documents and Dimensionality Reduction:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"It retrieves embeddings for the documents and then applies UMAP (Uniform Manifold Approximation and Projection) to reduce the dimensionality of these embeddings for visualization purposes."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"embeddings = chroma_collection.get(include=['embeddings'])['embeddings']\numap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)\nprojected_dataset_embeddings = project_embeddings(embeddings, umap_transform)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Setting Up OpenAI Client:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The code loads the OpenAI API key from an environment variable and initializes the OpenAI client."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\nimport openai\nfrom openai import OpenAI\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nopenai_client = OpenAI()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"creating-a-dataset",children:"Creating a dataset"}),"\n",(0,r.jsx)(n.p,{children:"Generating Queries with GPT-3.5:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"It defines a function to generate a set of questions important for analyzing an annual report using GPT-3.5."}),"\n",(0,r.jsx)(n.li,{children:"The generated questions are split by newline and returned as a list."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def generate_queries(model="gpt-3.5-turbo"):\n    messages = [\n        {\n            "role": "system",\n            "content": "You are a helpful expert financial research assistant. You help users analyze financial statements to better understand companies. "\n            "Suggest 10 to 15 short questions that are important to ask when analyzing an annual report. "\n            "Do not output any compound questions (questions with multiple sentences or conjunctions)."\n            "Output each question on a separate line divided by a newline."\n        },\n    ]\n\n    response = openai_client.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n    content = response.choices[0].message.content\n    content = content.split("\\n")\n    return content\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"generated_queries = generate_queries()\nfor query in generated_queries:\n    print(query)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-txt",children:"1. What is the company's overall financial performance in terms of revenue and profitability?\n2. How has the company's revenue and profit margin trended over the past few years?\n3. What are the key drivers of the company's revenue growth or decline?\n4. What are the company's total assets, liabilities, and shareholders' equity?\n5. How has the company's debt level changed over time?\n6. What is the company's cash flow from operating activities?\n7. What are the company's major sources of revenue and how do they contribute to the overall business?\n8. How does the company manage its inventory and accounts receivable?\n9. What are the company's major expenses and how do they impact profitability?\n10. How does the company allocate its capital expenditures?\n11. What are the company's major risks and how is management addressing them?\n12. What is the company's dividend policy and history?\n13. Does the company have any legal or regulatory issues that could impact its financial performance?\n14. How does the company compare to its competitors in terms of financial performance and market position?\n15. What is the company's strategy for future growth and how does it plan to achieve its goals?\n"})}),"\n",(0,r.jsx)(n.p,{children:"Querying the document collection:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["It uses the generated queries to retrieve relevant documents from the ",(0,r.jsx)(n.code,{children:"chroma_collection"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"results = chroma_collection.query(query_texts=generated_queries, n_results=10, include=['documents', 'embeddings'])\nretrieved_documents = results['documents']\n"})}),"\n",(0,r.jsx)(n.p,{children:"Evaluating Results:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A function is defined to use GPT-3.5 to evaluate the relevance of a statement (document content) to a given query, returning a binary relevance score."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def evaluate_results(query, statement, model="gpt-3.5-turbo"):\n    messages = [\n    {\n        "role": "system",\n        "content": "You are a helpful expert financial research assistant. You help users analyze financial statements to better understand companies. "\n        "For the given query, evaluate whether the following satement is relevant."\n        "Output only \'yes\' or \'no\'."\n    },\n    {\n        "role": "user",\n        "content": f"Query: {query}, Statement: {statement}"\n    }\n    ]\n\n    response = openai_client.chat.completions.create(\n        model=model,\n        messages=messages,\n        max_tokens=1\n    )\n    content = response.choices[0].message.content\n    if content == "yes":\n        return 1\n    return -1\n'})}),"\n",(0,r.jsx)(n.p,{children:"Creating embeddings for queries:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The code computes embeddings for the generated queries using the same embedding function as the documents."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"retrieved_embeddings = results['embeddings']\nquery_embeddings = embedding_function(generated_queries)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Preparing Embedding Adaptors:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The code initializes lists to store the embeddings of queries, documents, and their relevance labels (from the evaluation function)."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"adapter_query_embeddings = []\nadapter_doc_embeddings = []\nadapter_labels = []\n"})}),"\n",(0,r.jsx)(n.p,{children:"Populating adaptor datasets:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"It populates the embedding adaptor lists with the query embeddings, document embeddings, and relevance scores for each query-document pair."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"for q, query in enumerate(tqdm(generated_queries)):\n    for d, document in enumerate(retrieved_documents[q]):\n        adapter_query_embeddings.append(query_embeddings[q])\n        adapter_doc_embeddings.append(retrieved_embeddings[q][d])\n        adapter_labels.append(evaluate_results(query, document))\n\nlen(adapter_labels)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Creating a tensor dataset:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The lists of query and document embeddings, along with labels, are converted into PyTorch tensors and then into a TensorDataset for training a machine learning model."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"adapter_query_embeddings = torch.Tensor(np.array(adapter_query_embeddings))\nadapter_doc_embeddings = torch.Tensor(np.array(adapter_doc_embeddings))\nadapter_labels = torch.Tensor(np.expand_dims(np.array(adapter_labels),1))\n\ndataset = torch.utils.data.TensorDataset(adapter_query_embeddings, adapter_doc_embeddings, adapter_labels)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"setting-up-the-model",children:"Setting up the model"}),"\n",(0,r.jsx)(n.p,{children:"Setting up the model and loss function:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A model function is defined to update query embeddings using an adaptor matrix and then calculate cosine similarity with document embeddings."}),"\n",(0,r.jsx)(n.li,{children:"A mean squared error (MSE) loss function is defined to evaluate the performance of the adaptor matrix."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def model(query_embedding, document_embedding, adaptor_matrix):\n    updated_query_embedding = torch.matmul(adaptor_matrix, query_embedding)\n    return torch.cosine_similarity(updated_query_embedding, document_embedding, dim=0)\n\ndef mse_loss(query_embedding, document_embedding, adaptor_matrix, label):\n    return torch.nn.MSELoss()(model(query_embedding, document_embedding, adaptor_matrix), label)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Training the adaptor matrix:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The adaptor matrix is initialized randomly and trained over a number of epochs to minimize the MSE loss."}),"\n",(0,r.jsx)(n.li,{children:"The best adaptor matrix is saved based on the lowest loss achieved."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Initialize the adaptor matrix\nmat_size = len(adapter_query_embeddings[0])\nadapter_matrix = torch.randn(mat_size, mat_size, requires_grad=True)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"min_loss = float('inf')\nbest_matrix = None\n\nfor epoch in tqdm(range(100)):\n    for query_embedding, document_embedding, label in dataset:\n        loss = mse_loss(query_embedding, document_embedding, adapter_matrix, label)\n\n        if loss < min_loss:\n            min_loss = loss\n            best_matrix = adapter_matrix.clone().detach().numpy()\n\n        loss.backward()\n        with torch.no_grad():\n            adapter_matrix -= 0.01 * adapter_matrix.grad\n            adapter_matrix.grad.zero_()\n\nprint(f\"Best loss: {min_loss.detach().numpy()}\")\n# Best loss: 0.53941\n"})}),"\n",(0,r.jsx)(n.p,{children:"Testing the adaptor matrix:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The code applies the trained adaptor matrix to a test vector and visualizes the result using a bar plot."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"test_vector = torch.ones((mat_size,1))\nscaled_vector = np.matmul(best_matrix, test_vector).numpy()\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import matplotlib.pyplot as plt\nplt.bar(range(len(scaled_vector)), scaled_vector.flatten())\nplt.show()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Visualizing Adapted Query Embeddings:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The best adaptor matrix is applied to the query embeddings to create adapted query embeddings."}),"\n",(0,r.jsx)(n.li,{children:"Both original and adapted query embeddings are projected using UMAP for visualization."}),"\n",(0,r.jsx)(n.li,{children:"A scatter plot is generated to show the original and adapted query embeddings in the reduced dimensional space, allowing for visual comparison of their distributions."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"query_embeddings = embedding_function(generated_queries)\nadapted_query_embeddings = np.matmul(best_matrix, np.array(query_embeddings).T).T\n\nprojected_query_embeddings = project_embeddings(query_embeddings, umap_transform)\nprojected_adapted_query_embeddings = project_embeddings(adapted_query_embeddings, umap_transform)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Plot the projected query and retrieved documents in the embedding space\nplt.figure()\nplt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\nplt.scatter(projected_query_embeddings[:, 0], projected_query_embeddings[:, 1], s=150, marker='X', color='r', label=\"original\")\nplt.scatter(projected_adapted_query_embeddings[:, 0], projected_adapted_query_embeddings[:, 1], s=150, marker='X', color='green', label=\"adapted\")\n\nplt.gca().set_aspect('equal', 'datalim')\nplt.title(\"Adapted Queries\")\nplt.axis('off')\nplt.legend()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"other-techniques",children:"Other techniques"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Fine-tune the embedding model"}),"\n",(0,r.jsxs)(n.li,{children:["Fine-tune the LLM for retrieval","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2310.01352",children:"RA-DIT: Retrieval-Augmented Dual Instruction Tuning"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2310.07713",children:"InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Deep embedding adaptors"}),"\n",(0,r.jsx)(n.li,{children:"Deep relevance modelling"}),"\n",(0,r.jsx)(n.li,{children:"Deep chunking"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},47046:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/cross-encoder-69f13cc87eea289fd868eb467f35219b.png"},57287:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/cross-reranking-796e529ba1a878801fff0e332a6b39fc.png"},18483:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/embedding-adaptor-74ede95b51d956f0ab203d99d9f85922.png"},29061:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/expansion-a6bee5e5a60b1ad230fcb82933163a1f.png"},71997:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/multiple-d5d1d6620c9852572e09ad42b467e58f.png"},29537:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/reranking-49596f5459636ea0bc4ce75933ce9372.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>d,a:()=>a});var r=t(67294);const i={},s=r.createContext(i);function a(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);