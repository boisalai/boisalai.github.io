"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6412],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>f});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},p=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=c(t),d=a,f=m["".concat(s,".").concat(d)]||m[d]||u[d]||o;return t?r.createElement(f,i(i({ref:n},p),{},{components:t})):r.createElement(f,i({ref:n},p))}));function f(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,i=new Array(o);i[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[m]="string"==typeof e?e:a,i[1]=l;for(var c=2;c<o;c++)i[c]=t[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},6086:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var r=t(7462),a=(t(7294),t(3905));const o={},i="Llama",l={unversionedId:"snippets/llama",id:"snippets/llama",title:"Llama",description:"Running LLaMa 2 model inference in a Google Colab",source:"@site/docs/snippets/llama.md",sourceDirName:"snippets",slug:"/snippets/llama",permalink:"/docs/snippets/llama",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain",permalink:"/docs/snippets/langchain"}},s={},c=[{value:"Running LLaMa 2 model inference in a Google Colab",id:"running-llama-2-model-inference-in-a-google-colab",level:2},{value:"Finetuning",id:"finetuning",level:2}],p={toc:c},m="wrapper";function u(e){let{components:n,...t}=e;return(0,a.kt)(m,(0,r.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"llama"},"Llama"),(0,a.kt)("h2",{id:"running-llama-2-model-inference-in-a-google-colab"},"Running LLaMa 2 model inference in a Google Colab"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://ai.meta.com/resources/models-and-libraries/llama-downloads/"},"Request access")," to the next version of Llama."),(0,a.kt)("li",{parentName:"ul"},"After gaining access from Meta, head over to ",(0,a.kt)("a",{parentName:"li",href:"https://huggingface.co/meta-llama"},"Hugging Face"),'. Choose your desired model and submit a request to grant access. Expect a "granted access" email within 1-2 days.'),(0,a.kt)("li",{parentName:"ul"},'*Navigate to "Settings" in your Hugging Face account to create access tokens.'),(0,a.kt)("li",{parentName:"ul"},"Select T4 GPU on Google Colab.")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'!pip install transformers\n!huggingface-cli login\n\nfrom transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = "meta-llama/Llama-2-7b-chat-hf"\ntokenizer = AutoTokenizer.from_pretrained(model)\n\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map="auto")\n    \nsequences = pipeline(\n    \'Who are the key contributors to the field of artificial intelligence?\\n\',\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200)\n\nfor seq in sequences:\n    print(f"Result: {seq[\'generated_text\']}")\n')),(0,a.kt)("h2",{id:"finetuning"},"Finetuning"),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"2023-09-14")),(0,a.kt)("p",null,"See ",(0,a.kt)("a",{parentName:"p",href:"https://huggingface.co/docs/trl/sft_trainer"},"Supervised Fine-tuning Trainer"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from transformers import AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\n\nmodel = AutoModelForCausalLM.from_pretrainded(\n    "meta-llama/Llama-2-7b-hf",\n    load_in_4but=True\n)\n...\ndataset = load_dataset(script_args.dataset_name, split="train")\n\ntrainer = SFTTrainer(\n    model = model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field="text",\n    max_seq_length=args.max_seq_length,\n    args=training_arguments\n)\n\ntrainer.train()\n')))}u.isMDXComponent=!0}}]);