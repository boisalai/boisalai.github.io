"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[1780],{64970:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var t=a(85893),o=a(11151);const i={sidebar_label:"Retrieval Augmented Generation for Production with LangChain & LlamaIndex",sidebar_position:7},r="Retrieval Augmented Generation for Production with LangChain & LlamaIndex",s={id:"courses/activeloop/rag-for-production/index",title:"Retrieval Augmented Generation for Production with LangChain & LlamaIndex",description:"LangChain: Basic Concepts Recap",source:"@site/docs/courses/activeloop/rag-for-production/index.md",sourceDirName:"courses/activeloop/rag-for-production",slug:"/courses/activeloop/rag-for-production/",permalink:"/docs/courses/activeloop/rag-for-production/",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/activeloop/rag-for-production/index.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_label:"Retrieval Augmented Generation for Production with LangChain & LlamaIndex",sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"LangChain Chat with Your Data",permalink:"/docs/courses/deeplearning-ai/chat-with-your-data/"},next:{title:"Training and fine-tuning LLMs",permalink:"/docs/courses/fine-tuning-llms"}},d={},l=[{value:"LangChain: Basic Concepts Recap",id:"langchain-basic-concepts-recap",level:2},{value:"Chat Model",id:"chat-model",level:3},{value:"Embedding Model",id:"embedding-model",level:3},{value:"LLMChain",id:"llmchain",level:3},{value:"Sequential",id:"sequential",level:3},{value:"See also",id:"see-also",level:3},{value:"LlamaIndex Introduction: Precision and Simplicity in Information Retrieval",id:"llamaindex-introduction-precision-and-simplicity-in-information-retrieval",level:2},{value:"Data Connectors",id:"data-connectors",level:3},{value:"Nodes",id:"nodes",level:3},{value:"Indices",id:"indices",level:3},{value:"Query Engines",id:"query-engines",level:3},{value:"Routers",id:"routers",level:3},{value:"Saving and Loading Indexes Locally",id:"saving-and-loading-indexes-locally",level:3},{value:"More info on LlamaIndex",id:"more-info-on-llamaindex",level:3},{value:"Module 2 Introduction - Advanced Retrieval Augmented Generation",id:"module-2-introduction---advanced-retrieval-augmented-generation",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"retrieval-augmented-generation-for-production-with-langchain--llamaindex",children:"Retrieval Augmented Generation for Production with LangChain & LlamaIndex"}),"\n",(0,t.jsx)(n.h2,{id:"langchain-basic-concepts-recap",children:"LangChain: Basic Concepts Recap"}),"\n",(0,t.jsx)(n.h3,{id:"chat-model",children:"Chat Model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage\n\nchat = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)\nmessages = [\n    SystemMessage(\n        content="You are a helpful assistant."\n    ),\n    HumanMessage(\n        content="What is the capital of France?"\n    ),\n]\nanswer = chat(messages)\nprint(answer)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"embedding-model",children:"Embedding Model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain.embeddings import OpenAIEmbeddings\n\n# Initialize the model\nembeddings_model = OpenAIEmbeddings()\n\n# Embed a list of texts\nembeddings = embeddings_model.embed_documents(\n    ["Hi there!", "Oh, hello!", "What\'s your name?", "My friends call me World", "Hello World!"]\n)\n\nprint("Number of documents embedded:", len(embeddings))\nprint("Dimension of each embedding:", len(embeddings[0]))\n# Number of documents embedded: 5\n# Dimension of each embedding: 1536\n'})}),"\n",(0,t.jsx)(n.h3,{id:"llmchain",children:"LLMChain"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain.llmchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import StrOutputParser\n\ntemplate = """List all the colors in a rainbow"""\nprompt = PromptTemplate(\n    template=template, input_variables=[], output_parser=StrOutputParser()\n)\n\nchat = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)\nllm_chain = LLMChain(prompt=prompt, llm=chat)\n\nllm_chain.predict()\n'})}),"\n",(0,t.jsx)(n.p,{children:"The same code but with LangChain Expression Language (LCEL)."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import StrOutputParser\n\nprompt = PromptTemplate.from_template(\n    "List all the colors in a {item}."\n)\nrunnable = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\nrunnable.invoke({"item": "rainbow"})\n'})}),"\n",(0,t.jsx)(n.h3,{id:"sequential",children:"Sequential"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain.prompts import PromptTemplate\n\npost_prompt = PromptTemplate.from_template(\n    """You are a business owner. Given the theme of a post, write a social media post to share on my socials.\n\nTheme: {theme}\nContent: This is social media post based on the theme above:"""\n)\n\nreview_prompt = PromptTemplate.from_template(\n    """You are an expert social media manager. Given the presented social media post, it is your job to write a review for the post.\n\nSocial Media Post:\n{post}\nReview from a Social Media Expert:"""\n)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import StrOutputParser\n\nllm = ChatOpenAI(temperature=0.0)\n\nchain = (\n    {"post": post_prompt | llm | StrOutputParser()}\n    | review_prompt\n    | llm\n    | StrOutputParser()\n)\nchain.invoke({"theme": "Having a black friday sale with 50% off on everything."})\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain.schema.runnable import RunnablePassthrough\n\nllm = ChatOpenAI(temperature=0.0)\n\npost_chain = post_prompt | llm | StrOutputParser()\nreview_chain = review_prompt | llm | StrOutputParser()\nchain = {"post": post_chain} | RunnablePassthrough.assign(review=review_chain)\nchain.invoke({"theme": "Having a black friday sale with 50% off on everything."})\n'})}),"\n",(0,t.jsx)(n.h3,{id:"see-also",children:"See also"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://python.langchain.com/docs/expression_language/",children:"LangChain Expression Language (LCEL)"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://medium.com/aimonks/simple-guide-to-text-chunking-for-your-llm-applications-bddfe8ad7892",children:"Simple guide to Text Chunking for Your LLM Applications"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://medium.com/@ashu.goel_9925/understanding-text-chunking-for-the-llm-application-da59cbc2855b",children:"Understanding Text Chunking for the LLM Application"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://nanonets.com/blog/langchain/#module-ii-retrieval",children:"A Complete LangChain Guide"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.davidgentile.net/langchain-models/",children:"LangChain Models: Simple and Consistent Interfaces for LLMs, Chat, and Text Embeddings"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.kanaries.net/articles/langchain-openai#chains-combining-llms-and-prompts",children:"Unleash the Potential of LangChain in Web Application Development"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"llamaindex-introduction-precision-and-simplicity-in-information-retrieval",children:"LlamaIndex Introduction: Precision and Simplicity in Information Retrieval"}),"\n",(0,t.jsx)(n.h3,{id:"data-connectors",children:"Data Connectors"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://llamahub.ai/",children:"LlamaHub"})," is an open-source project that hosts data connectors."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import os\nimport logging\nimport sys\n\nfrom llama_index import download_loader\n\n# You can set the logging level to DEBUG for more verbose output,\n# or use level=logging.INFO for less detailed information.\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nWikipediaReader = download_loader(\"WikipediaReader\")\nloader = WikipediaReader()\n\ndocuments = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\nprint(len(documents))\n# 2\n"})}),"\n",(0,t.jsx)(n.h3,{id:"nodes",children:"Nodes"}),"\n",(0,t.jsxs)(n.p,{children:["In LlamaIndex, once data is ingested as documents, it passes through a processing structure that transforms these\ndocuments into ",(0,t.jsx)(n.code,{children:"Node"})," objects. Nodes are smaller, more granular data units created from the original\ndocuments. Besides their primary content, these nodes also contain metadata and contextual information."]}),"\n",(0,t.jsxs)(n.p,{children:["LlamaIndex features a ",(0,t.jsx)(n.code,{children:"NodeParser"})," class designed to convert the content of documents into structured\nnodes automatically. The ",(0,t.jsx)(n.code,{children:"SimpleNodeParser"})," converts a list of document objects into nodes."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from llama_index.node_parser import SimpleNodeParser\n\n# Assuming documents have already been loaded\n\n# Initialize the parser\nparser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=20)\n\n# Parse documents into nodes\nnodes = parser.get_nodes_from_documents(documents)\nprint(len(nodes))\n# 48\n"})}),"\n",(0,t.jsx)(n.p,{children:"The code above splits the two retrieved documents from the Wikipedia page into 48 smaller chunks with slight overlap."}),"\n",(0,t.jsx)(n.h3,{id:"indices",children:"Indices"}),"\n",(0,t.jsx)(n.p,{children:"At the heart of LlamaIndex is the capability to index and search various data formats like documents,\nPDFs, and database queries. Indexing is an initial step for storing information in a database; it\nessentially transforms the unstructured data into embeddings that capture semantic meaning and\noptimize the data format so it can be easily accessed and queried."}),"\n",(0,t.jsx)(n.p,{children:"LlamaIndex has a variety of index types, each fulfills a specific role. We have highlighted some\nof the popular index types in the following subsections."}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/DocSummary.html",children:"Summary Index"}),"\nextracts a summary from each document and stores it with all the nodes in that document. Since it\u2019s not always\neasy to match small node embeddings with a query, sometimes having a document summary helps."]}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_guide.html",children:"Vector Store Index"}),"\ngenerates embeddings during index construction to identify the top-k most similar nodes in response to a query.\nIt\u2019s suitable for small-scale applications and easily scalable to accommodate larger datasets using high-performance vector databases.\nSee also ",(0,t.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/community/integrations/vector_stores.html",children:"Vector Store"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"query-engines",children:"Query Engines"}),"\n",(0,t.jsxs)(n.p,{children:["The next step is to leverage the generated indexes to query through the information. The Query Engine is a wrapper that\ncombines a Retriever and a Response Synthesizer into a pipeline. The pipeline uses the query string to fetch nodes and\nthen sends them to the LLM to generate a response. A query engine can be created by calling\nthe ",(0,t.jsx)(n.code,{children:"as_query_engine()"})," method on an already-created index."]}),"\n",(0,t.jsxs)(n.p,{children:["The code below uses the documents fetched from the Wikipedia page to construct a Vector Store Index using\nthe ",(0,t.jsx)(n.code,{children:"GPTVectorStoreIndex"})," class. The ",(0,t.jsx)(n.code,{children:".from_documents()"})," method simplifies building indexes on these processed documents.\nThe created index can then be utilized to generate a ",(0,t.jsx)(n.code,{children:"query_engine"})," object, allowing us to ask questions\nbased on the documents using the ",(0,t.jsx)(n.code,{children:".query()"})," method."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from llama_index import GPTVectorStoreIndex\n\nindex = GPTVectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query("What does NLP stands for?")\nprint(response.response)\n# NLP stands for Natural Language Processing.\n'})}),"\n",(0,t.jsxs)(n.p,{children:["See also ",(0,t.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/examples/query_engine/custom_query_engine.html",children:"Defining a Custom Query Engine"})]}),"\n",(0,t.jsx)(n.h3,{id:"routers",children:"Routers"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/querying/router/root.html#routers",children:"Routers"}),"\nplay a role in determining the most appropriate retriever for extracting context from the knowledge base. The routing function selects the optimal query engine for each task, improving performance and accuracy."]}),"\n",(0,t.jsx)(n.p,{children:"These functions are beneficial when dealing with multiple data sources, each holding unique information. Consider an application that employs a SQL database and a Vector Store as its knowledge base. In this setup, the router can determine which data source is most applicable to the given query."}),"\n",(0,t.jsxs)(n.p,{children:["You can see a working example of incorporating the\nrouters ",(0,t.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/querying/router/root.html#routers",children:"in this tutorial"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"saving-and-loading-indexes-locally",children:"Saving and Loading Indexes Locally"}),"\n",(0,t.jsxs)(n.p,{children:["All the examples we explored involved storing indexes on cloud-based vector stores like Deep Lake.\nHowever, there are scenarios where saving the data on a disk might be necessary for rapid testing.\nThe concept of storing refers to saving the index data, which includes the nodes and their associated\nembeddings, to disk. This is done using the ",(0,t.jsx)(n.code,{children:"persist()"})," method from the ",(0,t.jsx)(n.code,{children:"storage_context"}),"\nobject related to the index."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# store index as vector embeddings on the disk\nindex.storage_context.persist()\n# This saves the data in the 'storage' by default\n# to minimize repetitive processing\n"})}),"\n",(0,t.jsx)(n.p,{children:"If the index already exists in storage, you can load it directly instead of recreating it. We simply\need to determine whether the index already exists on disk and proceed accordingly; here is how to do it:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Index Storage Checks\nimport os.path\nfrom llama_index import (\n    VectorStoreIndex,\n    StorageContext,\n    load_index_from_storage,\n)\nfrom llama_index import download_loader\n\n# Let's see if our index already exists in storage.\nif not os.path.exists(\"./storage\"):\n    # If not, we'll load the Wikipedia data and create a new index\n    WikipediaReader = download_loader(\"WikipediaReader\")\n    loader = WikipediaReader()\n    documents = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\n    index = VectorStoreIndex.from_documents(documents)\n    # Index storing\n    index.storage_context.persist()\n\nelse:\n    # If the index already exists, we'll just load it:\n    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n    index = load_index_from_storage(storage_context)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"more-info-on-llamaindex",children:"More info on LlamaIndex"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/",children:"LlamaIndex Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://cookbook.openai.com/examples/third_party/financial_document_analysis_with_llamaindex",children:"Financial Document Analysis with LlamaIndex"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.datacamp.com/tutorial/llama-index-adding-personal-data-to-llms",children:"LlamaIndex: Adding Personal Data to LLMs"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.datacamp.com/blog/the-pros-and-cons-of-using-llm-in-the-cloud-versus-running-llm-locally",children:"The Pros and Cons of Using LLMs in the Cloud Versus Running LLMs Locally"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://howaibuildthis.substack.com/p/llamaindex-how-to-use-index-correctly",children:"LlamaIndex: How to use Index correctly"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-2-introduction---advanced-retrieval-augmented-generation",children:"Module 2 Introduction - Advanced Retrieval Augmented Generation"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://learn.activeloop.ai/courses/take/rag/multimedia/51352087-module-2-introduction-advanced-retrieval-augmented-generation",children:"This page"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'!pip3 install deeplake langchain openai tiktoken llama-index\n\nimport os, getpass\nos.environ[\'ACTIVELOOP_TOKEN\'] = getpass.getpass()\nos.environ[\'OPENAI_API_KEY\'] = getpass.getpass()\n\n!mkdir -p \'data/paul_graham/\'\n!curl \'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\' -o \'data/paul_graham/paul_graham_essay.txt\'\n\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader("./data/paul_graham/").load_data()\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)\n\n# By default, the node/chunks ids are set to random uuids. To ensure same id\'s per run, we manually set them.\nfor idx, node in enumerate(nodes):\n    node.id_ = f"node_{idx}"\n\nprint(f"Number of Documents: {len(documents)}")\nprint(f"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}")\n\nfrom llama_index import VectorStoreIndex, ServiceContext, StorageContext\nfrom llama_index.vector_stores import DeepLakeVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms import OpenAI\n\n# Create a DeepLakeVectorStore locally to store the vectors\ndataset_path = "./data/paul_graham/deep_lake_db"\nvector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True, exec_option="compute_engine")\n\n# LLM that will answer questions with the retrieved context\nllm = OpenAI(model="gpt-3.5-turbo-1106")\nembed_model = OpenAIEmbedding()\n\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nvector_index = VectorStoreIndex(nodes, service_context=service_context, storage_context=storage_context, show_progress=True)\n\nimport deeplake\nlocal = "./data/paul_graham/deep_lake_db"\nhub_path = "hub://genai360/LlamaIndex_paulgraham_essay"\nhub_managed_path = "hub://genai360/LlamaIndex_paulgraham_essay_managed"\n\n# First upload our local vector store\ndeeplake.deepcopy(local, hub_path, overwrite=True)\n# Create a managed vector store under a different name\ndeeplake.deepcopy(hub_path, hub_managed_path, overwrite=True, runtime={"tensor_db": True})\n\ndb = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, exec_option="compute_engine", read_only=True,)\n\n# Fetch dataset docs and ids \ndocs = db.vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)[\'value\']\nids = db.vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)[\'value\']\nprint(len(docs))\n\nfrom openai import OpenAI\nclient = OpenAI()\n\ndef generate_question(text):\n    try:\n        response = client.chat.completions.create(\n            model="gpt-3.5-turbo-1106",\n            messages=[\n                {"role": "system", "content": "You are a world class expert for generating questions based on provided context. \\\n                        You make sure the question can be answered by the text."},\n                {\n                    "role": "user",\n                    "content": text,\n                },\n            ],\n        )\n        return response.choices[0].message.content\n    except:\n        question_string = "No question generated"\n        return question_string\n\nimport random\nfrom tqdm import tqdm\n\ndef generate_queries(docs: list[str], ids: list[str], n: int):\n\n    questions = []\n    relevances = []\n    pbar = tqdm(total=n)\n    while len(questions) < n:\n        # 1. randomly draw a piece of text and relevance id\n        r = random.randint(0, len(docs)-1)\n        text, label = docs[r], ids[r]\n\n        # 2. generate queries and assign and relevance id\n        generated_qs = [generate_question(text)]\n        if generated_qs == ["No question generated"]:\n            print("No question generated")\n            continue\n\n        questions.extend(generated_qs)\n        relevances.extend([[(label, 1)] for _ in generated_qs])\n        pbar.update(len(generated_qs))\n\n    return questions[:n], relevances[:n]\n\nquestions, relevances = generate_queries(docs, ids, n=40)\nprint(len(questions)) #40\nprint(questions[0])\n\n# Train deep memory\njob_id = db.vectorstore.deep_memory.train(\n    queries=questions,\n    relevance=relevances,\n)\n'})})]})}function m(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},11151:(e,n,a)=>{a.d(n,{Z:()=>s,a:()=>r});var t=a(67294);const o={},i=t.createContext(o);function r(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);