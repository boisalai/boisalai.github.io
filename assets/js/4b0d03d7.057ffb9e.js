"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[9768],{43172:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>d});var r=t(85893),a=t(11151);const i={sidebar_label:"LangChain Chat with Your Data",sidebar_position:7},s="LangChain: Chat with Your Data",o={id:"courses/deeplearning-ai/sp07-chat-with-your-data",title:"LangChain: Chat with Your Data",description:"DeepLearning.AI, LangChain: Chat with Your Data.",source:"@site/docs/courses/deeplearning-ai/sp07-chat-with-your-data.md",sourceDirName:"courses/deeplearning-ai",slug:"/courses/deeplearning-ai/sp07-chat-with-your-data",permalink:"/docs/courses/deeplearning-ai/sp07-chat-with-your-data",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/deeplearning-ai/sp07-chat-with-your-data.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_label:"LangChain Chat with Your Data",sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"Vector Databases from Embeddings to Applications",permalink:"/docs/courses/deeplearning-ai/sp06-vector-database"},next:{title:"Advanced Retrieval for AI with Chroma",permalink:"/docs/courses/deeplearning-ai/sp08-advanced-rag"}},l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Document Loading",id:"document-loading",level:2},{value:"Loaders",id:"loaders",level:3},{value:"Retrieval augmented generation",id:"retrieval-augmented-generation",level:3},{value:"PDFs",id:"pdfs",level:3},{value:"Youtube",id:"youtube",level:3},{value:"URLs",id:"urls",level:3},{value:"Notion",id:"notion",level:3},{value:"Document Splitting",id:"document-splitting",level:2},{value:"Recursive splitting details",id:"recursive-splitting-details",level:3},{value:"Token splitting",id:"token-splitting",level:3},{value:"Context aware splitting",id:"context-aware-splitting",level:3},{value:"Vectorstored and Embedding",id:"vectorstored-and-embedding",level:2},{value:"Embeddings",id:"embeddings",level:3},{value:"Vectorstores",id:"vectorstores",level:3},{value:"Similarity Search",id:"similarity-search",level:3},{value:"Failure modes",id:"failure-modes",level:3},{value:"Retrieval",id:"retrieval",level:2},{value:"Vectorstore retrieval",id:"vectorstore-retrieval",level:3},{value:"Similarity Search (again)",id:"similarity-search-again",level:3},{value:"Addressing Diversity: Maximum marginal relevance",id:"addressing-diversity-maximum-marginal-relevance",level:3},{value:"Addressing Specificity: working with metadata",id:"addressing-specificity-working-with-metadata",level:3},{value:"Addressing Specificity: working with metadata using self-query retriever",id:"addressing-specificity-working-with-metadata-using-self-query-retriever",level:3},{value:"Additional tricks: compression",id:"additional-tricks-compression",level:3},{value:"Combining various techniques",id:"combining-various-techniques",level:3},{value:"Other types of retrieval",id:"other-types-of-retrieval",level:3},{value:"Question Answering",id:"question-answering",level:2},{value:"Overview",id:"overview",level:3},{value:"RetrievalQA chain",id:"retrievalqa-chain",level:3},{value:"Prompt",id:"prompt",level:3},{value:"RetrievalQA chain types",id:"retrievalqa-chain-types",level:3},{value:"RetrievalQA limitations",id:"retrievalqa-limitations",level:3},{value:"Chat",id:"chat",level:2},{value:"Memory",id:"memory",level:3},{value:"ConversationalRetrievalChain",id:"conversationalretrievalchain",level:3},{value:"Create a chatbot that works on your documents",id:"create-a-chatbot-that-works-on-your-documents",level:3},{value:"Create a chatbot",id:"create-a-chatbot",level:3},{value:"Acknowledgments",id:"acknowledgments",level:3}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"langchain-chat-with-your-data",children:"LangChain: Chat with Your Data"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"DeepLearning.AI"}),", ",(0,r.jsx)(n.a,{href:"https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/",children:"LangChain: Chat with Your Data"}),"."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"2023-10-11."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The course delves into two main topics:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Retrieval Augmented Generation (RAG), a common LLM application that retrieves contextual documents from an external dataset,"}),"\n",(0,r.jsx)(n.li,{children:"and a guide to building a chatbot that responds to queries based on the content of your documents, rather than the information it has learned in training."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"You\u2019ll learn about:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Document Loading: Learn the fundamentals of data loading and discover over 80 unique loaders LangChain provides to access diverse data sources, including audio and video."}),"\n",(0,r.jsx)(n.li,{children:"Document Splitting: Discover the best practices and considerations for splitting data."}),"\n",(0,r.jsx)(n.li,{children:"Vector stores and embeddings: Dive into the concept of embeddings and explore vector store integrations within LangChain."}),"\n",(0,r.jsx)(n.li,{children:"Retrieval: Grasp advanced techniques for accessing and indexing data in the vector store, enabling you to retrieve the most relevant information beyond semantic queries."}),"\n",(0,r.jsx)(n.li,{children:"Question Answering: Build a one-pass question-answering solution. 6. Chat: Learn how to track and select pertinent information from conversations and data sources, as you build your own chatbot using LangChain.Start building practical applications that allow you to interact with data using LangChain and LLMs."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Learn directly from LangChain creator, Harrison Chase"}),"\n",(0,r.jsx)(n.li,{children:"Access over 80 unique loaders to handle accessing various data sources using LangChain"}),"\n",(0,r.jsx)(n.li,{children:"Build your own chatbot so chat directly with information from your own documents and data"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"Retrieval Augmented Generation"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"s01",src:t(72366).Z+"",width:"1436",height:"966"})}),"\n",(0,r.jsx)(n.h2,{id:"document-loading",children:"Document Loading"}),"\n",(0,r.jsx)(n.h3,{id:"loaders",children:"Loaders"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Loaders deal with the specifics of accessing and convertig data.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Accessing: Web Sites, Data Bases, Youtube, arXiv..."}),"\n",(0,r.jsx)(n.li,{children:"Data Types: PDF, HTML, JSON, Word, PowerPoint..."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Returns a list of ",(0,r.jsx)(n.code,{children:"Document"})," objects."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"retrieval-augmented-generation",children:"Retrieval augmented generation"}),"\n",(0,r.jsx)(n.p,{children:"In retrieval augmented generation (RAG), an LLM retrieves contextual documents from an external dataset as part of its execution."}),"\n",(0,r.jsx)(n.p,{children:"This is useful if we want to ask question about specific documents (e.g., our PDFs, a set of videos, etc)."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"s01",src:t(23330).Z+"",width:"2468",height:"707"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#! pip install langchain\n\nimport os\nimport openai\nimport sys\nsys.path.append('../..')\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n\nopenai.api_key  = os.environ['OPENAI_API_KEY']\n"})}),"\n",(0,r.jsx)(n.h3,{id:"pdfs",children:"PDFs"}),"\n",(0,r.jsxs)(n.p,{children:["Let's load a PDF ",(0,r.jsx)(n.a,{href:"https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf",children:"transcript"}),"\nfrom Andrew Ng's famous CS229 course! These documents are the result of automated transcription so words and sentences are sometimes split unexpectedly."]}),"\n",(0,r.jsxs)(n.p,{children:["Each page is a ",(0,r.jsx)(n.code,{children:"Document"}),". A ",(0,r.jsx)(n.code,{children:"Document"})," contains text (",(0,r.jsx)(n.code,{children:"page_content"}),") and ",(0,r.jsx)(n.code,{children:"metadata"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# The course will show the pip installs you would need to install packages on your own machine.\n# These packages are already installed on this platform and should not be run again.\n#! pip install pypdf\n\nfrom langchain.document_loaders import PyPDFLoader\nloader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\npages = loader.load()\n\nlen(pages)\n# 22\npage = pages[0]\nprint(page.page_content[0:500])\n# ...\nprint(page.metadata)\n# {'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"youtube",children:"Youtube"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from langchain.document_loaders.generic import GenericLoader\nfrom langchain.document_loaders.parsers import OpenAIWhisperParser\nfrom langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n\n# ! pip install yt_dlp\n# ! pip install pydub\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note"}),": This can take several minutes to complete."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'url="https://www.youtube.com/watch?v=jGwO_UgTS7I"\nsave_dir="docs/youtube/"\nloader = GenericLoader(\n    YoutubeAudioLoader([url],save_dir),\n    OpenAIWhisperParser()\n)\ndocs = loader.load()\ndocs[0].page_content[0:500]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"urls",children:"URLs"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader("https://github.com/basecamp/handbook/blob/master/37signals-is-you.md")\ndocs = loader.load()\nprint(docs[0].page_content[:500])\n'})}),"\n",(0,r.jsx)(n.h3,{id:"notion",children:"Notion"}),"\n",(0,r.jsxs)(n.p,{children:["Follow steps ",(0,r.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/notion",children:"here"})," for an example Notion site such as ",(0,r.jsx)(n.a,{href:"https://yolospace.notion.site/Blendle-s-Employee-Handbook-e31bff7da17346ee99f531087d8b133f",children:"this one"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Duplicate the page into your own Notion space and export as ",(0,r.jsx)(n.code,{children:"Markdown / CSV"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"Unzip it and save it as a folder that contains the markdown file for the Notion page."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.document_loaders import NotionDirectoryLoader\nloader = NotionDirectoryLoader("docs/Notion_DB")\ndocs = loader.load()\nprint(docs[0].page_content[0:200])\nprint(docs[0].metadata)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"document-splitting",children:"Document Splitting"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n\nchunk_size = 26\nchunk_overlap = 4\n\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\nc_splitter = CharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"text1 = 'abcdefghijklmnopqrstuvwxyz'\nr_splitter.split_text(text1)\n# ['abcdefghijklmnopqrstuvwxyz']\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'\nr_splitter.split_text(text2)\n# ['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n\nr_splitter.split_text(text3)\n# ['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']\n\nc_splitter.split_text(text3)\n# ['a b c d e f g h i j k l m n o p q r s t u v w x y z']\n\nc_splitter = CharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap,\n    separator = ' '\n)\nc_splitter.split_text(text3)\n# ['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']\n"})}),"\n",(0,r.jsx)(n.h3,{id:"recursive-splitting-details",children:"Recursive splitting details"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"RecursiveCharacterTextSplitter"})," is recommended for generic text."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'some_text = """When writing documents, writers will use document structure to group content. \\\nThis can convey to the reader, which idea\'s are related. For example, closely related ideas \\\nare in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\nParagraphs are often delimited with a carriage return or two carriage returns. \\\nCarriage returns are the "backslash n" you see embedded in this string. \\\nSentences have a period at the end, but also, have a space.\\\nand words are separated by space."""\n\nlen(some_text) # 496\n\nc_splitter = CharacterTextSplitter(\n    chunk_size=450,\n    chunk_overlap=0,\n    separator = \' \'\n)\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=450,\n    chunk_overlap=0,\n    separators=["\\n\\n", "\\n", " ", ""]\n)\n\nc_splitter.split_text(some_text)\n\nr_splitter.split_text(some_text)\n'})}),"\n",(0,r.jsx)(n.p,{children:"Let's reduce the chunk size a bit and add a period to our separators:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'r_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=150,\n    chunk_overlap=0,\n    separators=["\\n\\n", "\\n", "\\. ", " ", ""]\n)\nr_splitter.split_text(some_text)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'r_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=150,\n    chunk_overlap=0,\n    separators=["\\n\\n", "\\n", "(?<=\\. )", " ", ""]\n)\nr_splitter.split_text(some_text)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.document_loaders import PyPDFLoader\nloader = PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture01.pdf")\npages = loader.load()\n\nfrom langchain.text_splitter import CharacterTextSplitter\ntext_splitter = CharacterTextSplitter(\n    separator="\\n",\n    chunk_size=1000,\n    chunk_overlap=150,\n    length_function=len\n)\n\ndocs = text_splitter.split_documents(pages)\n\nlen(docs) # 77\nlen(pages) # 22\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.document_loaders import NotionDirectoryLoader\nloader = NotionDirectoryLoader("docs/Notion_DB")\nnotion_db = loader.load()\n\ndocs = text_splitter.split_documents(notion_db)\n\nlen(notion_db) # 52\nlen(docs) # 353\n'})}),"\n",(0,r.jsx)(n.h3,{id:"token-splitting",children:"Token splitting"}),"\n",(0,r.jsx)(n.p,{children:"We can also split on token count explicity, if we want."}),"\n",(0,r.jsx)(n.p,{children:"This can be useful because LLMs often have context windows designated in tokens."}),"\n",(0,r.jsx)(n.p,{children:"Tokens are often ~4 characters."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from langchain.text_splitter import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\ntext1 = \"foo bar bazzyfoo\"\ntext_splitter.split_text(text1)\n# ['foo', ' bar', ' b', 'az', 'zy', 'foo']\n\ntext_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\ndocs = text_splitter.split_documents(pages)\ndocs[0]\n# Document(page_content='MachineLearning-Lecture01  \\n', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0})\npages[0].metadata\n# {'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"context-aware-splitting",children:"Context aware splitting"}),"\n",(0,r.jsx)(n.p,{children:"Chunking aims to keep text with common context together."}),"\n",(0,r.jsx)(n.p,{children:"A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting."}),"\n",(0,r.jsxs)(n.p,{children:["We can use ",(0,r.jsx)(n.code,{children:"MarkdownHeaderTextSplitter"})," to preserve header metadata in our chunks, as show below."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.document_loaders import NotionDirectoryLoader\nfrom langchain.text_splitter import MarkdownHeaderTextSplitter\n\nmarkdown_document = """# Title\\n\\n \\\n## Chapter 1\\n\\n \\\nHi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n### Section \\n\\n \\\nHi this is Lance \\n\\n\n## Chapter 2\\n\\n \\\nHi this is Molly"""\n\nheaders_to_split_on = [\n    ("#", "Header 1"),\n    ("##", "Header 2"),\n    ("###", "Header 3"),\n]\n\nmarkdown_splitter = MarkdownHeaderTextSplitter(\n    headers_to_split_on=headers_to_split_on\n)\nmd_header_splits = markdown_splitter.split_text(markdown_document)\n\nmd_header_splits[0]\n\nmd_header_splits[1]\n'})}),"\n",(0,r.jsx)(n.p,{children:"Try on a real Markdown file, like a Notion database."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'loader = NotionDirectoryLoader("docs/Notion_DB")\ndocs = loader.load()\ntxt = \' \'.join([d.page_content for d in docs])\n\nheaders_to_split_on = [\n    ("#", "Header 1"),\n    ("##", "Header 2"),\n]\nmarkdown_splitter = MarkdownHeaderTextSplitter(\n    headers_to_split_on=headers_to_split_on\n)\n\nmd_header_splits = markdown_splitter.split_text(txt)\n\nmd_header_splits[0]\n'})}),"\n",(0,r.jsx)(n.h2,{id:"vectorstored-and-embedding",children:"Vectorstored and Embedding"}),"\n",(0,r.jsx)(n.p,{children:"Recall the overall workflow for retrieval augmented generation (RAG):"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"overview.jpeg",src:t(82502).Z+"",width:"2468",height:"707"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\nimport openai\nimport sys\nsys.path.append('../..')\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n\nopenai.api_key  = os.environ['OPENAI_API_KEY']\n"})}),"\n",(0,r.jsxs)(n.p,{children:["We just discussed ",(0,r.jsx)(n.code,{children:"Document Loading"})," and ",(0,r.jsx)(n.code,{children:"Splitting"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.document_loaders import PyPDFLoader\n\n# Load PDF\nloaders = [\n    # Duplicate documents on purpose - messy data\n    PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture01.pdf"),\n    PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture01.pdf"), # Simulate dirty data.\n    PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture02.pdf"),\n    PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture03.pdf")\n]\ndocs = []\nfor loader in loaders:\n    docs.extend(loader.load())\n\n# Split\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 1500,\n    chunk_overlap = 150\n)\n\nsplits = text_splitter.split_documents(docs)\nlen(splits)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"embeddings",children:"Embeddings"}),"\n",(0,r.jsx)(n.p,{children:"Let's take our splits and embed them."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.embeddings.openai import OpenAIEmbeddings\nembedding = OpenAIEmbeddings()\n\nsentence1 = "i like dogs"\nsentence2 = "i like canines"\nsentence3 = "the weather is ugly outside"\n\nembedding1 = embedding.embed_query(sentence1)\nembedding2 = embedding.embed_query(sentence2)\nembedding3 = embedding.embed_query(sentence3)\n\nimport numpy as np\n\nnp.dot(embedding1, embedding2)\n# 0.963185...\nnp.dot(embedding1, embedding3)\n# 0.770999...\nnp.dot(embedding2, embedding3)\n# 0.759633...\n'})}),"\n",(0,r.jsx)(n.h3,{id:"vectorstores",children:"Vectorstores"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# ! pip install chromadb\n\nfrom langchain.vectorstores import Chroma\n\npersist_directory = 'docs/chroma/'\n\n!rm -rf ./docs/chroma  # remove old database files if any\n\nvectordb = Chroma.from_documents(\n    documents=splits,\n    embedding=embedding,\n    persist_directory=persist_directory\n)\n\nprint(vectordb._collection.count()) # 209\n"})}),"\n",(0,r.jsx)(n.h3,{id:"similarity-search",children:"Similarity Search"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "is there an email i can ask for help"\ndocs = vectordb.similarity_search(question, k=3)\nprint(len(docs)) # 3\nprint(docs[0].page_content)\n'})}),"\n",(0,r.jsx)(n.p,{children:"Let's save this so we can use it later!"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"vectordb.persist()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"failure-modes",children:"Failure modes"}),"\n",(0,r.jsx)(n.p,{children:"This seems great, and basic similarity search will get you 80% of the way there very easily."}),"\n",(0,r.jsx)(n.p,{children:"But there are some failure modes that can creep up."}),"\n",(0,r.jsx)(n.p,{children:"Here are some edge cases that can arise - we'll fix them in the next class."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "what did they say about matlab?"\ndocs = vectordb.similarity_search(question, k=5)\ndocs[0]\ndocs[1]\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Notice that we're getting duplicate chunks (because of the duplicate ",(0,r.jsx)(n.code,{children:"MachineLearning-Lecture01.pdf"})," in the index)."]}),"\n",(0,r.jsx)(n.p,{children:"Semantic search fetches all similar documents, but does not enforce diversity."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"docs[0]"})," and ",(0,r.jsx)(n.code,{children:"docs[1]"})," are indentical."]}),"\n",(0,r.jsx)(n.p,{children:"We can see a new failure mode."}),"\n",(0,r.jsx)(n.p,{children:"The question below asks a question about the third lecture, but includes results from other lectures as well."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "what did they say about regression in the third lecture?"\ndocs = vectordb.similarity_search(question,k=5)\n\nfor doc in docs:\n    print(doc.metadata)\n\nprint(docs[4].page_content)\n'})}),"\n",(0,r.jsx)(n.p,{children:"Approaches discussed in the next lecture can be used to address both!"}),"\n",(0,r.jsx)(n.h2,{id:"retrieval",children:"Retrieval"}),"\n",(0,r.jsx)(n.p,{children:"Retrieval is the centerpiece of our retrieval augmented generation (RAG) flow."}),"\n",(0,r.jsx)(n.p,{children:"Let's get our vectorDB from before."}),"\n",(0,r.jsx)(n.h3,{id:"vectorstore-retrieval",children:"Vectorstore retrieval"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\nimport openai\nimport sys\nsys.path.append('../..')\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n\nopenai.api_key  = os.environ['OPENAI_API_KEY']\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!pip install lark\n"})}),"\n",(0,r.jsx)(n.h3,{id:"similarity-search-again",children:"Similarity Search (again)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\npersist_directory = 'docs/chroma/'\n\nembedding = OpenAIEmbeddings()\nvectordb = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embedding\n)\n\nprint(vectordb._collection.count())\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'texts = [\n    """The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).""",\n    """A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.""",\n    """A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.""",\n]\n\nsmalldb = Chroma.from_texts(texts, embedding=embedding)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "Tell me about all-white mushrooms with large fruiting bodies"\nsmalldb.similarity_search(question, k=2)\nsmalldb.max_marginal_relevance_search(question,k=2, fetch_k=3)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"addressing-diversity-maximum-marginal-relevance",children:"Addressing Diversity: Maximum marginal relevance"}),"\n",(0,r.jsx)(n.p,{children:"Last class we introduced one problem: how to enforce diversity in the search results."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"Maximum marginal relevance"})," strives to achieve both relevance to the query ",(0,r.jsx)(n.em,{children:"and diversity"})," among the results."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "what did they say about matlab?"\ndocs_ss = vectordb.similarity_search(question,k=3)\ndocs_ss[0].page_content[:100]\ndocs_ss[1].page_content[:100]\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Note the difference in results with ",(0,r.jsx)(n.code,{children:"MMR"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)\ndocs_mmr[0].page_content[:100]\ndocs_mmr[1].page_content[:100]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"addressing-specificity-working-with-metadata",children:"Addressing Specificity: working with metadata"}),"\n",(0,r.jsx)(n.p,{children:"In last lecture, we showed that a question about the third lecture can include results from other lectures as well."}),"\n",(0,r.jsxs)(n.p,{children:["To address this, many vectorstores support operations on ",(0,r.jsx)(n.code,{children:"metadata"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"metadata"})," provides context for each embedded chunk."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "what did they say about regression in the third lecture?"\n\ndocs = vectordb.similarity_search(\n    question,\n    k=3,\n    filter={"source":"docs/cs229_lectures/MachineLearning-Lecture03.pdf"}\n)\n\nfor d in docs:\n    print(d.metadata)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"addressing-specificity-working-with-metadata-using-self-query-retriever",children:"Addressing Specificity: working with metadata using self-query retriever"}),"\n",(0,r.jsx)(n.p,{children:"But we have an interesting challenge: we often want to infer the metadata from the query itself."}),"\n",(0,r.jsxs)(n.p,{children:["To address this, we can use ",(0,r.jsx)(n.code,{children:"SelfQueryRetriever"}),", which uses an LLM to extract:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.code,{children:"query"})," string to use for vector search"]}),"\n",(0,r.jsx)(n.li,{children:"A metadata filter to pass in as well"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Most vector databases support metadata filters, so this doesn't require any new databases or indexes."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from langchain.llms import OpenAI\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'metadata_field_info = [\n    AttributeInfo(\n        name="source",\n        description="The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`",\n        type="string",\n    ),\n    AttributeInfo(\n        name="page",\n        description="The page from the lecture",\n        type="integer",\n    ),\n]\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'document_content_description = "Lecture notes"\nllm = OpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectordb,\n    document_content_description,\n    metadata_field_info,\n    verbose=True\n)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "what did they say about regression in the third lecture?"\ndocs = retriever.get_relevant_documents(question)\n\nfor d in docs:\n    print(d.metadata)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"You will receive a warning"})," about ",(0,r.jsx)(n.code,{children:"predict_and_parse"})," being deprecated the first time you executing the next line. This can be safely ignored."]}),"\n",(0,r.jsx)(n.h3,{id:"additional-tricks-compression",children:"Additional tricks: compression"}),"\n",(0,r.jsx)(n.p,{children:"Another approach for improving the quality of retrieved docs is compression."}),"\n",(0,r.jsx)(n.p,{children:"Information most relevant to a query may be buried in a document with a lot of irrelevant text."}),"\n",(0,r.jsx)(n.p,{children:"Passing that full document through your application can lead to more expensive LLM calls and poorer responses."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Contextual compression"})," is meant to fix this."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\n# Wrap our vectorstore\nllm = OpenAI(temperature=0)\ncompressor = LLMChainExtractor.from_llm(llm)\n\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=vectordb.as_retriever()\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def pretty_print_docs(docs):\n    print(f"\\n{\'-\' * 100}\\n".join([f"Document {i+1}:\\n\\n" + d.page_content for i, d in enumerate(docs)]))\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "what did they say about matlab?"\ncompressed_docs = compression_retriever.get_relevant_documents(question)\npretty_print_docs(compressed_docs)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"combining-various-techniques",children:"Combining various techniques"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'compression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=vectordb.as_retriever(search_type = "mmr")\n)\n\nquestion = "what did they say about matlab?"\ncompressed_docs = compression_retriever.get_relevant_documents(question)\npretty_print_docs(compressed_docs)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"other-types-of-retrieval",children:"Other types of retrieval"}),"\n",(0,r.jsx)(n.p,{children:"It's worth noting that vectordb as not the only kind of tool to retrieve documents."}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"LangChain"})," retriever abstraction includes other ways to retrieve documents, such as TF-IDF or SVM."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.retrievers import SVMRetriever\nfrom langchain.retrievers import TFIDFRetriever\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Load PDF\nloader = PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture01.pdf")\npages = loader.load()\nall_page_text=[p.page_content for p in pages]\njoined_page_text=" ".join(all_page_text)\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\nsplits = text_splitter.split_text(joined_page_text)\n\n# Retrieve\nsvm_retriever = SVMRetriever.from_texts(splits,embedding)\ntfidf_retriever = TFIDFRetriever.from_texts(splits)\n\nquestion = "What are major topics for this class?"\ndocs_svm=svm_retriever.get_relevant_documents(question)\ndocs_svm[0]\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "what did they say about matlab?"\ndocs_tfidf=tfidf_retriever.get_relevant_documents(question)\ndocs_tfidf[0]\n'})}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query",children:"self-querying"}),"\nretriever in particular is his favorite."]}),"\n",(0,r.jsx)(n.h2,{id:"question-answering",children:"Question Answering"}),"\n",(0,r.jsx)(n.h3,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Recall the overall workflow for retrieval augmented generation (RAG):"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"overview.jpeg",src:t(82502).Z+"",width:"2468",height:"707"})}),"\n",(0,r.jsxs)(n.p,{children:["We discussed ",(0,r.jsx)(n.code,{children:"Document Loading"})," and ",(0,r.jsx)(n.code,{children:"Splitting"})," as well as ",(0,r.jsx)(n.code,{children:"Storage"})," and ",(0,r.jsx)(n.code,{children:"Retrieval"}),"."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"s03.png",src:t(45689).Z+"",width:"1288",height:"1450"})}),"\n",(0,r.jsx)(n.p,{children:"Let's load our vectorDB."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\nimport openai\nimport sys\nsys.path.append('../..')\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n\nopenai.api_key  = os.environ['OPENAI_API_KEY']\n"})}),"\n",(0,r.jsx)(n.p,{children:"The code below was added to assign the openai LLM version filmed until it is deprecated, currently in Sept 2023.\nLLM responses can often vary, but the responses may be significantly different when using a different model version."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import datetime\ncurrent_date = datetime.datetime.now().date()\nif current_date < datetime.date(2023, 9, 2):\n    llm_name = "gpt-3.5-turbo-0301"\nelse:\n    llm_name = "gpt-3.5-turbo"\nprint(llm_name)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\npersist_directory = 'docs/chroma/'\nembedding = OpenAIEmbeddings()\nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n\nprint(vectordb._collection.count()) # 209.\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "What are major topics for this class?"\ndocs = vectordb.similarity_search(question,k=3)\nlen(docs) # 3.\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nllm_name = "gpt-3.4-turbo"\nllm = ChatOpenAI(model_name=llm_name, temperature=0)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"retrievalqa-chain",children:"RetrievalQA chain"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.chains import RetrievalQA\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever()\n)\n\nresult = qa_chain({"query": question})\nresult["result"]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"prompt",children:"Prompt"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.prompts import PromptTemplate\n\n# Build prompt\ntemplate = """Use the following pieces of context to answer the question at the end.\nIf you don\'t know the answer, just say that you don\'t know, don\'t try to make up an\nanswer. Use three sentences maximum. Keep the answer as concise as possible. Always\nsay "thanks for asking!" at the end of the answer.\n{context}\nQuestion: {question}\nHelpful Answer:"""\nQA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Run chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}\n)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "Is probability a class topic?"\nresult = qa_chain({"query": question})\nresult["result"]\nresult["source_documents"][0]\n'})}),"\n",(0,r.jsx)(n.p,{children:"Three additional methods: Map_reduce, Refine, Map_rerank."}),"\n",(0,r.jsx)(n.h3,{id:"retrievalqa-chain-types",children:"RetrievalQA chain types"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'qa_chain_mr = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    chain_type="map_reduce"\n)\n\nresult = qa_chain_mr({"query": question})\nresult["result"]\n'})}),"\n",(0,r.jsxs)(n.p,{children:["If you wish to experiment on the ",(0,r.jsx)(n.code,{children:"LangChain plus platform"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Go to ",(0,r.jsx)(n.a,{href:"https://www.langchain.plus/",children:"langchain plus platform"})," and sign up"]}),"\n",(0,r.jsx)(n.li,{children:"Create an API key from your account's settings"}),"\n",(0,r.jsx)(n.li,{children:"Use this API key in the code below"}),"\n",(0,r.jsx)(n.li,{children:"uncomment the code"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Note, the endpoint in the video differs from the one below. Use the one below."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#import os\n#os.environ["LANGCHAIN_TRACING_V2"] = "true"\n#os.environ["LANGCHAIN_ENDPOINT"] = "https://api.langchain.plus"\n#os.environ["LANGCHAIN_API_KEY"] = LCP_API_KEY # replace dots with your api key\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'qa_chain_mr = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    chain_type="map_reduce"\n)\nresult = qa_chain_mr({"query": question})\nresult["result"]\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'qa_chain_mr = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    chain_type="refine"\n)\nresult = qa_chain_mr({"query": question})\nresult["result"]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"retrievalqa-limitations",children:"RetrievalQA limitations"}),"\n",(0,r.jsx)(n.p,{children:"QA fails to preserve conversational history."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"qa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever()\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "Is probability a class topic?"\nresult = qa_chain({"query": question})\nresult["result"]\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "why are those prerequesites needed?"\nresult = qa_chain({"query": question})\nresult["result"]\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Note, The LLM response varies. Some responses ",(0,r.jsx)(n.strong,{children:"do"})," include a reference to probability which might be gleaned from referenced documents. The point is simply that the model does not have access to past questions or answers, this will be covered in the next section."]}),"\n",(0,r.jsx)(n.h2,{id:"chat",children:"Chat"}),"\n",(0,r.jsx)(n.p,{children:"Recall the overall workflow for retrieval augmented generation (RAG):"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"overview.jpeg",src:t(82502).Z+"",width:"2468",height:"707"})}),"\n",(0,r.jsxs)(n.p,{children:["We discussed ",(0,r.jsx)(n.code,{children:"Document Loading"})," and ",(0,r.jsx)(n.code,{children:"Splitting"})," as well as ",(0,r.jsx)(n.code,{children:"Storage"})," and ",(0,r.jsx)(n.code,{children:"Retrieval"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["We then showed how ",(0,r.jsx)(n.code,{children:"Retrieval"})," can be used for output generation in Q+A using ",(0,r.jsx)(n.code,{children:"RetrievalQA"})," chain."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\nimport openai\nimport sys\nsys.path.append('../..')\n\nimport panel as pn  # GUI\npn.extension()\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n\nopenai.api_key  = os.environ['OPENAI_API_KEY']\n"})}),"\n",(0,r.jsx)(n.p,{children:"The code below was added to assign the openai LLM version filmed until it is deprecated, currently in Sept 2023.\nLLM responses can often vary, but the responses may be significantly different when using a different model version."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import datetime\ncurrent_date = datetime.datetime.now().date()\nif current_date < datetime.date(2023, 9, 2):\n    llm_name = "gpt-3.5-turbo-0301"\nelse:\n    llm_name = "gpt-3.5-turbo"\nprint(llm_name)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["If you wish to experiment on ",(0,r.jsx)(n.code,{children:"LangChain plus platform"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Go to ",(0,r.jsx)(n.a,{href:"https://www.langchain.plus/",children:"langchain plus platform"})," and sign up"]}),"\n",(0,r.jsx)(n.li,{children:"Create an api key from your account's settings"}),"\n",(0,r.jsx)(n.li,{children:"Use this api key in the code below"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#import os\n#os.environ["LANGCHAIN_TRACING_V2"] = "true"\n#os.environ["LANGCHAIN_ENDPOINT"] = "https://api.langchain.plus"\n#os.environ["LANGCHAIN_API_KEY"] = "..."\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\npersist_directory = 'docs/chroma/'\nembedding = OpenAIEmbeddings()\nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "What are major topics for this class?"\ndocs = vectordb.similarity_search(question,k=3)\nlen(docs)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nllm = ChatOpenAI(model_name=llm_name, temperature=0)\nllm.predict("Hello world!")\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Build prompt\nfrom langchain.prompts import PromptTemplate\ntemplate = """Use the following pieces of context to answer the question at the end. If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer.\n{context}\nQuestion: {question}\nHelpful Answer:"""\nQA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template,)\n\n# Run chain\nfrom langchain.chains import RetrievalQA\nquestion = "Is probability a class topic?"\nqa_chain = RetrievalQA.from_chain_type(llm,\n                                       retriever=vectordb.as_retriever(),\n                                       return_source_documents=True,\n                                       chain_type_kwargs={"prompt": QA_CHAIN_PROMPT})\n\nresult = qa_chain({"query": question})\nresult["result"]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"memory",children:"Memory"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.memory import ConversationBufferMemory\nmemory = ConversationBufferMemory(\n    memory_key="chat_history",\n    return_messages=True\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"conversationalretrievalchain",children:"ConversationalRetrievalChain"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from langchain.chains import ConversationalRetrievalChain\nretriever=vectordb.as_retriever()\nqa = ConversationalRetrievalChain.from_llm(\n    llm,\n    retriever=retriever,\n    memory=memory\n)\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "Is probability a class topic?"\nresult = qa({"question": question})\nresult[\'answer\']\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'question = "why are those prerequesites needed?"\nresult = qa({"question": question})\nresult[\'answer\']\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"s04",src:t(87498).Z+"",width:"1406",height:"1446"})}),"\n",(0,r.jsx)(n.h3,{id:"create-a-chatbot-that-works-on-your-documents",children:"Create a chatbot that works on your documents"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain.document_loaders import TextLoader\nfrom langchain.chains import RetrievalQA,  ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import TextLoader\nfrom langchain.document_loaders import PyPDFLoader\n"})}),"\n",(0,r.jsx)(n.p,{children:"The chatbot code has been updated a bit since filming. The GUI appearance also\nvaries depending on the platform it is running on."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def load_db(file, chain_type, k):\n    # load documents\n    loader = PyPDFLoader(file)\n    documents = loader.load()\n    # split documents\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n    docs = text_splitter.split_documents(documents)\n    # define embedding\n    embeddings = OpenAIEmbeddings()\n    # create vector database from data\n    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n    # define retriever\n    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": k})\n    # create a chatbot chain. Memory is managed externally.\n    qa = ConversationalRetrievalChain.from_llm(\n        llm=ChatOpenAI(model_name=llm_name, temperature=0),\n        chain_type=chain_type,\n        retriever=retriever,\n        return_source_documents=True,\n        return_generated_question=True,\n    )\n    return qa\n\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import panel as pn\nimport param\n\nclass cbfs(param.Parameterized):\n    chat_history = param.List([])\n    answer = param.String("")\n    db_query  = param.String("")\n    db_response = param.List([])\n\n    def __init__(self,  **params):\n        super(cbfs, self).__init__( **params)\n        self.panels = []\n        self.loaded_file = "docs/cs229_lectures/MachineLearning-Lecture01.pdf"\n        self.qa = load_db(self.loaded_file,"stuff", 4)\n\n    def call_load_db(self, count):\n        if count == 0 or file_input.value is None:  # init or no file specified :\n            return pn.pane.Markdown(f"Loaded File: {self.loaded_file}")\n        else:\n            file_input.save("temp.pdf")  # local copy\n            self.loaded_file = file_input.filename\n            button_load.button_style="outline"\n            self.qa = load_db("temp.pdf", "stuff", 4)\n            button_load.button_style="solid"\n        self.clr_history()\n        return pn.pane.Markdown(f"Loaded File: {self.loaded_file}")\n\n    def convchain(self, query):\n        if not query:\n            return pn.WidgetBox(pn.Row(\'User:\', pn.pane.Markdown("", width=600)), scroll=True)\n        result = self.qa({"question": query, "chat_history": self.chat_history})\n        self.chat_history.extend([(query, result["answer"])])\n        self.db_query = result["generated_question"]\n        self.db_response = result["source_documents"]\n        self.answer = result[\'answer\']\n        self.panels.extend([\n            pn.Row(\'User:\', pn.pane.Markdown(query, width=600)),\n            pn.Row(\'ChatBot:\', pn.pane.Markdown(self.answer, width=600, style={\'background-color\': \'#F6F6F6\'}))\n        ])\n        inp.value = \'\'  #clears loading indicator when cleared\n        return pn.WidgetBox(*self.panels,scroll=True)\n\n    @param.depends(\'db_query \', )\n    def get_lquest(self):\n        if not self.db_query :\n            return pn.Column(\n                pn.Row(pn.pane.Markdown(f"Last question to DB:", styles={\'background-color\': \'#F6F6F6\'})),\n                pn.Row(pn.pane.Str("no DB accesses so far"))\n            )\n        return pn.Column(\n            pn.Row(pn.pane.Markdown(f"DB query:", styles={\'background-color\': \'#F6F6F6\'})),\n            pn.pane.Str(self.db_query )\n        )\n\n    @param.depends(\'db_response\', )\n    def get_sources(self):\n        if not self.db_response:\n            return\n        rlist=[pn.Row(pn.pane.Markdown(f"Result of DB lookup:", styles={\'background-color\': \'#F6F6F6\'}))]\n        for doc in self.db_response:\n            rlist.append(pn.Row(pn.pane.Str(doc)))\n        return pn.WidgetBox(*rlist, width=600, scroll=True)\n\n    @param.depends(\'convchain\', \'clr_history\')\n    def get_chats(self):\n        if not self.chat_history:\n            return pn.WidgetBox(pn.Row(pn.pane.Str("No History Yet")), width=600, scroll=True)\n        rlist=[pn.Row(pn.pane.Markdown(f"Current Chat History variable", styles={\'background-color\': \'#F6F6F6\'}))]\n        for exchange in self.chat_history:\n            rlist.append(pn.Row(pn.pane.Str(exchange)))\n        return pn.WidgetBox(*rlist, width=600, scroll=True)\n\n    def clr_history(self,count=0):\n        self.chat_history = []\n        return\n'})}),"\n",(0,r.jsx)(n.h3,{id:"create-a-chatbot",children:"Create a chatbot"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"cb = cbfs()\n\nfile_input = pn.widgets.FileInput(accept='.pdf')\nbutton_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\nbutton_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\nbutton_clearhistory.on_click(cb.clr_history)\ninp = pn.widgets.TextInput( placeholder='Enter text here\u2026')\n\nbound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\nconversation = pn.bind(cb.convchain, inp)\n\njpg_pane = pn.pane.Image( './img/chat-with-your-data/convchain.jpg')\n\ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=300),\n    pn.layout.Divider(),\n)\ntab2= pn.Column(\n    pn.panel(cb.get_lquest),\n    pn.layout.Divider(),\n    pn.panel(cb.get_sources ),\n)\ntab3= pn.Column(\n    pn.panel(cb.get_chats),\n    pn.layout.Divider(),\n)\ntab4=pn.Column(\n    pn.Row( file_input, button_load, bound_button_load),\n    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n    pn.layout.Divider(),\n    pn.Row(jpg_pane.clone(width=400))\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n)\ndashboard\n"})}),"\n",(0,r.jsx)(n.p,{children:"Feel free to copy this code and modify it to add your own features."}),"\n",(0,r.jsxs)(n.p,{children:["You can try alternate memory and retriever models by changing the configuration in ",(0,r.jsx)(n.code,{children:"load_db"})," function and the ",(0,r.jsx)(n.code,{children:"convchain"})," method. ",(0,r.jsx)(n.a,{href:"https://panel.holoviz.org/",children:"Panel"})," and ",(0,r.jsx)(n.a,{href:"https://param.holoviz.org/",children:"Param"})," have many useful features and widgets you can use to extend the GUI."]}),"\n",(0,r.jsx)(n.h3,{id:"acknowledgments",children:"Acknowledgments"}),"\n",(0,r.jsxs)(n.p,{children:["Panel based chatbot inspired by Sophia Yang, ",(0,r.jsx)(n.a,{href:"https://github.com/sophiamyang/tutorials-LangChain",children:"github"})]})]})}function h(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},82502:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/overview-9fbd91de9282eb806bda1c6db501ecec.jpeg"},72366:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/s01-c1ac1b7929c1b8c50539b226351b6640.png"},23330:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/s02-9fbd91de9282eb806bda1c6db501ecec.jpeg"},45689:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/s03-9f27f25e9fb347952b8443e88276c0f5.png"},87498:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/s04-5ca3896cff25c799faacd320c011869b.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>o,a:()=>s});var r=t(67294);const a={},i=r.createContext(a);function s(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);