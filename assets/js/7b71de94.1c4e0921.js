"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[4087],{17929:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>h});var a=t(85893),o=t(11151);const i={sidebar_label:"LangChain for LLM Application Development",sidebar_position:6,tags:["LangChain","Prompts","Chains","Agents","DeepLearning.AI"]},r="LangChain for LLM Application Development",s={id:"courses/deeplearning-ai/langchain-1",title:"LangChain for LLM Application Development",description:"2023-08-25",source:"@site/docs/courses/deeplearning-ai/langchain-1.md",sourceDirName:"courses/deeplearning-ai",slug:"/courses/deeplearning-ai/langchain-1",permalink:"/docs/courses/deeplearning-ai/langchain-1",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/deeplearning-ai/langchain-1.md",tags:[{label:"LangChain",permalink:"/docs/tags/lang-chain"},{label:"Prompts",permalink:"/docs/tags/prompts"},{label:"Chains",permalink:"/docs/tags/chains"},{label:"Agents",permalink:"/docs/tags/agents"},{label:"DeepLearning.AI",permalink:"/docs/tags/deep-learning-ai"}],version:"current",sidebarPosition:6,frontMatter:{sidebar_label:"LangChain for LLM Application Development",sidebar_position:6,tags:["LangChain","Prompts","Chains","Agents","DeepLearning.AI"]},sidebar:"tutorialSidebar",previous:{title:"ChatGPT Prompt Engineering for Developers",permalink:"/docs/courses/deeplearning-ai/chatgpt-prompt-engineering"},next:{title:"Pair Programming with a Large Language Model",permalink:"/docs/courses/deeplearning-ai/pair-prog-with-llm"}},l={},h=[{value:"Introduction",id:"introduction",level:2},{value:"Models, Prompts and Parsers",id:"models-prompts-and-parsers",level:2},{value:"Setup",id:"setup",level:3},{value:"Chat API: OpenAI",id:"chat-api-openai",level:3},{value:"Chat API: LangChain",id:"chat-api-langchain",level:3},{value:"Model",id:"model",level:4},{value:"Prompt template",id:"prompt-template",level:4},{value:"Output Parsers",id:"output-parsers",level:3},{value:"Parse the LLM output string into a Python dictionary",id:"parse-the-llm-output-string-into-a-python-dictionary",level:3},{value:"Memory",id:"memory",level:2},{value:"Memory Types",id:"memory-types",level:3},{value:"Setup",id:"setup-1",level:3},{value:"ConversationBufferMemory",id:"conversationbuffermemory",level:3},{value:"ConversationBufferWindowMemory",id:"conversationbufferwindowmemory",level:3},{value:"ConversationTokenBufferMemory",id:"conversationtokenbuffermemory",level:3},{value:"ConversationSummaryMemory",id:"conversationsummarymemory",level:3},{value:"Chains",id:"chains",level:2},{value:"Setup",id:"setup-2",level:3},{value:"LLMChain",id:"llmchain",level:3},{value:"SimpleSequentialChain",id:"simplesequentialchain",level:3},{value:"SequentialChain",id:"sequentialchain",level:4},{value:"Router Chain",id:"router-chain",level:3},{value:"Questions and Answer",id:"questions-and-answer",level:2},{value:"Setup",id:"setup-3",level:3},{value:"Step By Step",id:"step-by-step",level:3},{value:"Evaluation",id:"evaluation",level:2},{value:"Setup",id:"setup-4",level:3},{value:"Create our QandA application",id:"create-our-qanda-application",level:3},{value:"Coming up with test datapoints",id:"coming-up-with-test-datapoints",level:3},{value:"Hard-coded examples",id:"hard-coded-examples",level:3},{value:"LLM-Generated examples",id:"llm-generated-examples",level:3},{value:"Combine examples",id:"combine-examples",level:3},{value:"Manual Evaluation",id:"manual-evaluation",level:3},{value:"LLM assisted evaluation",id:"llm-assisted-evaluation",level:3},{value:"LangChain evaluation platform",id:"langchain-evaluation-platform",level:3},{value:"Agents",id:"agents",level:2},{value:"Setup",id:"setup-5",level:3},{value:"Built-in LangChain tools",id:"built-in-langchain-tools",level:3},{value:"Wikipedia example",id:"wikipedia-example",level:3},{value:"Python Agent",id:"python-agent",level:3},{value:"View detailed outputs of the chains",id:"view-detailed-outputs-of-the-chains",level:3},{value:"Define your own tool",id:"define-your-own-tool",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"See also",id:"see-also",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"langchain-for-llm-application-development",children:"LangChain for LLM Application Development"}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:"2023-08-25"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Short course",(0,a.jsx)("br",{}),"\n",(0,a.jsx)(n.a,{href:"https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/",children:"LangChain for LLM Application Development"}),(0,a.jsx)("br",{}),"\nDeepLearning.AI",(0,a.jsx)("br",{}),"\nHarrison Chase, Andrew Ng"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Learn LangChain directly from the creator of the framework, Harrison Chase."}),"\n",(0,a.jsx)(n.li,{children:"Apply LLMs to your proprietary data to build personal assistants and specialized chatbots."}),"\n",(0,a.jsx)(n.li,{children:"Use agents, chained calls, and memories to expand your use of LLMs."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://python.langchain.com",children:"LangChain"})," is a framework\nfor developing applications powered by language models."]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"https://github.com/langchain-ai/langchain",children:"LangChain repository"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"models-prompts-and-parsers",children:"Models, Prompts and Parsers"}),"\n",(0,a.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,a.jsxs)(n.p,{children:["Get your ",(0,a.jsx)(n.a,{href:"https://platform.openai.com/account/api-keys",children:"OpenAI API Key"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"!pip install python-dotenv\n!pip install openai\n\nimport os\nimport openai\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nopenai.api_key = os.environ['OPENAI_API_KEY']\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# account for deprecation of LLM model\nimport datetime\n# Get the current date\ncurrent_date = datetime.datetime.now().date()\n\n# Define the date after which the model should be set to "gpt-3.5-turbo"\ntarget_date = datetime.date(2024, 6, 12)\n\n# Set the model variable based on the current date\nif current_date > target_date:\n    llm_model = "gpt-3.5-turbo"\nelse:\n    llm_model = "gpt-3.5-turbo-0301"\n'})}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"https://platform.openai.com/docs/models/gpt-3-5",children:"GPT-3.5"})," for the difference between\n",(0,a.jsx)(n.code,{children:"gpt-3.5-turbo"})," and ",(0,a.jsx)(n.code,{children:"gpt-3.5-turbo-0301"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"chat-api-openai",children:"Chat API: OpenAI"}),"\n",(0,a.jsxs)(n.p,{children:["Let's start with a direct API calls to OpenAI. See ",(0,a.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference",children:"OpenAI API Reference"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def get_completion(prompt, model=llm_model):\n    messages = [{"role": "user", "content": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0, \n    )\n    return response.choices[0].message["content"]\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"get_completion(\"What is 1+1?\")\n# 'As an AI language model, I can tell you that the answer to 1+1 is 2.'\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'customer_email = """\nArrr, I be fuming that me blender lid \\\nflew off and splattered me kitchen walls \\\nwith smoothie! And to make matters worse,\\\nthe warranty don\'t cover the cost of \\\ncleaning up me kitchen. I need yer help \\\nright now, matey!\n"""\n\nstyle = """American English \\\nin a calm and respectful tone\n"""\n\nprompt = f"""Translate the text \\\nthat is delimited by triple backticks \ninto a style that is {style}.\ntext: ```{customer_email}```\n"""\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"response = get_completion(prompt)\nresponse\n# 'I am quite upset that my blender lid came off and caused my smoothie to splatter all over \n# my kitchen walls. Additionally, the warranty does not cover the cost of cleaning up the \n# mess. Would you be able to assist me at this time, my friend? Thank you kindly.'\n"})}),"\n",(0,a.jsx)(n.h3,{id:"chat-api-langchain",children:"Chat API: LangChain"}),"\n",(0,a.jsx)(n.p,{children:"Let's try how we can do the same using LangChain."}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"https://api.python.langchain.com/en/latest/api_reference.html",children:"langchain API Reference"}),"\nfor details about ",(0,a.jsx)(n.code,{children:"ChatOpenAI"})," and ",(0,a.jsx)(n.code,{children:"ChatPromptTemplate"}),"."]}),"\n",(0,a.jsx)(n.h4,{id:"model",children:"Model"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"!pip install --upgrade langchain\n\nfrom langchain.chat_models import ChatOpenAI\n\n# To control the randomness and creativity of the generated\n# text by an LLM, use temperature = 0.0\nchat = ChatOpenAI(temperature=0.0, model=llm_model)\nchat\n# ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, \n# client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, \n# model_name='gpt-3.5-turbo-0301', temperature=0.0, model_kwargs={}, \n# openai_api_key=None, openai_api_base=None, openai_organization=None, \n# request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)\n"})}),"\n",(0,a.jsx)(n.h4,{id:"prompt-template",children:"Prompt template"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'template_string = """Translate the text \\\nthat is delimited by triple backticks \\\ninto a style that is {style}. \\\ntext: ```{text}```\n"""\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from langchain.prompts import ChatPromptTemplate\n\nprompt_template = ChatPromptTemplate.from_template(template_string)\nprompt_template.messages[0].prompt\n# PromptTemplate(\n#   input_variables=['style', 'text'], output_parser=None, partial_variables={}, \n#   template='Translate the text that is delimited by triple backticks into a \n#   style that is {style}. text: ```{text}```\\n', \n#   template_format='f-string', validate_template=True)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'customer_style = """American English in a calm and respectful tone"""\n\ncustomer_email = """\nArrr, I be fuming that me blender lid \\\nflew off and splattered me kitchen walls \\\nwith smoothie! And to make matters worse, \\\nthe warranty don\'t cover the cost of \\\ncleaning up me kitchen. I need yer help \\\nright now, matey!\n"""\n\ncustomer_messages = prompt_template.format_messages(\n                    style=customer_style,\n                    text=customer_email)\n\nprint(type(customer_messages))\nprint(type(customer_messages[0]))\n# <class \'list\'>\n# <class \'langchain.schema.HumanMessage\'>\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'print(customer_messages[0])\n# content="Translate the text that is delimited by triple backticks into a style that is \n#   American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that \n#   me blender lid flew off and splattered me kitchen walls with smoothie! And to make \n#   matters worse, the warranty don\'t cover the cost of cleaning up me kitchen. I need \n#   yer help right now, matey!\\n```\\n" additional_kwargs={} example=False\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Call the LLM to translate to the style of the customer message\ncustomer_response = chat(customer_messages)\n\nprint(customer_response.content)\n# I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls \n# with smoothie. To add to my frustration, the warranty doesn't cover the cost of cleaning \n# up my kitchen. Can you please help me out, friend?\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'service_reply = """Hey there customer, \\\nthe warranty does not cover \\\ncleaning expenses for your kitchen \\\nbecause it\'s your fault that \\\nyou misused your blender \\\nby forgetting to put the lid on before \\\nstarting the blender. \\\nTough luck! See ya!\n"""\n\nservice_style_pirate = """\\\na polite tone \\\nthat speaks in English Pirate\\\n"""\n\nservice_messages = prompt_template.format_messages(\n    style=service_style_pirate,\n    text=service_reply)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"service_response = chat(service_messages)\nprint(service_response.content)\n# Ahoy there, me hearty customer! I be sorry to inform ye that the warranty be not coverin' \n# the expenses o' cleaning yer galley, as 'tis yer own fault fer misusin' yer blender by \n# forgettin' to put the lid on afore startin' it. Aye, tough luck! Farewell and may the winds \n# be in yer favor!\n"})}),"\n",(0,a.jsx)(n.h3,{id:"output-parsers",children:"Output Parsers"}),"\n",(0,a.jsx)(n.p,{children:"Let's start with defining how we would like the LLM output to look like:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'{\n  "gift": False,\n  "delivery_days": 5,\n  "price_value": "pretty affordable!"\n}\n\ncustomer_review = """\\\nThis leaf blower is pretty amazing.  It has four settings:\\\ncandle blower, gentle breeze, windy city, and tornado. \\\nIt arrived in two days, just in time for my wife\'s \\\nanniversary present. \\\nI think my wife liked it so much she was speechless. \\\nSo far I\'ve been the only one using it, and I\'ve been \\\nusing it every other morning to clear the leaves on our lawn. \\\nIt\'s slightly more expensive than the other leaf blowers \\\nout there, but I think it\'s worth it for the extra features.\n"""\n\nreview_template = """\\\nFor the following text, extract the following information:\n\ngift: Was the item purchased as a gift for someone else? \\\nAnswer True if yes, False if not or unknown.\n\ndelivery_days: How many days did it take for the product \\\nto arrive? If this information is not found, output -1.\n\nprice_value: Extract any sentences about the value or price,\\\nand output them as a comma separated Python list.\n\nFormat the output as JSON with the following keys:\ngift\ndelivery_days\nprice_value\n\ntext: {text}\n"""\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from langchain.prompts import ChatPromptTemplate\n\nprompt_template = ChatPromptTemplate.from_template(review_template)\nprint(prompt_template)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"messages = prompt_template.format_messages(text=customer_review)\nchat = ChatOpenAI(temperature=0.0, model=llm_model)\nresponse = chat(messages)\n\nprint(response.content)\n# {\n#     \"gift\": true,\n#     \"delivery_days\": 2,\n#     \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, \n#        but I think it's worth it for the extra features.\"]\n# }\n\ntype(response.content)\n# str\n\n# You will get an error by running this line of code because 'gift' is not a dictionary\n# 'gift' is a string\nresponse.content.get('gift')\n"})}),"\n",(0,a.jsx)(n.h3,{id:"parse-the-llm-output-string-into-a-python-dictionary",children:"Parse the LLM output string into a Python dictionary"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from langchain.output_parsers import ResponseSchema\nfrom langchain.output_parsers import StructuredOutputParser\n"})}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"https://api.python.langchain.com/en/latest/api_reference.html",children:"langchain API Reference"}),"\nfor details about ",(0,a.jsx)(n.code,{children:"ResponseSchema"})," and ",(0,a.jsx)(n.code,{children:"StructuredOutputParser"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'gift_schema = ResponseSchema(name="gift",\n                             description="Was the item purchased\\\n                             as a gift for someone else? \\\n                             Answer True if yes,\\\n                             False if not or unknown.")\ndelivery_days_schema = ResponseSchema(name="delivery_days",\n                                      description="How many days\\\n                                      did it take for the product\\\n                                      to arrive? If this \\\n                                      information is not found,\\\n                                      output -1.")\nprice_value_schema = ResponseSchema(name="price_value",\n                                    description="Extract any\\\n                                    sentences about the value or \\\n                                    price, and output them as a \\\n                                    comma separated Python list.")\n\nresponse_schemas = [gift_schema, \n                    delivery_days_schema,\n                    price_value_schema]\n\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"format_instructions = output_parser.get_format_instructions()\nprint(format_instructions)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing "\\`\\`\\`json" and "\\`\\`\\`":\n\n``json\n{\n    "gift": string  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n    "delivery_days": string  // How many days did it take for the product to arrive? If this information is not found, output -1.\n    "price_value": string  // Extract any sentences about the value or price, and output them as a comma separated Python list.\n}\n``\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'review_template_2 = """\\\nFor the following text, extract the following information:\n\ngift: Was the item purchased as a gift for someone else? \\\nAnswer True if yes, False if not or unknown.\n\ndelivery_days: How many days did it take for the product\\\nto arrive? If this information is not found, output -1.\n\nprice_value: Extract any sentences about the value or price,\\\nand output them as a comma separated Python list.\n\ntext: {text}\n\n{format_instructions}\n"""\n\nprompt = ChatPromptTemplate.from_template(template=review_template_2)\n\nmessages = prompt.format_messages(text=customer_review, \n                                  format_instructions=format_instructions)\nprint(messages[0].content)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'response = chat(messages)\nprint(response.content)\n# ```json\n# {\n#   "gift": true,\n#   "delivery_days": "2",\n#   "price_value": ["It\'s slightly more expensive than the other leaf blowers out there, but I think it\'s worth it for the extra features."]\n# }\n# ```\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"output_dict = output_parser.parse(response.content)\noutput_dict\n# {'gift': True,\n#  'delivery_days': '2',\n#  'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n# }\n\ntype(output_dict)\n# dict\n\noutput_dict.get('delivery_days')\n# '2'\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html",children:"Language Models Perform Reasoning via Chain of Thought"}),".\nThis method enables models to decompose multi-step problems into intermediate steps."]}),"\n",(0,a.jsx)(n.h2,{id:"memory",children:"Memory"}),"\n",(0,a.jsxs)(n.p,{children:["Memory maintains Chain state, incorporating context from past runs.\nSee ",(0,a.jsx)(n.a,{href:"https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.memory",children:"langchain.memory"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"memory-types",children:"Memory Types"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"ConversationBufferMemory"}),": This memory allows for storing of messages and then extracts the messages in a variable."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"ConversationBufferWindowMemory"}),": This memory keeps a list of the interactions of the comversation over time. It only uses the last K interactions."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"ConversationTokenBufferMemory"}),": This memory keeps a buffer of recente interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"ConversationSummaryMomery"}),": This memory creates a summary of the conversation over time."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Additional Memory Types (supported by LangChain):"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Vectore data memory: Stores text (from conversation or elsewhere) in a vectore database and retrieves the most relevant blocks of text."}),"\n",(0,a.jsx)(n.li,{children:"Entity memories: Usinh an LLM, it remembers details about specific entities."}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"setup-1",children:"Setup"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n\nimport warnings\nwarnings.filterwarnings('ignore')\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# account for deprecation of LLM model\nimport datetime\n# Get the current date\ncurrent_date = datetime.datetime.now().date()\n\n# Define the date after which the model should be set to "gpt-3.5-turbo"\ntarget_date = datetime.date(2024, 6, 12)\n\n# Set the model variable based on the current date\nif current_date > target_date:\n    llm_model = "gpt-3.5-turbo"\nelse:\n    llm_model = "gpt-3.5-turbo-0301"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"conversationbuffermemory",children:"ConversationBufferMemory"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"ConversationChain"})," is a chain to have a conversation and load context from memory.\n",(0,a.jsx)(n.code,{children:"ConversationBufferMemory"})," is a buffer for storing conversation memory.\nSee ",(0,a.jsx)(n.a,{href:"https://api.python.langchain.com/en/latest/api_reference.html",children:"langchain API Reference"}),"\nfor more."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"llm = ChatOpenAI(temperature=0.0, model=llm_model)\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(\n    llm=llm, \n    memory = memory,\n    verbose=True\n)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'conversation.predict(input="Hi, my name is Andrew")\nconversation.predict(input="What is 1+1?")\nconversation.predict(input="What is my name?")\nprint(memory.buffer)\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"Human: Hi, my name is Andrew\nAI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\nHuman: What is 1+1?\nAI: The answer to 1+1 is 2.\nHuman: What is my name?\nAI: Your name is Andrew, as you mentioned earlier.\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"memory.load_memory_variables({})\n"})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew, it's nice to meet you. \nMy name is AI. How can I assist you today?\\nHuman: What is 1+1?\\nAI: The answer to \n1+1 is 2.\\nHuman: What is my name?\\nAI: Your name is Andrew, as you mentioned \nearlier.\"}\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'memory = ConversationBufferMemory()\nmemory.save_context({"input": "Hi"}, \n                    {"output": "What\'s up"})\nprint(memory.buffer)\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"Human: Hi\nAI: What's up\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"memory.load_memory_variables({})\n"})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"{'history': \"Human: Hi\\nAI: What's up\"}\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'memory.save_context({"input": "Not much, just hanging"}, \n                    {"output": "Cool"})\nmemory.load_memory_variables({})\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"conversationbufferwindowmemory",children:"ConversationBufferWindowMemory"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.memory import ConversationBufferWindowMemory\n\nmemory = ConversationBufferWindowMemory(k=1)               \nmemory.save_context({"input": "Hi"},\n                    {"output": "What\'s up"})\nmemory.save_context({"input": "Not much, just hanging"},\n                    {"output": "Cool"})\n\u200b\nmemory.load_memory_variables({})\n# {\'history\': \'Human: Not much, just hanging\\nAI: Cool\'}\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"ConversationBufferWindowMemory"})," is a buffer for storing conversation memory\ninside a limited size window. Here, ",(0,a.jsx)(n.code,{children:"k=1"})," is the number of messages to store in buffer.\nSee ",(0,a.jsx)(n.a,{href:"https://api.python.langchain.com/en/latest/api_reference.html",children:"langchain API Reference"}),"\nfor more."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'llm = ChatOpenAI(temperature=0.0, model=llm_model)\nmemory = ConversationBufferWindowMemory(k=1)\nconversation = ConversationChain(\n    llm=llm, \n    memory = memory,\n    verbose=False\n)\n\nconversation.predict(input="Hi, my name is Andrew")\n# "Hello Andrew, it\'s nice to meet you. My name is AI. How can I assist you today?"\nconversation.predict(input="What is 1+1?")\n# \'The answer to 1+1 is 2.\'\nconversation.predict(input="What is my name?")\n# "I\'m sorry, I don\'t have access to that information. Could you please tell me your name?"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"conversationtokenbuffermemory",children:"ConversationTokenBufferMemory"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"ConversationTokenBufferMemory"})," is a conversation chat memory with token limit."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'!pip install tiktoken\n\nfrom langchain.memory import ConversationTokenBufferMemory\nfrom langchain.llms import OpenAI\nllm = ChatOpenAI(temperature=0.0, model=llm_model)\n\nmemory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\nmemory.save_context({"input": "AI is what?!"},\n                    {"output": "Amazing!"})\nmemory.save_context({"input": "Backpropagation is what?"},\n                    {"output": "Beautiful!"})\nmemory.save_context({"input": "Chatbots are what?"}, \n                    {"output": "Charming!"})\n\nmemory.load_memory_variables({})\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"conversationsummarymemory",children:"ConversationSummaryMemory"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"ConversationSummaryMemory"})," is a conversation summarizer to chat memory."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.memory import ConversationSummaryBufferMemory\n\n# create a long string\nschedule = "There is a meeting at 8am with your product team. \\\nYou will need your powerpoint presentation prepared. \\\n9am-12pm have time to work on your LangChain \\\nproject which will go quickly because Langchain is such a powerful tool. \\\nAt Noon, lunch at the italian resturant with a customer who is driving \\\nfrom over an hour away to meet you to understand the latest in AI. \\\nBe sure to bring your laptop to show the latest LLM demo."\n\nmemory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\nmemory.save_context({"input": "Hello"}, {"output": "What\'s up"})\nmemory.save_context({"input": "Not much, just hanging"},\n                    {"output": "Cool"})\nmemory.save_context({"input": "What is on the schedule today?"}, \n                    {"output": f"{schedule}"})\n\nmemory.load_memory_variables({})\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"{'history': \"System: The human and AI engage in small talk before discussing the day's schedule. \nThe AI informs the human of a morning meeting with the product team, time to work on the \nLangChain project, and a lunch meeting with a customer interested in the latest AI \ndevelopments.\"}\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'conversation = ConversationChain(\n    llm=llm, \n    memory = memory,\n    verbose=True\n)\n\nconversation.predict(input="What would be a good demo to show?")\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"> Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative \nand provides lots of specific details from its context. If the AI does not know the answer \nto a question, it truthfully says it does not know.\n\nCurrent conversation:\nSystem: The human and AI engage in small talk before discussing the day's schedule. The AI \ninforms the human of a morning meeting with the product team, time to work on the LangChain \nproject, and a lunch meeting with a customer interested in the latest AI developments.\nHuman: What would be a good demo to show?\nAI:\n\n> Finished chain.\n\n\"Based on the customer's interest in AI developments, I would suggest showcasing our latest \nnatural language processing capabilities. We could demonstrate how our AI can accurately \nunderstand and respond to complex language queries, and even provide personalized recommendations \nbased on the user's preferences. Additionally, we could highlight our AI's ability to learn \nand adapt over time, making it a valuable tool for businesses looking to improve their \ncustomer experience.\"\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"memory.load_memory_variables({})\n"})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"{'history': \"System: The human and AI engage in small talk before discussing the day's schedule. \nThe AI informs the human of a morning meeting with the product team, time to work on the \nLangChain project, and a lunch meeting with a customer interested in the latest AI developments. \nThe human asks what would be a good demo to show.\\nAI: Based on the customer's interest in AI \ndevelopments, I would suggest showcasing our latest natural language processing capabilities. We\ncould demonstrate how our AI can accurately understand and respond to complex language queries, \nand even provide personalized recommendations based on the user's preferences. Additionally, we \ncould highlight our AI's ability to learn and adapt over time, making it a valuable tool for \nbusinesses looking to improve their customer experience.\"}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"chains",children:"Chains"}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:"2023-08-29"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"setup-2",children:"Setup"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import warnings\nwarnings.filterwarnings('ignore')\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# account for deprecation of LLM model\nimport datetime\n# Get the current date\ncurrent_date = datetime.datetime.now().date()\n\n# Define the date after which the model should be set to "gpt-3.5-turbo"\ntarget_date = datetime.date(2024, 6, 12)\n\n# Set the model variable based on the current date\nif current_date > target_date:\n    llm_model = "gpt-3.5-turbo"\nelse:\n    llm_model = "gpt-3.5-turbo-0301"\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import pandas as pd\ndf = pd.read_csv('Data.csv')\nprint(df.head())\n"})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"                   Product                                             Review\n0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n4  Milk Frother Handheld\\n   I loved this product. But they only seem to l...\n"})}),"\n",(0,a.jsx)(n.h3,{id:"llmchain",children:"LLMChain"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import LLMChain\n\nllm = ChatOpenAI(temperature=0.9, model=llm_model)\n\nprompt = ChatPromptTemplate.from_template("What is the best name to describe a company that makes \\\n            {product}?")\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\nproduct = "Queen Size Sheet Set"\nchain.run(product)\n'})}),"\n",(0,a.jsx)(n.p,{children:"A higher temperature value typically makes the output more diverse and creative but might also\nincrease its likelihood of straying from the context. Conversely, a lower temperature value makes\nthe AI's responses more focused and deterministic, sticking closely to the most likely prediction."}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"'\"Royal Linens\" would be a fitting name for a company that makes queen size sheet sets, as it \nimplies luxury and high quality.'\n"})}),"\n",(0,a.jsx)(n.h3,{id:"simplesequentialchain",children:"SimpleSequentialChain"}),"\n",(0,a.jsxs)(n.p,{children:["Sequential chains allow you to connect multiple chains and compose them into pipelines that execute\nsome specific scenario.\nSee ",(0,a.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/foundational/sequential_chains",children:"Sequential"})," for more information."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.chains import SimpleSequentialChain\n\n# prompt template 1\nfirst_prompt = ChatPromptTemplate.from_template(\n    "What is the best name to describe a company that makes {product}?"\n)\n\n# Chain 1\nchain_one = LLMChain(llm=llm, prompt=first_prompt)\n\n# prompt template 2\nsecond_prompt = ChatPromptTemplate.from_template(\n    "Write a 20 words description for the following company:{company_name}"\n)\n\n# chain 2\nchain_two = LLMChain(llm=llm, prompt=second_prompt)\n\noverall_simple_chain = SimpleSequentialChain(\n        chains=[chain_one, chain_two], verbose=True)\n'})}),"\n",(0,a.jsx)(n.h4,{id:"sequentialchain",children:"SequentialChain"}),"\n",(0,a.jsx)(n.p,{children:"The simple sequential chain works well when there's only a single input and a single output.\nBut what about when we have multiple inputs and outputs?"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.chains import SequentialChain\n\nllm = ChatOpenAI(temperature=0.9, model=llm_model)\n\n# prompt template 1: translate to english\nfirst_prompt = ChatPromptTemplate.from_template(\n    "Translate the following review to english:"\n    "\\n\\n{Review}"\n)\n# chain 1: input= Review and output= English_Review\nchain_one = LLMChain(llm=llm, prompt=first_prompt, output_key="English_Review")\n\n# prompt template 2: summarize\nsecond_prompt = ChatPromptTemplate.from_template(\n    "Can you summarize the following review in 1 sentence:"\n    "\\n\\n{English_Review}"\n)\n# chain 2: input= English_Review and output= summary\nchain_two = LLMChain(llm=llm, prompt=second_prompt, \n                     output_key="summary"\n                    )                  \n\n# prompt template 3: translate to english\nthird_prompt = ChatPromptTemplate.from_template(\n    "What language is the following review:\\n\\n{Review}"\n)\xf9\xf9\xf9\xf9\n# chain 3: input= Review and output= language\nchain_three = LLMChain(llm=llm, prompt=third_prompt,\n                       output_key="language"\n                      )\n\n# prompt template 4: follow up message\nfourth_prompt = ChatPromptTemplate.from_template(\n    "Write a follow up response to the following "\n    "summary in the specified language:"\n    "\\n\\nSummary: {summary}\\n\\nLanguage: {language}"\n)\n# chain 4: input= summary, language and output= followup_message\nchain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n                      output_key="followup_message"\n                     )\n\n# overall_chain: input= Review \n# and output= English_Review,summary, followup_message\noverall_chain = SequentialChain(\n    chains=[chain_one, chain_two, chain_three, chain_four],\n    input_variables=["Review"],\n    output_variables=["English_Review", "summary", "followup_message"],\n    verbose=True\n)\n\nreview = df.Review[5]\noverall_chain(review)\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"> Entering new SequentialChain chain...\n\n> Finished chain.\n{'Review': \"Je trouve le go\xfbt m\xe9diocre. La mousse ne tient pas, c'est bizarre. J'ach\xe8te les m\xeames dans le commerce et \nle go\xfbt est bien meilleur...\\nVieux lot ou contrefa\xe7on !?\",\n 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the \n taste is much better... Old batch or counterfeit!?\",\n 'summary': \"The reviewer is disappointed with the taste and foam of the product and questions if it's an old batch or \n counterfeit.\",\n 'followup_message': 'R\xe9ponse : Nous sommes d\xe9sol\xe9s que vous ayez \xe9t\xe9 d\xe9\xe7u par le go\xfbt et la mousse de notre produit. \n Nous vous assurons que nous prenons toutes les mesures n\xe9cessaires pour maintenir la qualit\xe9 constante de nos produits. \n Si vous pensez que vous avez re\xe7u un lot p\xe9rim\xe9 ou contrefait, veuillez nous contacter imm\xe9diatement pour que nous \n puissions enqu\xeater sur la situation et prendre les mesures appropri\xe9es. Nous appr\xe9cions vos commentaires car ils nous \n aident \xe0 am\xe9liorer constamment notre produit. Merci.'}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"router-chain",children:"Router Chain"}),"\n",(0,a.jsxs)(n.p,{children:["In this section, we use the RouterChain paradigm to create a chain that dynamically selects the next chain to use for a\ngiven input. See ",(0,a.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/foundational/router",children:"Router"})," for more information."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'physics_template = """You are a very smart physics professor. \\\nYou are great at answering questions about physics in a concise\\\nand easy to understand manner. \\\nWhen you don\'t know the answer to a question you admit\\\nthat you don\'t know.\n\nHere is a question:\n{input}"""\n\n\nmath_template = """You are a very good mathematician. \\\nYou are great at answering math questions. \\\nYou are so good because you are able to break down \\\nhard problems into their component parts, \nanswer the component parts, and then put them together\\\nto answer the broader question.\n\nHere is a question:\n{input}"""\n\nhistory_template = """You are a very good historian. \\\nYou have an excellent knowledge of and understanding of people,\\\nevents and contexts from a range of historical periods. \\\nYou have the ability to think, reflect, debate, discuss and \\\nevaluate the past. You have a respect for historical evidence\\\nand the ability to make use of it to support your explanations \\\nand judgements.\n\nHere is a question:\n{input}"""\n\ncomputerscience_template = """ You are a successful computer scientist.\\\nYou have a passion for creativity, collaboration,\\\nforward-thinking, confidence, strong problem-solving capabilities,\\\nunderstanding of theories and algorithms, and excellent communication \\\nskills. You are great at answering coding questions. \\\nYou are so good because you know how to solve a problem by \\\ndescribing the solution in imperative steps \\\nthat a machine can easily interpret and you know how to \\\nchoose a solution that has a good balance between \\\ntime complexity and space complexity. \n\nHere is a question:\n{input}"""\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'prompt_infos = [\n    {\n        "name": "physics", \n        "description": "Good for answering questions about physics", \n        "prompt_template": physics_template\n    },\n    {\n        "name": "math", \n        "description": "Good for answering math questions", \n        "prompt_template": math_template\n    },\n    {\n        "name": "History", \n        "description": "Good for answering history questions", \n        "prompt_template": history_template\n    },\n    {\n        "name": "computer science", \n        "description": "Good for answering computer science questions", \n        "prompt_template": computerscience_template\n    }\n]\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.chains.router import MultiPromptChain\nfrom langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\nfrom langchain.prompts import PromptTemplate\n\nllm = ChatOpenAI(temperature=0, model=llm_model)\n\ndestination_chains = {}\nfor p_info in prompt_infos:\n    name = p_info["name"]\n    prompt_template = p_info["prompt_template"]\n    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n    chain = LLMChain(llm=llm, prompt=prompt)\n    destination_chains[name] = chain  \n    \ndestinations = [f"{p[\'name\']}: {p[\'description\']}" for p in prompt_infos]\ndestinations_str = "\\n".join(destinations)\n\ndefault_prompt = ChatPromptTemplate.from_template("{input}")\ndefault_chain = LLMChain(llm=llm, prompt=default_prompt)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'MULTI_PROMPT_ROUTER_TEMPLATE = """Given a raw text input to a \\\nlanguage model select the model prompt best suited for the input. \\\nYou will be given the names of the available prompts and a \\\ndescription of what the prompt is best suited for. \\\nYou may also revise the original input if you think that revising\\\nit will ultimately lead to a better response from the language model.\n\n<< FORMATTING >>\nReturn a markdown code snippet with a JSON object formatted to look like:\n``json\n{{{{\n    "destination": string \\ name of the prompt to use or "DEFAULT"\n    "next_inputs": string \\ a potentially modified version of the original input\n}}}}\n``\n\nREMEMBER: "destination" MUST be one of the candidate prompt \\\nnames specified below OR it can be "DEFAULT" if the input is not\\\nwell suited for any of the candidate prompts.\nREMEMBER: "next_inputs" can just be the original input \\\nif you don\'t think any modifications are needed.\n\n<< CANDIDATE PROMPTS >>\n{destinations}\n\n<< INPUT >>\n{{input}}\n\n<< OUTPUT (remember to include the ```json)>>"""\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n    destinations=destinations_str\n)\nrouter_prompt = PromptTemplate(\n    template=router_template,\n    input_variables=["input"],\n    output_parser=RouterOutputParser(),\n)\n\nrouter_chain = LLMRouterChain.from_llm(llm, router_prompt)\n\nchain = MultiPromptChain(router_chain=router_chain, \n                         destination_chains=destination_chains, \n                         default_chain=default_chain, verbose=True\n                        )\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'chain.run("What is black body radiation?")\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"> Entering new MultiPromptChain chain...\nphysics: {'input': 'What is black body radiation?'}\n> Finished chain.\n\"Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an object that \nabsorbs all radiation that falls on it and emits radiation at all wavelengths. The radiation emitted by a black body \ndepends only on its temperature and follows a specific distribution known as Planck's law. This type of radiation is \nimportant in understanding the behavior of stars, as well as in the development of technologies such as incandescent \nlight bulbs and infrared cameras.\"\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'chain.run("what is 2 + 2")\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"> Entering new MultiPromptChain chain...\nmath: {'input': 'what is 2 + 2'}\n> Finished chain.\n'As an AI language model, I can answer this question easily. The answer to 2 + 2 is 4.'\n"})}),"\n",(0,a.jsx)(n.h2,{id:"questions-and-answer",children:"Questions and Answer"}),"\n",(0,a.jsx)(n.h3,{id:"setup-3",children:"Setup"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'!pip install --upgrade langchain\n!pip install docarray\n\nimport os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n\n# account for deprecation of LLM model\nimport datetime\n# Get the current date\ncurrent_date = datetime.datetime.now().date()\n\n# Define the date after which the model should be set to "gpt-3.5-turbo"\ntarget_date = datetime.date(2024, 6, 12)\n\n# Set the model variable based on the current date\nif current_date > target_date:\n    llm_model = "gpt-3.5-turbo"\nelse:\n    llm_model = "gpt-3.5-turbo-0301"\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import CSVLoader\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom IPython.display import display, Markdown\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"file = 'OutdoorClothingCatalog_1000.csv'\nloader = CSVLoader(file_path=file)\n\nimport pandas as pd\ndf = pd.read_csv(file)\nprint(df.head()) \n"})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"   Unnamed: 0                                               name   \n0           0                           Women's Campside Oxfords  \\\n1           1           Recycled Waterhog Dog Mat, Chevron Weave   \n2           2  Infant and Toddler Girls' Coastal Chill Swimsu...   \n3           3         Refresh Swimwear, V-Neck Tankini Contrasts   \n4           4                             EcoFlex 3L Storm Pants   \n\n                                         description  \n0  This ultracomfortable lace-to-toe Oxford boast...  \n1  Protect your floors from spills and splashing ...  \n2  She'll love the bright colors, ruffles and exc...  \n3  Whether you're going for a swim or heading out...  \n4  Our new TEK O2 technology makes our four-seaso...  \n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.indexes import VectorstoreIndexCreator\n\nindex = VectorstoreIndexCreator(\n    vectorstore_cls=DocArrayInMemorySearch\n).from_loaders([loader])\n\nquery ="Please list all your shirts with sun protection \\\nin a table in markdown and summarize each one."\n\nresponse = index.query(query)\n# display(Markdown(response))\nprint(response)\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"| Name | Description |\n| --- | --- |\n| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated, 100% polyester, wrinkle-resistant, front and back cape venting, two front bellows pockets |\n| Men's Plaid Tropic Shirt, Short-Sleeve | UPF 50+ rated, 52% polyester and 48% nylon, machine washable and dryable, front and back cape venting, two front bellows pockets |\n| Men's TropicVibe Shirt, Short-Sleeve | UPF 50+ rated, 71% Nylon, 29% Polyester, 100% Polyester knit mesh, wrinkle resistant, front and back cape venting, two front bellows pockets |\n| Sun Shield Shirt by | UPF 50+ rated, 78% nylon, 22% Lycra Xtra Life fiber, wicks moisture, fits comfortably over swimsuit, abrasion resistant |\n\nAll four shirts provide UPF 50+ sun protection, blocking 98% of the sun's harmful rays. The Men's Tropical Plaid Short-Sleeve Shirt is made of 100% polyester and is wrinkle-resistant. The Men's Plaid Trop\n"})}),"\n",(0,a.jsx)(n.p,{children:"LLMs on Documents function by inspecting limited portions of text, typically a few thousand words at a time. This is where embeddings and vector stores play a crucial role."}),"\n",(0,a.jsx)(n.p,{children:"An embedding vector captures the essence and meaning of content. Similar content will yield similar vectors in tests. A vector database serves as a repository for these vector representations. We establish a vector database that gets populated with text segments extracted from incoming documents."}),"\n",(0,a.jsx)(n.p,{children:"When dealing with a substantial incoming document, our initial step involves dividing it into smaller segments. This approach generates text portions smaller than the original document, a practical strategy since the entire document might be too extensive for the language model to process in one go. Our aim is to create these compact portions to ensure that only the most pertinent segments are presented to the language model. Subsequently, we generate an embedding for each of these sections and store them within the vector database. This constitutes the process of index creation."}),"\n",(0,a.jsx)(n.p,{children:"With the index in place, we can utilize it in real-time to identify text segments most relevant to an incoming query. Upon receiving a query, we generate an embedding corresponding to that query. This embedding is then compared against all vectors stored in the vector database, from which we select the top n most similar vectors. These selected vectors are returned, and we can present them to the language model through a prompt to obtain a final response. Thus, the aforementioned sequence of actions is accomplished with just a few lines of code."}),"\n",(0,a.jsx)(n.h3,{id:"step-by-step",children:"Step By Step"}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/document_loaders/csv",children:"CSV"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Read the file."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from langchain.document_loaders import CSVLoader\nloader = CSVLoader(file_path=file)\n\ndocs = loader.load()\nprint(docs[0])\n"})}),"\n",(0,a.jsx)(n.p,{children:"Create embeddings."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.embeddings import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\nembed = embeddings.embed_query("Hi my name is Harrison")\nprint(len(embed))\n# 1536\nprint(embed[:5])\n# [-0.021913960576057434, 0.006774206645786762, -0.018190348520874977, -0.039148248732089996, -0.014089343138039112]\n'})}),"\n",(0,a.jsx)(n.p,{children:"Load embeddings into a vector store."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"db = DocArrayInMemorySearch.from_documents(\n    docs, \n    embeddings\n)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Use this vectore store."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'query = "Please suggest a shirt with sunblocking"\ndocs = db.similarity_search(query)\n\nprint(len(docs))\n# 4\nprint(docs[0])\n'})}),"\n",(0,a.jsx)(n.p,{children:"To question our own documents, we need to create a retreiver from this\nvector store."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'retriever = db.as_retriever()\n\nllm = ChatOpenAI(temperature=0.0, model=llm_model)\n\nqdocs = "".join([docs[i].page_content for i in range(len(docs))])\n\nresponse = llm.call_as_llm(f"{qdocs} Question: Please list all your \\\nshirts with sun protection in a table in markdown and summarize each one.") \n\n# display(Markdown(response))\nprint(response)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["All of those steps can be encapsulated with the LangChain chain.\nWe will create a RetreivalQA Chain. See ",(0,a.jsx)(n.a,{href:"https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa",children:"QA using a Retriever"}),"\nfor more information."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'qa_stuff = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type="stuff", \n    retriever=retriever, \n    verbose=True\n)\n\nquery =  "Please list all your shirts with sun protection in a table \\\nin markdown and summarize each one."\n\nresponse = qa_stuff.run(query)\n\n# display(Markdown(response))\nprint(response)\n'})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Stuffing: Simplest method, stuffing all related data into the prompt as context; implemented as StuffDocumentsChain."}),"\n",(0,a.jsx)(n.li,{children:"Map Reduce: Runs an initial prompt on each chunk of data, then combines the outputs; implemented as MapReduceDocumentsChain."}),"\n",(0,a.jsx)(n.li,{children:"Refine: Refines output based on new documents; calls are not independent and cannot be parallelized like MapReduceDocumentsChain."}),"\n",(0,a.jsx)(n.li,{children:"Map-Rerank: Runs an initial prompt on each chunk of data with a score for certainty, ranks responses, and returns the highest score."}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"response = index.query(query, llm=llm)\n\nindex = VectorstoreIndexCreator(\n    vectorstore_cls=DocArrayInMemorySearch,\n    embedding=embeddings,\n).from_loaders([loader])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"evaluation",children:"Evaluation"}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:"2023-08-30"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"setup-4",children:"Setup"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# account for deprecation of LLM model\nimport datetime\n# Get the current date\ncurrent_date = datetime.datetime.now().date()\n\n# Define the date after which the model should be set to "gpt-3.5-turbo"\ntarget_date = datetime.date(2024, 6, 12)\n\n# Set the model variable based on the current date\nif current_date > target_date:\n    llm_model = "gpt-3.5-turbo"\nelse:\n    llm_model = "gpt-3.5-turbo-0301"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"create-our-qanda-application",children:"Create our QandA application"}),"\n",(0,a.jsx)(n.p,{children:"Import necessary modules and classes for setting up a question-and-answer (Q&A) application using the Langchain library."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import CSVLoader\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.vectorstores import DocArrayInMemorySearch\n"})}),"\n",(0,a.jsxs)(n.p,{children:["A CSV file named ",(0,a.jsx)(n.code,{children:"OutdoorClothingCatalog_1000.csv"})," is loaded using the CSVLoader class."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"file = 'OutdoorClothingCatalog_1000.csv'\nloader = CSVLoader(file_path=file)\ndata = loader.load()\n"})}),"\n",(0,a.jsxs)(n.p,{children:["An index is created using the ",(0,a.jsx)(n.a,{href:"https://api.python.langchain.com/en/latest/indexes/langchain.indexes.vectorstore.VectorstoreIndexCreator.html",children:"VectorstoreIndexCreator"})," class."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"index = VectorstoreIndexCreator(\n    vectorstore_cls=DocArrayInMemorySearch\n).from_loaders([loader])\n"})}),"\n",(0,a.jsxs)(n.p,{children:["A Q&A system is set up using the ",(0,a.jsx)(n.code,{children:"ChatOpenAI"})," and ",(0,a.jsx)(n.code,{children:"RetrievalQA"})," classes. An instance of ",(0,a.jsx)(n.code,{children:"ChatOpenAI"})," is created with a temperature\nof 0.0 and a model called ",(0,a.jsx)(n.code,{children:"llm_model"}),". The ",(0,a.jsx)(n.code,{children:"RetrievalQA.from_chain_type"}),' function is used to create a Q&A system with a\nlanguage model (LLM), a chain type named "stuff", a retriever based on the previously created index,\nand additional settings like verbosity and document separator.']}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'llm = ChatOpenAI(temperature = 0.0, model=llm_model)\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type="stuff", \n    retriever=index.vectorstore.as_retriever(), \n    verbose=True,\n    chain_type_kwargs = {\n        "document_separator": "<<<<>>>>>"\n    }\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"coming-up-with-test-datapoints",children:"Coming up with test datapoints"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"data[10]\n"})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"Document(page_content=\": 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped \nknit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is \nwhen we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: \nOur most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon \nand 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a \nwide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.\", \nmetadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 10})\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"data[11]\n"})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"Document(page_content=': 11\\nname: Ultra-Lofty 850 Stretch Down Hooded Jacket\\ndescription: This technical stretch down \njacket from our DownTek collection is sure to keep you warm and comfortable with its full-stretch construction providing \nexceptional range of motion. With a slightly fitted style that falls at the hip and best with a midweight layer, this \njacket is suitable for light activity up to 20\xb0 and moderate activity up to -30\xb0. The soft and durable 100% polyester \nshell offers complete windproof protection and is insulated with warm, lofty goose down. Other features include welded \nbaffles for a no-stitch construction and excellent stretch, an adjustable hood, an interior media port and mesh stash \npocket and a hem drawcord. Machine wash and dry. Imported.', \nmetadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 11})\n"})}),"\n",(0,a.jsx)(n.h3,{id:"hard-coded-examples",children:"Hard-coded examples"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'examples = [\n    {\n        "query": "Do the Cozy Comfort Pullover Set have side pockets?",\n        "answer": "Yes"\n    },\n    {\n        "query": "What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?",\n        "answer": "The DownTek collection"\n    }\n]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"llm-generated-examples",children:"LLM-Generated examples"}),"\n",(0,a.jsxs)(n.p,{children:["LLM Chain for generating examples for question answering.\nSee ",(0,a.jsx)(n.a,{href:"https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.generate_chain.QAGenerateChain.html"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.evaluation.qa import QAGenerateChain\n\nexample_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))\n\n# the warning below can be safely ignored\nnew_examples = example_gen_chain.apply_and_parse(\n    [{"doc": t} for t in data[:5]]\n)\n\nnew_examples[0]\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"{'query': \"What is the approximate weight of the Women's Campside Oxfords per pair?\",\n 'answer': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\"}\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"data[0]\n"})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"Document(page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable \nlace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a \nbroken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For \nhalf sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \n\\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole \nwith Cleansport NXT\xae antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. \nModerate arch contour of innersole. EVA foam midsole for cushioning and support. \nChain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \n\\n\\nQuestions? Please contact us for any inquiries.\", \nmetadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0})\n"})}),"\n",(0,a.jsx)(n.h3,{id:"combine-examples",children:"Combine examples"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'examples += new_examples\n\nqa.run(examples[0]["query"])\n'})}),"\n",(0,a.jsx)(n.h3,{id:"manual-evaluation",children:"Manual Evaluation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import langchain\nlangchain.debug = True\n\nqa.run(examples[0]["query"])\n\n# Turn off the debug mode\nlangchain.debug = False\n'})}),"\n",(0,a.jsx)(n.h3,{id:"llm-assisted-evaluation",children:"LLM assisted evaluation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'predictions = qa.apply(examples)\n\nfrom langchain.evaluation.qa import QAEvalChain\n\nllm = ChatOpenAI(temperature=0, model=llm_model)\neval_chain = QAEvalChain.from_llm(llm)\n\ngraded_outputs = eval_chain.evaluate(examples, predictions)\n\nfor i, eg in enumerate(examples):\n    print(f"Example {i}:")\n    print("Question: " + predictions[i][\'query\'])\n    print("Real Answer: " + predictions[i][\'answer\'])\n    print("Predicted Answer: " + predictions[i][\'result\'])\n    print("Predicted Grade: " + graded_outputs[i][\'text\'])\n    print()\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"Example 0:\nQuestion: Do the Cozy Comfort Pullover Set have side pockets?\nReal Answer: Yes\nPredicted Answer: The Cozy Comfort Pullover Set, Stripe has side pockets on the pull-on pants.\nPredicted Grade: CORRECT\n\nExample 1:\nQuestion: What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\nReal Answer: The DownTek collection\nPredicted Answer: The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection.\nPredicted Grade: CORRECT\n\nExample 2:\nQuestion: What is the approximate weight of the Women's Campside Oxfords per pair?\nReal Answer: The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\nPredicted Answer: The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\nPredicted Grade: CORRECT\n\nExample 3:\nQuestion: What are the dimensions for the small and medium sizes of the Recycled Waterhog Dog Mat, Chevron Weave?\nReal Answer: The dimensions for the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\" and the dimensions for the medium size are 22.5\" x 34.5\".\nPredicted Answer: The dimensions for the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\" and the dimensions for the medium size are 22.5\" x 34.5\".\nPredicted Grade: CORRECT\n\nExample 4:\nQuestion: What are some features of the Infant and Toddler Girls' Coastal Chill Swimsuit?\nReal Answer: The swimsuit features bright colors, ruffles, and exclusive whimsical prints. It is made of four-way-stretch and chlorine-resistant fabric that keeps its shape and resists snags. The swimsuit also has a UPF 50+ rated fabric that provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage.\nPredicted Answer: The Infant and Toddler Girls' Coastal Chill Swimsuit is a two-piece swimsuit with bright colors, ruffles, and exclusive whimsical prints. It is made of four-way-stretch and chlorine-resistant fabric that keeps its shape and resists snags. The swimsuit has UPF 50+ rated fabric that provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. It is machine washable and should be line dried for best results.\nPredicted Grade: CORRECT\n\nExample 5:\nQuestion: What is the fabric composition of the Refresh Swimwear, V-Neck Tankini Contrasts?\nReal Answer: The Refresh Swimwear, V-Neck Tankini Contrasts is made of 82% recycled nylon with 18% Lycra\xae spandex for the body and 90% recycled nylon with 10% Lycra\xae spandex for lining.\nPredicted Answer: The Refresh Swimwear, V-Neck Tankini Contrasts is made of 82% recycled nylon with 18% Lycra\xae spandex for the body and 90% recycled nylon with 10% Lycra\xae spandex for the lining.\nPredicted Grade: CORRECT\n\nExample 6:\nQuestion: What is the new technology used in the EcoFlex 3L Storm Pants?\nReal Answer: The new technology used in the EcoFlex 3L Storm Pants is TEK O2 technology, which offers the most breathability they've ever tested.\nPredicted Answer: The new technology used in the EcoFlex 3L Storm Pants is TEK O2 technology, which makes the pants more breathable and waterproof.\nPredicted Grade: CORRECT\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"graded_outputs[0]\n"})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"{'text': 'CORRECT'}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"langchain-evaluation-platform",children:"LangChain evaluation platform"}),"\n",(0,a.jsxs)(n.p,{children:["The LangChain evaluation platform, LangChain Plus, can be accessed here ",(0,a.jsx)(n.a,{href:"https://www.langchain.plus/",children:"https://www.langchain.plus/"}),". Use the invite code ",(0,a.jsx)(n.code,{children:"lang_learners_2023"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"agents",children:"Agents"}),"\n",(0,a.jsxs)(n.p,{children:["What are Agents? We can think of agents as enabling \u201ctools\u201d for LLMs. Like how a human would use a calculator for maths\nor perform a Google search for information \u2014 agents allow an LLM to do the same thing. See\n",(0,a.jsx)(n.a,{href:"https://www.pinecone.io/learn/series/langchain/langchain-agents/",children:"Superpower LLMs with Conversational Agents"})," for more information."]}),"\n",(0,a.jsx)(n.h3,{id:"setup-5",children:"Setup"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n\nimport warnings\nwarnings.filterwarnings("ignore")\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# account for deprecation of LLM model\nimport datetime\n# Get the current date\ncurrent_date = datetime.datetime.now().date()\n\n# Define the date after which the model should be set to "gpt-3.5-turbo"\ntarget_date = datetime.date(2024, 6, 12)\n\n# Set the model variable based on the current date\nif current_date > target_date:\n    llm_model = "gpt-3.5-turbo"\nelse:\n    llm_model = "gpt-3.5-turbo-0301"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"built-in-langchain-tools",children:"Built-in LangChain tools"}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/tools/",children:"Tools"}),",\n",(0,a.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/tools/wikipedia",children:"Wikipedia"})," and\n",(0,a.jsx)(n.a,{href:"https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_math.base.LLMMathChain.html",children:"LLMMathChain"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'!pip install -U wikipedia\n\nfrom langchain.agents.agent_toolkits import create_python_agent\nfrom langchain.agents import load_tools, initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.tools.python.tool import PythonREPLTool\nfrom langchain.python import PythonREPL\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(temperature=0, model=llm_model)\n\ntools = load_tools(["llm-math", "wikipedia"], llm=llm)\n\nagent= initialize_agent(\n    tools, \n    llm, \n    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n    handle_parsing_errors=True,\n    verbose = True)\n\nagent("What is the 25% of 300?")\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"> Entering new AgentExecutor chain...\nThought: We need to calculate 25% of 300, which means we need to multiply 300 by 0.25.\n\nAction:\n``\n{\n  \"action\": \"Calculator\",\n  \"action_input\": \"300*0.25\"\n}\n``\n\nObservation: Answer: 75.0\nThought:The calculator tool returned the answer 75.0, which is 25% of 300.\n\nFinal Answer: 25% of 300 is 75.0.\n\n> Finished chain.\n{'input': 'What is the 25% of 300?', 'output': '25% of 300 is 75.0.'}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"wikipedia-example",children:"Wikipedia example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'question = "Tom M. Mitchell is an American computer scientist \\\nand the Founders University Professor at Carnegie Mellon University (CMU)\\\nwhat book did he write?"\nresult = agent(question)\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:'> Entering new AgentExecutor chain...\nThought: I should use Wikipedia to find the answer to this question.\n\nAction:\n``\n{\n  "action": "Wikipedia",\n  "action_input": "Tom M. Mitchell"\n}\n``\n\nObservation: Page: Tom M. Mitchell\nSummary: Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University \nProfessor at Carnegie Mellon University (CMU). He is a founder and former Chair of the Machine Learning Department at \nCMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and \ncognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National \nAcademy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American \nAssociation for the Advancement of Science and a Fellow and past President of the Association for the Advancement of \nArtificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science \nat Carnegie Mellon.\n\nPage: Tom Mitchell (Australian footballer)\nSummary: Thomas Mitchell (born 31 May 1993) is a professional Australian rules footballer playing for the Collingwood \nFootball Club in the Australian Football League (AFL). He previously played for the Sydney Swans from 2012 to 2016, \nand the Hawthorn Football Club between 2017 and 2022. Mitchell won the Brownlow Medal as the league\'s best and fairest \nplayer in 2018 and set the record for the most disposals in a VFL/AFL match, accruing 54 in a game against Collingwood \nduring that season.\nThought:The book that Tom M. Mitchell wrote is "Machine Learning".\n\nAction:\n``\n{\n  "action": "Wikipedia",\n  "action_input": "Machine Learning (book)"\n}\n``\n\nObservation: Page: Machine learning\nSummary: Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human \nprogrammers would be cost-prohibitive, and instead the problems are solved by helping machines \'discover\' their \'own\' \nalgorithms, without needing to be explicitly told what to do by any human-developed algorithms. Recently, generative \nartificial neural networks have been able to surpass results of many previous approaches. Machine learning approaches \nhave been applied to large language models, computer vision, speech recognition, email filtering, agriculture and \nmedicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML \nare provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field \nof study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across \nbusiness problems under the name predictive analytics. Although not all machine learning is statistically-based, \ncomputational statistics is an important source of the field\'s methods.\n\nPage: Outline of machine learning\nSummary: The following outline is provided as an overview of and topical guide to machine learning. Machine learning is \na subfield of soft computing within computer science that evolved from the study of pattern recognition and \ncomputational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a "field \nof study that gives computers the ability to learn without being explicitly programmed". Machine learning explores the \nstudy and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by \nbuilding a model from an example training set of input observations in order to make data-driven predictions or \ndecisions expressed as outputs, rather than following strictly static program instructions.\n\nPage: Quantum machine learning\nSummary: Quantum machine learning is the integration of quantum algorithms within machine learning programs.The most \ncommon use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum \ncomputer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense \nquantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to \nimprove computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve \nboth classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. \nThese routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms \ncan be used to analyze quantum states instead of classical data.Beyond quantum computing, the term "quantum machine \nlearning" is also associated with classical machine learning methods applied to data generated from quantum experiments \n(i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new \nquantum experiments.Quantum machine learning also extends to a branch of research that explores methodological and \nstructural similarities between certain physical systems and learning systems, in particular neural networks. For \nexample, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning \nand vice versa.Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum \ninformation, sometimes referred to as "quantum learning theory".\nThought:Tom M. Mitchell wrote the book "Machine Learning".\n\nFinal Answer: The book that Tom M. Mitchell wrote is "Machine Learning".\n\n> Finished chain.\n'})}),"\n",(0,a.jsx)(n.h3,{id:"python-agent",children:"Python Agent"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://api.python.langchain.com/en/latest/tools/langchain.tools.python.tool.PythonREPLTool.html",children:"PythonREPLTool"}),"\nis a tool for running python code in a REPL."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'agent = create_python_agent(\n    llm,\n    tool=PythonREPLTool(),\n    verbose=True\n)\n\ncustomer_list = [["Harrison", "Chase"], \n                 ["Lang", "Chain"],\n                 ["Dolly", "Too"],\n                 ["Elle", "Elem"], \n                 ["Geoff","Fusion"], \n                 ["Trance","Former"],\n                 ["Jen","Ayai"]\n                ]\n\nagent.run(f"""Sort these customers by \\\nlast name and then first name \\\nand print the output: {customer_list}""") \n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"> Entering new AgentExecutor chain...\nI can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide \na key function to sorted() that returns a tuple of the last name and first name in that order.\nAction: Python REPL\nAction Input:\n``\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], \n['Trance', 'Former'], ['Jen', 'Ayai']]\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\nfor customer in sorted_customers:\n    print(customer)\n``\nObservation: ['Jen', 'Ayai']\n['Lang', 'Chain']\n['Harrison', 'Chase']\n['Elle', 'Elem']\n['Trance', 'Former']\n['Geoff', 'Fusion']\n['Dolly', 'Too']\n\nThought:The customers are now sorted by last name and then first name. \nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], \n['Geoff', 'Fusion'], ['Dolly', 'Too']]\n\n> Finished chain.\n\"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], \n['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n"})}),"\n",(0,a.jsx)(n.h3,{id:"view-detailed-outputs-of-the-chains",children:"View detailed outputs of the chains"}),"\n",(0,a.jsx)(n.p,{children:"To find out exactly what's going on under the hood..."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import langchain\nlangchain.debug=True\nagent.run(f"""Sort these customers by \\\nlast name and then first name \\\nand print the output: {customer_list}""") \nlangchain.debug=False\n'})}),"\n",(0,a.jsx)(n.h3,{id:"define-your-own-tool",children:"Define your own tool"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'!pip install DateTime\n\nfrom langchain.agents import tool\nfrom datetime import date\n\n@tool\ndef time(text: str) -> str:\n    """Returns todays date, use this for any questions related to knowing todays date. \\\n    The input should always be an empty string, and this function will always return todays \\\n    date - any date mathmatics should occur outside this function."""\n    return str(date.today())\n\nagent= initialize_agent(\n    tools + [time], \n    llm, \n    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n    handle_parsing_errors=True,\n    verbose = True)\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note:"})," The agent will sometimes come to the wrong conclusion (agents are a work in progress!).\nIf it does, please try running it again."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'try:\n    result = agent("whats the date today?") \nexcept: \n    print("exception on external access")\n'})}),"\n",(0,a.jsx)(n.p,{children:"The preceding code returns this."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:'> Entering new AgentExecutor chain...\nThought: I need to use the `time` tool to get today\'s date.\nAction:\n``\n{\n  "action": "time",\n  "action_input": ""\n}\n``\n\nObservation: 2023-08-31\nThought:I have successfully retrieved today\'s date using the `time` tool.\nFinal Answer: Today\'s date is August 31, 2023.\n\n> Finished chain.\n'})}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.h2,{id:"see-also",children:"See also"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"https://www.pinecone.io/learn/series/langchain/",children:"LangChain AI Handbook"}),"."]}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>s,a:()=>r});var a=t(67294);const o={},i=a.createContext(o);function r(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);