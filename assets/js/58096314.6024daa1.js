"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6412],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>d});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var c=r.createContext({}),s=function(e){var n=r.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},p=function(e){var n=s(e.components);return r.createElement(c.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},f=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=s(t),f=a,d=m["".concat(c,".").concat(f)]||m[f]||u[f]||o;return t?r.createElement(d,i(i({ref:n},p),{},{components:t})):r.createElement(d,i({ref:n},p))}));function d(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,i=new Array(o);i[0]=f;var l={};for(var c in n)hasOwnProperty.call(n,c)&&(l[c]=n[c]);l.originalType=e,l[m]="string"==typeof e?e:a,i[1]=l;for(var s=2;s<o;s++)i[s]=t[s];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}f.displayName="MDXCreateElement"},6086:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>l,toc:()=>s});var r=t(7462),a=(t(7294),t(3905));const o={},i="Llama",l={unversionedId:"snippets/llama",id:"snippets/llama",title:"Llama",description:"Running LLaMa 2 model inference in a Google Colab",source:"@site/docs/snippets/llama.md",sourceDirName:"snippets",slug:"/snippets/llama",permalink:"/docs/snippets/llama",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain",permalink:"/docs/snippets/langchain"}},c={},s=[{value:"Running LLaMa 2 model inference in a Google Colab",id:"running-llama-2-model-inference-in-a-google-colab",level:2}],p={toc:s},m="wrapper";function u(e){let{components:n,...t}=e;return(0,a.kt)(m,(0,r.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"llama"},"Llama"),(0,a.kt)("h2",{id:"running-llama-2-model-inference-in-a-google-colab"},"Running LLaMa 2 model inference in a Google Colab"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://ai.meta.com/resources/models-and-libraries/llama-downloads/"},"Request access")," to the next version of Llama."),(0,a.kt)("li",{parentName:"ul"},"After gaining access from Meta, head over to ",(0,a.kt)("a",{parentName:"li",href:"https://huggingface.co/meta-llama"},"Hugging Face"),'. Choose your desired model and submit a request to grant access. Expect a "granted access" email within 1-2 days.'),(0,a.kt)("li",{parentName:"ul"},'*Navigate to "Settings" in your Hugging Face account to create access tokens.'),(0,a.kt)("li",{parentName:"ul"},"Select T4 GPU on Google Colab.")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'!pip install transformers\n!huggingface-cli login\n\nfrom transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = "meta-llama/Llama-2-7b-chat-hf"\ntokenizer = AutoTokenizer.from_pretrained(model)\n\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map="auto")\n    \nsequences = pipeline(\n    \'Who are the key contributors to the field of artificial intelligence?\\n\',\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200)\n\nfor seq in sequences:\n    print(f"Result: {seq[\'generated_text\']}")\n')))}u.isMDXComponent=!0}}]);