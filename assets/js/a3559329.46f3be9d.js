"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2825],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>d});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),p=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},m=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},g=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,m=o(e,["components","mdxType","originalType","parentName"]),u=p(n),g=a,d=u["".concat(l,".").concat(g)]||u[g]||c[g]||i;return n?r.createElement(d,s(s({ref:t},m),{},{components:n})):r.createElement(d,s({ref:t},m))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,s=new Array(i);s[0]=g;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o[u]="string"==typeof e?e:a,s[1]=o;for(var p=2;p<i;p++)s[p]=n[p];return r.createElement.apply(null,s)}return r.createElement.apply(null,n)}g.displayName="MDXCreateElement"},6788:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var r=n(7462),a=(n(7294),n(3905));const i={},s="1. Expressions r\xe9guli\xe8res",o={unversionedId:"courses/ift-7022/week-1",id:"courses/ift-7022/week-1",title:"1. Expressions r\xe9guli\xe8res",description:"Cours du 5 septembre 2023 :",source:"@site/docs/courses/ift-7022/week-1.md",sourceDirName:"courses/ift-7022",slug:"/courses/ift-7022/week-1",permalink:"/docs/courses/ift-7022/week-1",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"IFT-7022 Traitement automatique de la langue naturelle",permalink:"/docs/courses/ift-7022/"},next:{title:"2. Pr\xe9traitement de textes et distance minimale d'\xe9dition",permalink:"/docs/courses/ift-7022/week-2"}},l={},p=[],m={toc:p},u="wrapper";function c(e){let{components:t,...i}=e;return(0,a.kt)(u,(0,r.Z)({},m,i,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"1-expressions-r\xe9guli\xe8res"},"1. Expressions r\xe9guli\xe8res"),(0,a.kt)("p",null,"Cours du 5 septembre 2023 :"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"\ud83d\udcd5 Jurafsky & Martin, 3e \xe9dition, ",(0,a.kt)("a",{parentName:"li",href:"https://web.stanford.edu/~jurafsky/slp3/2.pdf"},"chapitre 2, section 2.1"),". ")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"s01",src:n(3978).Z,width:"2370",height:"1586"})),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Regular expression")," (often shortened to ",(0,a.kt)("strong",{parentName:"li"},"regex"),"), can be used to specify strings we might want to extract from a document."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Text normalization")," means converting it to a more convenient, standard form."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Tokenization")," is used in natural language processing (NLP) to split paragraphs and sentences into smaller units that can be more easily assigned meaning."),(0,a.kt)("li",{parentName:"ul"},"We need to tokenize ",(0,a.kt)("strong",{parentName:"li"},"emoticons")," like ",(0,a.kt)("inlineCode",{parentName:"li"},":)")," or ",(0,a.kt)("strong",{parentName:"li"},"hashtags")," like ",(0,a.kt)("inlineCode",{parentName:"li"},"#nlproc"),"."),(0,a.kt)("li",{parentName:"ul"},"Another part of text normalization is ",(0,a.kt)("strong",{parentName:"li"},"lemmatization"),", the task of determining that two words have the same root, despite their surface differences."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Stemming")," refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word."),(0,a.kt)("li",{parentName:"ul"},"Text normalization also includes ",(0,a.kt)("strong",{parentName:"li"},"sentence segmentation"),": breaking up a text into individual sentences, using cues like sentence segmentation periods or exclamation points."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Edit distance")," is a metric that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other.")),(0,a.kt)("p",null,"Before almost any natural language processing of a text, the text has to be normalized. At least three tasks are commonly applied as part of any normalization process:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Tokenizing (segmenting) words"),(0,a.kt)("li",{parentName:"ol"},"Normalizing word formats"),(0,a.kt)("li",{parentName:"ol"},"Segmenting sentences")),(0,a.kt)("p",null,"Most tokenization schemes have two parts: a ",(0,a.kt)("strong",{parentName:"p"},"token learner"),", and a ",(0,a.kt)("strong",{parentName:"p"},"token segmenter"),".\nThe token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set\nof tokens. The token segmenter takes a raw test sentence and segments it into the\ntokens in the vocabulary."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Case folding")," is another kind of normalization, mapping everything to lower case."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Lemmatization")," is the task of determining that two words have the same root, despite their surface differences."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Stemming")," refers to the process of reducing a word to its word stem that affixes to suffixes and prefixes or the roots."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Sentence segmentation"),". The most useful cues for segmenting a text into sentences are punctuation, like periods, question marks, and exclamation points."),(0,a.kt)("li",{parentName:"ul"},"The ",(0,a.kt)("strong",{parentName:"li"},"minimum edit distance")," between two strings is defined minimum edit distance\nas the minimum number of editing operations (operations like insertion, deletion,\nsubstitution) needed to transform one string into another."),(0,a.kt)("li",{parentName:"ul"},"The ",(0,a.kt)("strong",{parentName:"li"},"Levenshtein distance")," between two sequences is the simplest weighting factor in\nwhich each of the three operations has a cost of 1. ")),(0,a.kt)("p",null,"Exemples \xe0 faire avec ",(0,a.kt)("a",{parentName:"p",href:"https://regexr.com/"},"https://regexr.com/"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-txt"},"# Trouver les la, le, les, l', etc.\n/le|la|l'|les/g\n/les|le|l'|le/g\n/\\b(les|le|l'|le)/g  # D\xe9but des mots.\n/\\b(les|le|l'|le)\\b/g  # Fin des mots.\n/\\b[Ll](e|a|'|es|es|e|'|e)\\b/g  # Isoler les Ll.\n/\\b[Ll](e|a|'|es)\\b/g  # Enlever les doublons.\n/\\b[Ll][ea']s?\\b/g  # Rendre le s optionel.\n\n# Trouver les courriels.\n/[a-z]+/g\n/[a-z]+.[a-z]+.[0-9]+/g\n/[a-z]+.[a-z]+.[0-9]@[a-z]+.[a-z]+.[a-z]+/g\n/[\\w]+.[\\w]+.[\\d]@[\\w]+.[\\w]+.[\\w]+/g  # Simplifier.\n/[\\w]+\\.[\\w]+\\.[\\d]@[\\w]+\\.[\\w]+\\.[\\w]+/g  # Forcer le point.\n/([\\w]+)\\.([\\w]+)\\.[\\d]@([\\w]+\\.[\\w]+\\.[\\w]+)/g  # Cr\xe9er des groupes.\n/([\\w]+)\\.([\\w]+)\\.[\\d]{1,3}@([\\w]+\\.[\\w]+\\.[\\w]+)/g  # Nombre de r\xe9p\xe9titions.\n\n# Exemples du prof le 12 septembre 2023\n[^\\n]([A-Z\xc0-\xdc][-A-z\xc0-\xfc]+)/g\n/(Canada|Qu\xe9bec)/g\n/(\\d[\\d\\s,]*(millions)?)\\s(d'h|h)/g  # pour obtenir les populations.\n")),(0,a.kt)("p",null,"Regex en Python."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Module ",(0,a.kt)("inlineCode",{parentName:"li"},"re"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"compile")," fonction qui g\xe9n\xe8re une version compil\xe9e d'une regex."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"match")," ou ",(0,a.kt)("inlineCode",{parentName:"li"},"search")," fait le matching entre une regex et une cha\xeene."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"sub")," remplace la premi\xe8re occurrence d'un patron par une cha\xeene. ")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import re\n\nexample_string = "this is a test"\np = re.compile(r"\\w+")\n\nm = p.search(example_string) print("Start index:", m.start())  # Start index: 0\nprint(" End index:", m.end())  # End index: 4\nprint(m.group())  # this\nprint(p.findall(example_string))  # [\u2018this\', \'is\', \'a\', \'test\']\n\np = re.compile("is")\nprint(p.match(example_string))  # None\nprint(re.sub(r"(this) (is) (.+)", "\\\\2 \\\\1 \\\\3", example_string))  # \'is this a test\'\n')),(0,a.kt)("p",null,"See also: "),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://docs.python.org/3/library/re.html"},(0,a.kt)("inlineCode",{parentName:"a"},"re")," - Regular expression operations"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://regex101.com/"},"Online regular expression tester"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://regexr.com/"},"RegExr"),". "),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"http://www.regextester.com/"},"Regex Tester"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://alf.nu/RegexGolf?world=regex&level=r00"},"Regex Golf"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://cheatography.com/davechild/cheat-sheets/regular-expressions/"},"Regular Expressions cheat sheet"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://docs.python.org/3/howto/regex.html"},"Regular Expression HOWTO"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://developers.google.com/edu/python/regular-expressions"},"Python Regular Expressions"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://missing.csail.mit.edu/2020/data-wrangling/"},"Data Wrangling")," from CSAIL MIT.")))}c.isMDXComponent=!0},3978:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/s01-999a3ce9e11f795a4e464b67b29be0ff.png"}}]);