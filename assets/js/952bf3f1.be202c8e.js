"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[5201],{47324:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>a,toc:()=>d});var o=t(85893),s=t(11151);const r={},i="Qwen",a={id:"references/llms/qwen",title:"Qwen",description:"Running Qwen on MPS",source:"@site/docs/references/llms/qwen.md",sourceDirName:"references/llms",slug:"/references/llms/qwen",permalink:"/docs/references/llms/qwen",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/references/llms/qwen.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Prompt",permalink:"/docs/references/llms/prompt"},next:{title:"Databases",permalink:"/docs/references/databases/"}},l={},d=[{value:"Running Qwen on MPS",id:"running-qwen-on-mps",level:2},{value:"See also",id:"see-also",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"qwen",children:"Qwen"}),"\n",(0,o.jsx)(n.h2,{id:"running-qwen-on-mps",children:"Running Qwen on MPS"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Step 1: Load the model and tokenizer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = "mps"  # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    "Owen/Owen1.5-4B-Chat",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained("Owen/Owen1.5-4B-Chat")\n\n# Step 2: Define your prompt \nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n# Step 3: Tokenise your prompt\nmodel_inputs = tokenizer([text], return_tensors="pt").to(device)\n\n# Step 4: Generate the response\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in\n    zip(model_inputs.input_ids, generated_ids)\n]\n\n# Step 5: Decode the response\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'})}),"\n",(0,o.jsx)(n.p,{children:"That's it! \ud83e\udd17"}),"\n",(0,o.jsx)(n.h2,{id:"see-also",children:"See also"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://venturebeat.com/ai/meet-smaug-72b-the-new-king-of-open-source-ai/",children:"Meet \u2018Smaug-72B\u2019: The new king of open-source AI"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>a,a:()=>i});var o=t(67294);const s={},r=o.createContext(s);function i(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);