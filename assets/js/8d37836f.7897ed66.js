"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[7726],{80189:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>u,frontMatter:()=>r,metadata:()=>a,toc:()=>c});var i=s(85893),t=s(11151);const r={sidebar_label:"8 Notions de base pour les RNN, GRU et LSTM",sidebar_position:10},d="Notions de base pour les RNN, GRU et LSTM",a={id:"courses/university/ift-7022/week-08-partie-2",title:"Notions de base pour les RNN, GRU et LSTM",description:"On pr\xe9sente dans ce notebook quelques exemples simples afin d'illustrer des notions utiles pour la programmation de r\xe9seaux r\xe9currents avec PyTorch :",source:"@site/docs/courses/university/ift-7022/week-08-partie-2.md",sourceDirName:"courses/university/ift-7022",slug:"/courses/university/ift-7022/week-08-partie-2",permalink:"/docs/courses/university/ift-7022/week-08-partie-2",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/university/ift-7022/week-08-partie-2.md",tags:[],version:"current",sidebarPosition:10,frontMatter:{sidebar_label:"8 Notions de base pour les RNN, GRU et LSTM",sidebar_position:10},sidebar:"tutorialSidebar",previous:{title:"8 Deep NLP - R\xe9seaux r\xe9currents pour le traitement de s\xe9quences (RNN, GRU, LSTM)",permalink:"/docs/courses/university/ift-7022/week-08-partie-1"},next:{title:"9 Deep NLP - Introduction aux mod\xe8les Transformers",permalink:"/docs/courses/university/ift-7022/week-09"}},o={},c=[{value:"1. Cr\xe9ation d&#39;une couche RNN ou GRU",id:"1-cr\xe9ation-dune-couche-rnn-ou-gru",level:2},{value:"1.1. Couche r\xe9currente RNN",id:"11-couche-r\xe9currente-rnn",level:3},{value:"1.2 Couche r\xe9currente GRU",id:"12-couche-r\xe9currente-gru",level:3},{value:"1.3 Stacked RNN",id:"13-stacked-rnn",level:3},{value:"1.4 RNN bidirectionnel",id:"14-rnn-bidirectionnel",level:3},{value:"1.5 Ajouter une couche de pr\xe9diction \xe0 la sortie d&#39;un RNN",id:"15-ajouter-une-couche-de-pr\xe9diction-\xe0-la-sortie-dun-rnn",level:3},{value:"2. Cr\xe9ation d&#39;un LSTM",id:"2-cr\xe9ation-dun-lstm",level:2},{value:"3. Passer des <em>embeddings</em> pr\xe9entra\xeen\xe9s directement comme input d&#39;un r\xe9seau r\xe9current",id:"3-passer-des-embeddings-pr\xe9entra\xeen\xe9s-directement-comme-input-dun-r\xe9seau-r\xe9current",level:2},{value:"4. Couche d&#39;<em>embeddings</em> pour convertir les mots en plongements",id:"4-couche-dembeddings-pour-convertir-les-mots-en-plongements",level:2},{value:"4.1 Pour aller rapidement \xe0 l&#39;essentiel",id:"41-pour-aller-rapidement-\xe0-lessentiel",level:3},{value:"4.2 Ajout d&#39;un jeton de <em>padding</em> (rembourrage) dans la couche d&#39;<em>embeddings</em>",id:"42-ajout-dun-jeton-de-padding-rembourrage-dans-la-couche-dembeddings",level:3},{value:"4.3 Cr\xe9er une <em>embedding</em> layer \xe0 partir de plongements existants",id:"43-cr\xe9er-une-embedding-layer-\xe0-partir-de-plongements-existants",level:3},{value:"4.4 Exemple avec Spacy",id:"44-exemple-avec-spacy",level:3},{value:"4.5 Ajout d&#39;un jeton de mot inconnu (unknown) au vocabulaire",id:"45-ajout-dun-jeton-de-mot-inconnu-unknown-au-vocabulaire",level:3},{value:"5. Padding des exemples d&#39;une minibatch",id:"5-padding-des-exemples-dune-minibatch",level:2},{value:"6. Packing des exemples d&#39;une <em>minibatch</em>",id:"6-packing-des-exemples-dune-minibatch",level:2},{value:"7. Exemple de r\xe9seau r\xe9current simple pour un probl\xe8me de classification",id:"7-exemple-de-r\xe9seau-r\xe9current-simple-pour-un-probl\xe8me-de-classification",level:2},{value:"7.1 La cr\xe9ation du jeu de donn\xe9es et des fonctions utilitaires",id:"71-la-cr\xe9ation-du-jeu-de-donn\xe9es-et-des-fonctions-utilitaires",level:3},{value:"7.2 Le r\xe9seau GRU",id:"72-le-r\xe9seau-gru",level:3},{value:"7.3 L&#39;entra\xeene avec Poutyne",id:"73-lentra\xeene-avec-poutyne",level:3},{value:"7.4 Quelques pr\xe9dictions pour tester le tout",id:"74-quelques-pr\xe9dictions-pour-tester-le-tout",level:3}];function l(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"notions-de-base-pour-les-rnn-gru-et-lstm",children:"Notions de base pour les RNN, GRU et LSTM"}),"\n",(0,i.jsx)(n.p,{children:"On pr\xe9sente dans ce notebook quelques exemples simples afin d'illustrer des notions utiles pour la programmation de r\xe9seaux r\xe9currents avec PyTorch :"}),"\n",(0,i.jsx)(n.h2,{id:"1-cr\xe9ation-dune-couche-rnn-ou-gru",children:"1. Cr\xe9ation d'une couche RNN ou GRU"}),"\n",(0,i.jsx)(n.h3,{id:"11-couche-r\xe9currente-rnn",children:"1.1. Couche r\xe9currente RNN"}),"\n",(0,i.jsxs)(n.p,{children:["On illustre ici la cr\xe9ation d'une couche RNN avec PyTorch en utilisant la classe ",(0,i.jsx)(n.code,{children:"nn.RNN"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["Les principaux arguments pour la cr\xe9ation d'une couche r\xe9currrente (",(0,i.jsx)(n.code,{children:"rnn_layer"}),") sont la dimension\nde la couches d'entr\xe9e (",(0,i.jsx)(n.code,{children:"input_size"}),"), la dimension de la couche cach\xe9e (",(0,i.jsx)(n.code,{children:"hidden_size"}),"). On limite\nnotre probl\xe8me jouet \xe0 une couche d'entr\xe9e de dimension 4 et une couche cach\xe9e contenant 3 neurones."]}),"\n",(0,i.jsxs)(n.p,{children:["Dans cet exemple-ci, la matrice ",(0,i.jsx)(n.strong,{children:"W"})," qui relie un input \xe0 la couche cach\xe9e a une dimension 4X3\ntandis que la matrice ",(0,i.jsx)(n.strong,{children:"U"})," qui relie la couche cach\xe9e pr\xe9c\xe9dente \xe0 la couche cach\xe9e actuelle est de 3X3.\nNous n'avons pas dans cette exemple de matrice ",(0,i.jsx)(n.strong,{children:"V"})," qui relierait la couche cach\xe9e \xe0 un couche de sortie\nadditionnelle qui ferait une pr\xe9diction (par exemple un classification du texte ou un \xe9tiquetage de mot)."]}),"\n",(0,i.jsxs)(n.p,{children:["Les poids du r\xe9seau r\xe9current (les matrices ",(0,i.jsx)(n.strong,{children:"W"})," et ",(0,i.jsx)(n.strong,{children:"U"}),") sont initialis\xe9e al\xe9atoirement lors de\nla cr\xe9ation de la couche RNN."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\nfrom torch import nn\n\ninput_dimension = 4\nhidden_dimension = 3\n\nrnn_layer = nn.RNN(input_size=input_dimension, hidden_size=hidden_dimension, batch_first=True)\nrnn_layer\n# RNN(4, 3, batch_first=True)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["On retrouve l'argument optionnel ",(0,i.jsx)(n.code,{children:"batch_first"})," qui indique la forme des tenseurs donn\xe9es en inputs\ndurant l'entra\xeenement (les ",(0,i.jsx)(n.em,{children:"minibatchs"}),"). Lorsque ",(0,i.jsx)(n.code,{children:"batch_first"})," est ",(0,i.jsx)(n.code,{children:"True"}),", la 1ere dimension du tenseur\nde minibatch correspond au nombre d'exemples dans la minibatch. Si sa valeur est ",(0,i.jsx)(n.code,{children:"False"}),", la premi\xe8re\ndimension correspond \xe0 la longueur des s\xe9quences."]}),"\n",(0,i.jsxs)(n.p,{children:["Pour illustrer l'utilisation de la couche RNN ",(0,i.jsx)(n.code,{children:"rnn_layer"}),", on lui soumet une seule s\xe9quence afin d'observer\nl'output qui est retourn\xe9e par la couche. La s\xe9quence contient 2 \xe9l\xe9ments, ce qui pourrait correspondre \xe0 la\nrepr\xe9sentation d'un court texte qui contient 2 ",(0,i.jsx)(n.em,{children:"embeddings"})," de mots. Comme on a d\xe9fini la dimension des\nembeddings \xe0 4, l'input au RNN correspond \xe0 une matrice 1 X 2 X 4 (c.-\xe0-d. une seule s\xe9quence qui contient\n2 mots repr\xe9sent\xe9s par des ",(0,i.jsx)(n.em,{children:"embeddings"})," de dimension 4)."]}),"\n",(0,i.jsxs)(n.p,{children:["Pour traiter la s\xe9quence, le RNN a besoin d'un \xe9tat cach\xe9 initiale ",(0,i.jsx)(n.em,{children:"h0"})," (",(0,i.jsx)(n.code,{children:"hidden0"})," dans le code). On y va ici\navec un vecteur de 0, indiquant que nous n'avons aucune information \xe0 prendre en compte avant de traiter cette\ns\xe9quence. Une autre option qu'on voit parfois est la g\xe9n\xe9ration d'un \xe9tat cach\xe9 initial contenant des valeurs al\xe9atoires."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'emb1 = [0.9, 0.8, 0.7 , 0.6]\nemb2 = [0.1, 0.2, 0.3 , 0.4]\nsequence = [[emb1, emb2]]\ninputs = torch.FloatTensor(sequence)\nprint("Inputs du RNN:\\n", inputs)\n\nhidden0 = torch.zeros(1, 1, 3)\nprint("\\n\xc9tat cach\xe9 initial du RNN:\\n", hidden0)\n\noutput, hidden = rnn_layer(inputs, hidden0)\n# Inputs du RNN:\n#  tensor([[[0.9000, 0.8000, 0.7000, 0.6000],\n#          [0.1000, 0.2000, 0.3000, 0.4000]]])\n#\n# \xc9tat cach\xe9 initial du RNN:\n#  tensor([[[0., 0., 0.]]])\n'})}),"\n",(0,i.jsxs)(n.p,{children:["On inspecte les r\xe9sultats produits par la couche RNN. Le premier r\xe9sultat ",(0,i.jsx)(n.code,{children:"output"})," contient la liste des \xe9tats cach\xe9s\npar le r\xe9seau dans le traitement de la s\xe9quence en input. On retrouvera donc 2 vecteurs d'\xe9tat cach\xe9 qui ont \xe9t\xe9\ng\xe9n\xe9r\xe9s pour les inputs ",(0,i.jsx)(n.code,{children:"emb1"})," et ",(0,i.jsx)(n.code,{children:"emb2"}),". Le deuxi\xe8me r\xe9sultat hidden correspond au dernier \xe9tat cach\xe9 g\xe9n\xe9r\xe9\npar le r\xe9seau (c.-\xe0-d. l'\xe9tat cach\xe9 associ\xe9 au dernier mot de la s\xe9quence - dans notre cas ",(0,i.jsx)(n.code,{children:"emb2"}),")."]}),"\n",(0,i.jsxs)(n.p,{children:["\xc0 noter dans ces exemples l'utilisation de la fonction ",(0,i.jsx)(n.code,{children:"squeeze()"})," qui \xe9limine les dimensions d'un tenseur \xe9gales \xe0 1."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'torch.set_printoptions(precision=4)\nprint("Output:", output)\nprint("Hidden:", hidden)\n# Output: tensor([[[-0.4870,  0.5099, -0.4016],\n#          [-0.4997, -0.2191, -0.3646]]], grad_fn=<TransposeBackward1>)\n# Hidden: tensor([[[-0.4997, -0.2191, -0.3646]]], grad_fn=<StackBackward>)\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'hidden1 = output[:, 0, :].squeeze().tolist()\nprint("\xc9tat cach\xe9 hidden1 qui correspond au 1er input: ", hidden1)\n# \xc9tat cach\xe9 hidden1 qui correspond au 1er input:  [-0.48698967695236206, 0.5099459290504456,\n# -0.4015676975250244]\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'hidden2 = output[:, 1, :].squeeze().tolist()\nprint("\xc9tat cach\xe9 hidden2 qui correspond au 2e input:", hidden2)\n# \xc9tat cach\xe9 hidden2 qui correspond au 2e input: [-0.49973031878471375, -0.2191120833158493,\n# -0.3646095395088196]\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'print("Dernier \xe9tat cach\xe9 du RNN:", hidden.squeeze().tolist())\nDernier \xe9tat cach\xe9 du RNN: [-0.49973031878471375, -0.2191120833158493, -0.3646095395088196]\n'})}),"\n",(0,i.jsx)(n.h3,{id:"12-couche-r\xe9currente-gru",children:"1.2 Couche r\xe9currente GRU"}),"\n",(0,i.jsxs)(n.p,{children:["On refait le m\xeame exercice avec un GRU. Comme le nombre d'input et d'output d'un GRU est le m\xeame que ceux d'un RNN,\nla seule modification \xe0 faire est d'utiliser la classe ",(0,i.jsx)(n.code,{children:"nn.GRU"})," pour cr\xe9er la couche r\xe9currente. Tout le reste est\nidentique et on r\xe9utilise l'input et l'\xe9tat cach\xe9 initial (",(0,i.jsx)(n.code,{children:"hidden0"}),") de la partie 1.1 de ce notebook."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"gru_layer = nn.GRU(input_size=input_dimension, hidden_size=hidden_dimension, batch_first=True)\noutput, hidden = gru_layer(inputs, hidden0)\ngru_layer\n# GRU(4, 3, batch_first=True)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"output\n# tensor([[[ 0.0524, -0.2793, -0.1730],\n#          [ 0.0454, -0.1000, -0.1087]]], grad_fn=<TransposeBackward1>)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"hidden\n# tensor([[[ 0.0454, -0.1000, -0.1087]]], grad_fn=<StackBackward>)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"13-stacked-rnn",children:"1.3 Stacked RNN"}),"\n",(0,i.jsxs)(n.p,{children:["Un r\xe9seau r\xe9current peut avoir plusieurs couches superpos\xe9es (",(0,i.jsx)(n.em,{children:"stacked RNNs"}),", voir section 9.5.1 de Jurafsky et Martin).\nLe nombre de couches est d\xe9termin\xe9 par le param\xe8tre ",(0,i.jsx)(n.code,{children:"num_layers"}),". Par d\xe9faut, les RNNs n'ont qu'une seule couche (",(0,i.jsx)(n.code,{children:"num_layers=1"}),")."]}),"\n",(0,i.jsx)(n.p,{children:"Voici un exemple \xe0 3 couches. Nous avons toujours des inputs de dimensions 3."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"stacked_rnn_layer = nn.RNN(input_size=input_dimension, hidden_size=hidden_dimension, batch_first=True, num_layers=3)\nstacked_rnn_layer\n# RNN(4, 3, num_layers=3, batch_first=True)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# 3 couches dans le RNN, 1 seul exemple \xe0 traiter, hidden_dimension = 3\nstacked_hidden0 = torch.zeros(3, 1, 3)\noutput, hidden = stacked_rnn_layer(inputs, stacked_hidden0)\nprint("\\n\xc9tat cach\xe9 initial du stacked RNN - 1 vecteur par couche:\\n", stacked_hidden0)\n# \xc9tat cach\xe9 initial du stacked RNN - 1 vecteur par couche:\n#  tensor([[[0., 0., 0.]],\n#         [[0., 0., 0.]],\n#         [[0., 0., 0.]]])\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'torch.set_printoptions(precision=4)\nprint("Les outputs produits en sortie de la derni\xe8re couche du stacked RNN pour chacun des 2 mots de la s\xe9quence (output):\\n", output)\nprint("\\nLes vecteurs de chacune des 3 couches cach\xe9es pour le dernier mot de la s\xe9quence (hidden):\\n", hidden)\nLes outputs produits en sortie de la derni\xe8re couche du stacked RNN pour chacun des 2 mots de la s\xe9quence (output):\n tensor([[[-0.4371, -0.6443, -0.3190],\n         [-0.6356, -0.3190, -0.2104]]], grad_fn=<TransposeBackward1>)\n\nLes vecteurs de chacune des 3 couches cach\xe9es pour le dernier mot de la s\xe9quence (hidden):\n tensor([[[-0.6729, -0.4056, -0.7022]],\n        [[ 0.1081,  0.8286,  0.5611]],\n        [[-0.6356, -0.3190, -0.2104]]], grad_fn=<StackBackward>)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"14-rnn-bidirectionnel",children:"1.4 RNN bidirectionnel"}),"\n",(0,i.jsxs)(n.p,{children:["On peut cr\xe9er un RNN bidirectionnel en activant l'argument ",(0,i.jsx)(n.code,{children:"bidirectional"}),". On obtient alors 2 RNNs qui traitent les inputs\ndans des directions oppos\xe9es - a) de gauche \xe0 droite et b) de droite \xe0 gauche (voir section 9.5.2 de Jurafsky et Martin).\nVous noterez dans cet exemple que les \xe9tats cach\xe9s produits par le RNN bidirectionnel (output) sont la concat\xe9nation des\n\xe9tats cach\xe9s de chacun des RNNs directionnels (c.-\xe0-d. les 2 \xe9tats associ\xe9s au m\xeame jeton sont mis bout \xe0 bout)."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"bidirectional_rnn_layer = nn.RNN(input_size=input_dimension, hidden_size=hidden_dimension,\n                                 batch_first=True, bidirectional=True)\nbidirectional_rnn_layer\n# RNN(4, 3, batch_first=True, bidirectional=True)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# 2 couches bidirectionnelles, 1 exemple, taille des \xe9tats cach\xe9s = 3\nbi_hidden0 = torch.zeros(2, 1, 3)\noutput, hidden = bidirectional_rnn_layer(inputs, bi_hidden0)\nprint("\\n\xc9tat cach\xe9 initial du biRNN - 1 vecteur pour initialiser chaque direction:\\n", bi_hidden0)\n# \xc9tat cach\xe9 initial du biRNN - 1 vecteur pour initialiser chaque direction:\n#  tensor([[[0., 0., 0.]],\n#         [[0., 0., 0.]]])\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'torch.set_printoptions(precision=4)\nprint("Les \xe9tats cach\xe9s du biRNN pour chaque mots - concat\xe9nation des \xe9tats cach\xe9s de chacun des RNNs directionnels (output):\\n", output)\nprint("\\nLes vecteurs en sortie de chacun de 2 RNNs directionnels (hidden):\\n", hidden)\n# Les \xe9tats cach\xe9s du biRNN pour chaque mots - concat\xe9nation des \xe9tats cach\xe9s de chacun des RNNs directionnels (output):\n#  tensor([[[ 0.8223,  0.0209, -0.7598, -0.4576,  0.9166, -0.2191],\n#          [ 0.5874, -0.4373, -0.5055, -0.3148,  0.5022,  0.4569]]],\n#        grad_fn=<TransposeBackward1>)\n#\n#Les vecteurs en sortie de chacun de 2 RNNs directionnels (hidden):\n# tensor([[[ 0.5874, -0.4373, -0.5055]],\n#        [[-0.4576,  0.9166, -0.2191]]], grad_fn=<StackBackward>)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"15-ajouter-une-couche-de-pr\xe9diction-\xe0-la-sortie-dun-rnn",children:"1.5 Ajouter une couche de pr\xe9diction \xe0 la sortie d'un RNN"}),"\n",(0,i.jsx)(n.p,{children:"On a vu dans le cours qu'un r\xe9seau r\xe9current peut \xeatre d\xe9fini par 3 matrices de poids :"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"W"})," qui relie les inputs \xe0 la couche cach\xe9e"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"U"})," qui relie l'\xe9tat cach\xe9 pr\xe9c\xe9dent \xe0 l'\xe9tat cach\xe9 actuel"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"V"})," qui permet de faire une pr\xe9diction \xe0 partir du contenu de l'\xe9tat cach\xe9 (c.-\xe0-d. une t\xeate de pr\xe9diction)."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Les exemples pr\xe9c\xe9dents ne contenaient que les 2 premi\xe8res matrices. Voici un exemple simplifi\xe9 de r\xe9seau r\xe9current GRU\n",(0,i.jsx)(n.code,{children:"SimpleGRUTagger"})," qui fait une pr\xe9diction binaire pour chaque mot d'un texte (un \xe9tiquetage). On applique une fonction\nd'activation ReLU \xe0 la couche interm\xe9diaire. On utilise les poids initiaux pour faire les pr\xe9dictions (pas d'entra\xeenement\ndu r\xe9seau dans cet exemple). \xc0 noter que lorsque l'\xe9tat initial ",(0,i.jsx)(n.em,{children:"h0"})," du r\xe9seau r\xe9current n'est pas sp\xe9cifi\xe9,\ndes vecteurs de 0s sont utilis\xe9s."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class SimpleGRUTagger(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleGRUTagger, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.gru_layer = nn.GRU(input_dim, hidden_dim, batch_first=True)  # D\xe9fini W et U\n        self.fc = nn.Linear(hidden_dim, output_dim)  # Correspond \xe0 V\n        self.relu = nn.ReLU()\n\n    def forward(self, x, h):\n        out, h = self.gru_layer(x, h)\n        # V est appliqu\xe9 \xe0 chacun des \xe9tats cach\xe9s = \xe9tiquetage de tous les mots\n        out = self.fc(self.relu(out))\n        return out, h\n\ninput_dimension = 4\nhidden_dimension = 3\noutput_dimension = 1\n\nsimple_tagger = SimpleGRUTagger(input_dimension, hidden_dimension, output_dimension)\nsimple_tagger\n# SimpleGRUTagger(\n#   (gru_layer): GRU(4, 3, batch_first=True)\n#   (fc): Linear(in_features=3, out_features=1, bias=True)\n#   (relu): ReLU()\n# )\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'print("Input - 2 _embeddings_ donn\xe9s \xe0 l\'\xe9tiqueteur GRU:\\n", inputs)\n# Input - 2 _embeddings_ donn\xe9s \xe0 l\'\xe9tiqueteur GRU:\n#  tensor([[[0.9000, 0.8000, 0.7000, 0.6000],\n#          [0.1000, 0.2000, 0.3000, 0.4000]]])\n\npredictions, hidden = simple_tagger(inputs, hidden0)\nprint("Les pr\xe9dictions pour ces 2 mots:\\n", predictions)\n# Les pr\xe9dictions pour ces 2 mots:\n#  tensor([[[-0.3666],\n#          [-0.3458]]], grad_fn=<AddBackward0>)\n\nprint("Le dernier \xe9tat cach\xe9 du r\xe9seau:\\n", hidden)\n# Le dernier \xe9tat cach\xe9 du r\xe9seau:\n#  tensor([[[-0.1405,  0.3199,  0.0720]]], grad_fn=<StackBackward>)\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Un exemple similaire pour un r\xe9seau ",(0,i.jsx)(n.code,{children:"SimpleGRUClassifier"})," qui fait la classification binaire d'une s\xe9quence\ncompl\xe8te. \xc0 noter que la seule diff\xe9rence est dans la fonction forward qui n'applique la t\xeate de pr\xe9diction (la matrice ",(0,i.jsx)(n.strong,{children:"V"}),")\nque sur l'\xe9tat final du r\xe9seau r\xe9current."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class SimpleGRUClassifier(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleGRUClassifier, self).__init__()  # Une diff\xe9rence mineure\n        self.hidden_dim = hidden_dim\n        self.gru_layer = nn.GRU(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x, h):\n        out, h = self.gru_layer(x, h)\n        # V est appliqu\xe9 au dernier \xe9tat cach\xe9 = classification du texte\n        out = self.fc(self.relu(out[:,-1]))\n        return out, h\n\noutput_dimension = 1\nsimple_classifier = SimpleGRUClassifier(input_dimension, hidden_dimension, output_dimension)\nprediction, hidden = simple_classifier(inputs, hidden0)\nprint("La pr\xe9diction pour ce mini-texte:\\n", prediction)\nprint("\\nLe dernier \xe9tat cach\xe9 du r\xe9seau:\\n", hidden)\n# La pr\xe9diction pour ce mini-texte:\n#  tensor([[0.4064]], grad_fn=<AddmmBackward>)\n#\n# Le dernier \xe9tat cach\xe9 du r\xe9seau:\n#  tensor([[[-0.0279,  0.3144,  0.2712]]], grad_fn=<StackBackward>)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"2-cr\xe9ation-dun-lstm",children:"2. Cr\xe9ation d'un LSTM"}),"\n",(0,i.jsxs)(n.p,{children:["D'un point de vue programmation, la seule diff\xe9rence entre un RNN et un LSTM est que le LSTM\nmanipule 2 vecteurs latents: 1 vecteur d'\xe9tat cach\xe9 (m\xe9moire court-terme - ",(0,i.jsx)(n.em,{children:"hidden"}),") et un vecteur de contexte (m\xe9moire\nlong-terme - ",(0,i.jsx)(n.em,{children:"context"}),"). Pour initialiser le traitement d'une s\xe9quence, il faut donc cr\xe9er 2 vecteurs d'initialisation\n(",(0,i.jsx)(n.code,{children:"hidden0"})," et ",(0,i.jsx)(n.code,{children:"context0"}),"). Et en sortie, ",(0,i.jsx)(n.code,{children:"hidden"})," contient les 2 vecteurs finaux, ceux de la couche cach\xe9e et du contexte long-terme."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"lstm_layer = nn.LSTM(input_size=input_dimension, hidden_size=hidden_dimension, batch_first=True)\n\nhidden0 = torch.zeros(1, 1, 3)\ncontext0 = torch.zeros(1, 1, 3)\n\noutput, (hidden, context) = lstm_layer(inputs, (hidden0, context0))\nlstm_layer\n# LSTM(4, 3, batch_first=True)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"output\ntensor([[[ 0.0474,  0.0503,  0.0086],\n         [ 0.1201,  0.1592, -0.0042]]], grad_fn=<TransposeBackward0>)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Ce sont les deux vecteurs d'\xe9tat cach\xe9 ",(0,i.jsx)(n.code,{children:"h1"})," et ",(0,i.jsx)(n.code,{children:"h2"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'print("Dernier \xe9tat cach\xe9:\\n", hidden)\n# Dernier \xe9tat cach\xe9:\n#  tensor([[[ 0.1201,  0.1592, -0.0042]]], grad_fn=<StackBackward>)\n\nprint("Dernier vecteur de contexte du r\xe9seau:\\n", context)\n# Dernier vecteur de contexte du r\xe9seau:\n#  tensor([[[ 0.3921,  0.3844, -0.0104]]], grad_fn=<StackBackward>)\n'})}),"\n",(0,i.jsxs)(n.p,{children:["On voit ici que le vecteur ",(0,i.jsx)(n.code,{children:"hidden"})," correspond au dernier \xe9tat cach\xe9 du LSTM (voir 2e vecteur de ",(0,i.jsx)(n.code,{children:"output"}),") tandis que le\nvecteur ",(0,i.jsx)(n.code,{children:"context"})," correspond au contexte long-terme qui a \xe9t\xe9 produit \xe0 la derni\xe8re r\xe9currence."]}),"\n",(0,i.jsxs)(n.h2,{id:"3-passer-des-embeddings-pr\xe9entra\xeen\xe9s-directement-comme-input-dun-r\xe9seau-r\xe9current",children:["3. Passer des ",(0,i.jsx)(n.em,{children:"embeddings"})," pr\xe9entra\xeen\xe9s directement comme input d'un r\xe9seau r\xe9current"]}),"\n",(0,i.jsxs)(n.p,{children:["On illustre ici le passage direct de plongements pr\xe9entra\xeen\xe9s \xe0 une couche r\xe9currente. On utilise les ",(0,i.jsx)(n.em,{children:"embeddings"})," de Spacy.\nCe code pour obtenir les ",(0,i.jsx)(n.em,{children:"embeddings"})," se retrouverait normalement dans une classe de Dataset."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import spacy\nfrom torch import FloatTensor\n\nspacy_en = spacy.load('en_core_web_md')\n\ndef get_spacy_embeddings(text, spacy_analyzer):\n    doc = spacy_analyzer(text)\n    _embeddings_ = [token.vector for token in doc]\n    return FloatTensor(embeddings)\nInit Plugin\nInit Graph Optimizer\nInit Kernel\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'example = "This is an example"\nembeddings = get_spacy_embeddings(example, spacy_en)\nprint("Dimensions des _embeddings_ de cet exemple:", embeddings.shape)\n# Dimensions des _embeddings_ de cet exemple: torch.Size([4, 300])\nprint("Les embeddings:")\nembeddings\n# Les embeddings:\n# tensor([[-0.0876,  0.3550,  0.0639,  ...,  0.0345, -0.1503,  0.4067],\n#         [-0.0850,  0.5020,  0.0024,  ..., -0.2151, -0.2630, -0.0060],\n#         [-0.0117,  0.1948,  0.0889,  ..., -0.0547, -0.1934,  0.1400],\n#         [-0.2713,  0.2902, -0.2893,  ..., -0.0983,  0.0059,  0.2856]])\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# On ajoute une dimension=1 pour obtenir un tenseur de minibatch qui contient notre seul\n# exemple contenant les embeddings\ninput_tensor = torch.unsqueeze(embeddings, dim=0)\n\nprint("Dimensions du tenseur en input du RNN =", input_tensor.shape)\nprint("\\t= (nombre d\'exemples, longueur des exemples, dimension des embeddings)")\ninput_tensor\n# Dimensions du tenseur en input du RNN = torch.Size([1, 4, 300])\n#  = (nombre d\'exemples, longueur des exemples, dimension des embeddings)\n# tensor([[[-0.0876,  0.3550,  0.0639,  ...,  0.0345, -0.1503,  0.4067],\n#          [-0.0850,  0.5020,  0.0024,  ..., -0.2151, -0.2630, -0.0060],\n#          [-0.0117,  0.1948,  0.0889,  ..., -0.0547, -0.1934,  0.1400],\n#          [-0.2713,  0.2902, -0.2893,  ..., -0.0983,  0.0059,  0.2856]]])\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# La dimension des vecteurs d'embeddings de Spacy\nspacy_embedding_dim = spacy_en.meta['vectors']['width']\ninput_dimension = spacy_embedding_dim\nhidden_dimension = 4  # Couche cach\xe9e de faible dimension pour visualiser les r\xe9sultats\n\nspacy_rnn_layer = nn.RNN(input_size=input_dimension, hidden_size=hidden_dimension, batch_first=True)\noutput, hidden = spacy_rnn_layer(input_tensor)\n\nprint(\"On retrouve encore une fois les usuels output:\\n\", output)\nprint(\"\\net hidden:\\n\", hidden)\n# On retrouve encore une fois les usuels output:\n#  tensor([[[ 0.7739,  0.9987, -0.8505,  0.7156],\n#          [ 0.7215,  0.9681, -0.0299, -0.7905],\n#          [ 0.9722,  0.9991,  0.0179, -0.8747],\n#          [-0.9392,  0.9870, -0.7154,  0.3561]]], grad_fn=<TransposeBackward1>)\n#\n# et hidden:\n#  tensor([[[-0.9392,  0.9870, -0.7154,  0.3561]]], grad_fn=<StackBackward>)\n"})}),"\n",(0,i.jsxs)(n.h2,{id:"4-couche-dembeddings-pour-convertir-les-mots-en-plongements",children:["4. Couche d'",(0,i.jsx)(n.em,{children:"embeddings"})," pour convertir les mots en plongements"]}),"\n",(0,i.jsxs)(n.p,{children:["PyTorch offre la classe ",(0,i.jsx)(n.code,{children:"nn.Embedding"})," qui permet de construire une couche de r\xe9seau qui convertit des mots en vecteurs\nde plongement (",(0,i.jsx)(n.em,{children:"embeddings"}),"). On d\xe9nomme cette couche ",(0,i.jsx)(n.em,{children:"embedding layer"}),". En gros, on peut voir cette classe comme une matrice\n(ou une couche lin\xe9aire) qui contient les ",(0,i.jsx)(n.em,{children:"embeddings"})," des mots d'un vocabulaire. Les mots sont associ\xe9s \xe0 des identifiants\n(des entiers de 0 \xe0 N-1 qui correspondent aux N mots de notre vocabulaire)."]}),"\n",(0,i.jsxs)(n.p,{children:["Pour traiter un texte avec un r\xe9seau r\xe9current contenant une couche d'",(0,i.jsx)(n.em,{children:"embeddings"}),", on remplace tout d'abord chaque mot\ndu texte par son identifiant et on passe cette liste d'entiers \xe0 la couche d'",(0,i.jsx)(n.em,{children:"embeddings"})," qui retourne les ",(0,i.jsx)(n.em,{children:"embeddings"}),"\nassoci\xe9s aux identifiants de mots. On peut par la suite rendre ces plongements disponible aux couches suivantes du r\xe9seau."]}),"\n",(0,i.jsx)(n.h3,{id:"41-pour-aller-rapidement-\xe0-lessentiel",children:"4.1 Pour aller rapidement \xe0 l'essentiel"}),"\n",(0,i.jsx)(n.p,{children:"Un exemple jouet pour illustrer la m\xe9canique de cette classe."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"num_words = 3\nembedding_dimension = 5\n\nembedding_layer = nn.Embedding(num_words, embedding_dimension)  # valeurs initialis\xe9es al\xe9atoirement\nembedding_layer\n# Embedding(3, 5)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["La couche d'",(0,i.jsx)(n.em,{children:"embeddings"})," est initialis\xe9e al\xe9atoirement avec des valeurs suivant une loi normale (0, 1)."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"text_ids = [2]  # Notre texte contient 1 seul mot qui correspond \xe0 l'identifiant 2 (c.-\xe0-d. 2e mot du vocabulaire)\nword_id = torch.LongTensor(text_ids)\nembedding_layer(word_id)\n# tensor([[-0.1250,  0.1272,  0.2590,  1.1000,  0.1559]],\n#        grad_fn=<EmbeddingBackward>)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"text_ids = [0, 1, 2]  # Notre texte contient 3 mots dont les identifiants dont 0, 1 et 2.\nword_ids = torch.LongTensor(text_ids)\nembedding_layer(word_ids)\n# tensor([[-0.4157, -0.2084, -0.8642,  0.8541,  0.3090],\n#         [-0.7784,  2.3099,  2.1679,  0.6308,  1.0504],\n#         [-0.1250,  0.1272,  0.2590,  1.1000,  0.1559]],\n#        grad_fn=<EmbeddingBackward>)\n"})}),"\n",(0,i.jsxs)(n.h3,{id:"42-ajout-dun-jeton-de-padding-rembourrage-dans-la-couche-dembeddings",children:["4.2 Ajout d'un jeton de ",(0,i.jsx)(n.em,{children:"padding"})," (rembourrage) dans la couche d'",(0,i.jsx)(n.em,{children:"embeddings"})]}),"\n",(0,i.jsxs)(n.p,{children:["Le rembourrage (",(0,i.jsx)(n.em,{children:"padding"}),") est fr\xe9quemment utilis\xe9 dans les r\xe9seaux de neurones, surtout pour le traitement de\nminibatch. Lorsqu'on veut traiter des s\xe9quences de longueurs fixes, le rembourrage consiste \xe0 ajouter des jetons\nde padding afin d'obtenir la bonne longueur de s\xe9quence. Par exemple, pour une longueur d\xe9sir\xe9e de 6 jetons,\nla liste de jetons du texte \"",(0,i.jsx)(n.em,{children:"ceci est un exemple"}),'" deviendrait: [ceci, est, un, exemple, &lt;PAD>, &lt;PAD>]']}),"\n",(0,i.jsxs)(n.p,{children:["Dans la couche d'",(0,i.jsx)(n.em,{children:"embeddings"}),", la convention est d'associer l'identifiant 0 au jeton de padding. Pour l'activer,\non choisit l'option ",(0,i.jsx)(n.code,{children:"padding_idx=0"}),". Il est toutefois possible de choisir un autre num\xe9ro d'identifiant au besoin\n(cela est seulement une convention)."]}),"\n",(0,i.jsx)(n.p,{children:"Par d\xe9faut, on associe au jeton de padding un vecteur rempli de 0s."}),"\n",(0,i.jsx)(n.p,{children:"Voici un exemple."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\nfrom torch import nn\n\nnum_words = 3\nembedding_dimension = 5\npadding_id = 0\npadding_token = '<PAD>'\n\nembedding_layer = nn.Embedding(num_words, embedding_dimension, padding_idx=0)\nembedding_layer\n# Embedding(3, 5, padding_idx=0)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"embedding_layer(torch.LongTensor([0]))\n# tensor([[0., 0., 0., 0., 0.]], grad_fn=<EmbeddingBackward>)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"text_ids = [2, 1, 0]  # Notre texte contient 3 mots ayant les identifiants 2, 1 et 0 (jeton de padding).\nword_ids = torch.LongTensor(text_ids)\ntext_embeddings = embedding_layer(word_ids)\ntext_embeddings\n# tensor([[ 0.1385, -0.3528,  1.1324, -0.3810,  1.7321],\n#         [-0.6047, -0.8128,  0.7661, -1.5609, -1.5598],\n#         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n#        grad_fn=<EmbeddingBackward>)\n"})}),"\n",(0,i.jsxs)(n.h3,{id:"43-cr\xe9er-une-embedding-layer-\xe0-partir-de-plongements-existants",children:["4.3 Cr\xe9er une ",(0,i.jsx)(n.em,{children:"embedding"})," layer \xe0 partir de plongements existants"]}),"\n",(0,i.jsxs)(n.p,{children:["La fonction ",(0,i.jsx)(n.code,{children:"from_pretrained"})," permet de construire une ",(0,i.jsx)(n.em,{children:"embedding"})," layer \xe0 partir d'une matrice de poids correspondant \xe0\ndes plongements de mots. Comme dans l'exemple pr\xe9c\xe9dent, on peut par la suite utiliser cette couche pour convertir un ou plusieurs mots en embeddings."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"embedding_w0 = [ 0.0,  0.0, 0.0, 0.0, 0.0]  # Padding\nembedding_w1 = [ 0.3002,  0.0271, -0.7447, -0.1957, -0.3382]\nembedding_w2 = [-0.3433,  0.0522,  0.2301, -0.9601, -0.0239]\nembedding_w3 = [-0.1272, -0.6078, -0.7775,  2.8373, -0.4550]\nembeddings_tensor = torch.FloatTensor([embedding_w0, embedding_w1, embedding_w2, embedding_w3])\nembedding_layer = nn.Embedding.from_pretrained(embeddings_tensor, padding_idx=0)\nembedding_layer\n# Embedding(4, 5, padding_idx=0)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"text_ids = [1]\ntext_ids_tensor = torch.LongTensor(text_ids)\nembedding_layer(text_ids_tensor)\n# tensor([[ 0.3002,  0.0271, -0.7447, -0.1957, -0.3382]])\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"text_ids = [1, 3, 0]\ntext_ids_tensor = torch.LongTensor(text_ids)\nembedding_layer(text_ids_tensor)\n# tensor([[ 0.3002,  0.0271, -0.7447, -0.1957, -0.3382],\n#         [-0.1272, -0.6078, -0.7775,  2.8373, -0.4550],\n#         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n"})}),"\n",(0,i.jsx)(n.h3,{id:"44-exemple-avec-spacy",children:"4.4 Exemple avec Spacy"}),"\n",(0,i.jsxs)(n.p,{children:["On refait l'exemple pr\xe9c\xe9dent mais de mani\xe8re plus concr\xe8te pour construire une couche d'",(0,i.jsx)(n.em,{children:"embeddings"})," \xe0 partir d'un corpus\nde taille limit\xe9e et de plongements de mots pr\xe9entra\xeen\xe9s (ceux de Spacy). \xc0 noter ici qu'on travaille avec un vocabulaire\nferm\xe9, c.-\xe0-d. qu'on suppose qu'on connait \xe0 l'avance tous les mots de notre vocabulaire. Autrement dit, on\nsuppose qu'il n'y a pas de mot inconnu."]}),"\n",(0,i.jsx)(n.p,{children:"\xc0 noter qu'on ajoute ici un jeton de padding au vocabulaire, ce qui sera utile pour la section 5 du notebook."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"padding_token = '<PAD>'\npadding_id = 0\n\nsmall_corpus = [\"This is a test\", \"To be or not to be\", \"That is the question\"]\n\ndef get_vocabulary(corpus, padding=True):\n    \"\"\"Une version simplifi\xe9e pour construire un vocabulaire \xe0 partir des textes d'un corpus.\"\"\"\n    token_set = set()\n    for text in corpus:\n        tokens = text.split()  # on utiliserait normalement un tokeniseur pour cette \xe9tape\n        for token in tokens:\n            token_set.add(token)\n    vocabulary = list(token_set)\n    if padding:\n        vocabulary.insert(padding_id, padding_token)  # ou vocabulary = [padding_token] + vocabulary si id=0\n    return vocabulary\nvocab = get_vocabulary(small_corpus, padding=True)\nprint(vocab)\n['<PAD>', 'a', 'not', 'the', 'test', 'That', 'To', 'is', 'to', 'be', 'question', 'or', 'This']\n"})}),"\n",(0,i.jsx)(n.p,{children:"On cr\xe9e ici les structures de donn\xe9es qui nous permettent de convertir un mot en identifiant (et l'inverse)."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"index_to_word = {index: word for index, word in enumerate(vocab)}\nword_to_index = {word: index for index, word in enumerate(vocab)}\nprint(\"index_to_word:\", index_to_word)\nprint(\"\\nword_to_index:\", word_to_index)\n# index_to_word: {0: '<PAD>', 1: 'a', 2: 'not', 3: 'the', 4: 'test', 5: 'That', 6: 'To', 7: 'is',\n# 8: 'to', 9: 'be', 10: 'question', 11: 'or', 12: 'This'}\n#\n# word_to_index: {'<PAD>': 0, 'a': 1, 'not': 2, 'the': 3, 'test': 4, 'That': 5, 'To': 6, 'is': 7,\n# 'to': 8, 'be': 9, 'question': 10, 'or': 11, 'This': 12}\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def convert_to_id(text):\n    tokens = text.split()  # on utiliserait normalement un tokeniseur pour cette \xe9tape\n    return [word_to_index[token] for token in tokens]\n\nconvert_to_id("This is a question")\n# [12, 7, 1, 10]\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def get_embedding_tensor(vocab, spacy_analyzer, padding=True):\n    pseudo_text = " ".join(vocab)\n    doc = spacy_analyzer(pseudo_text)\n    _embeddings_ = [token.vector for token in doc]\n    if padding:\n        embedding_dim = len(embeddings[0])  # un petit truc pour conna\xeetre la longueur des _embeddings_\n        padding_emb = get_padding_embedding(embedding_dim)\n        # on suppose que le jeton de padding correspond \xe0 l\'identifiant 0\n        embeddings.insert(0, padding_emb)\n    return FloatTensor(embeddings)\n\ndef get_padding_embedding(dimension):\n    return [0] * dimension  # un _embedding_ de padding = un vecteur de 0\n\nspacy_embeddings = get_embedding_tensor(vocab, spacy_en)\nprint(spacy_embeddings.shape)\nspacy_embeddings\n# torch.Size([16, 300])\n# tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n#         [-0.6221,  0.2501, -0.0375,  ..., -0.0087, -0.5015,  0.1330],\n#         [ 0.3235,  0.3555,  0.0294,  ...,  0.3456,  0.2777, -0.0781],\n#         ...,\n#         [-0.2226,  0.2236, -0.2385,  ...,  0.0078,  0.4053,  0.2960],\n#         [-0.2285,  0.1390, -0.3711,  ..., -0.4301,  0.3176,  0.1462],\n#         [-0.0876,  0.3550,  0.0639,  ...,  0.0345, -0.1503,  0.4067]])\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"embedding_layer = nn.Embedding.from_pretrained(spacy_embeddings)\nembedding_layer\n# Embedding(16, 300)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'input_text = "That is not a question"\ninput_ids = torch.LongTensor(convert_to_id(input_text))\ninput_ids\n# tensor([ 5,  7,  2,  1, 10])\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"text_embeddings = embedding_layer(input_ids)\ntext_embeddings\n# tensor([[-0.0498,  0.0270, -0.3879,  ..., -0.3036,  0.2707,  0.1805],\n#         [ 0.0611,  0.2672,  0.2177,  ...,  0.1498,  0.4010, -0.0943],\n#         [ 0.3235,  0.3555,  0.0294,  ...,  0.3456,  0.2777, -0.0781],\n#         [-0.6221,  0.2501, -0.0375,  ..., -0.0087, -0.5015,  0.1330],\n#         [-0.0850,  0.5020,  0.0024,  ..., -0.2151, -0.2630, -0.0060]])\n"})}),"\n",(0,i.jsx)(n.p,{children:"On peut \xe9galement faire cette conversion pour une minibatch."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'input_texts = ["That is not question", "To be a test"]\nminibatch_ids = torch.LongTensor([convert_to_id(x) for x in input_texts])\nprint("Les IDs des 2 exemples:\\n", minibatch_ids)\n\nminibatch_embeddings = embedding_layer(minibatch_ids)\nprint("\\nLa minibatch qui en r\xe9sulte:\\n", minibatch_embeddings)\n# Les IDs des 2 exemples:\n#  tensor([[ 5,  7,  2, 10],\n#         [ 6,  9,  1,  4]])\n#\n# La minibatch qui en r\xe9sulte:\n#  tensor([[[-0.0498,  0.0270, -0.3879,  ..., -0.3036,  0.2707,  0.1805],\n#          [ 0.0611,  0.2672,  0.2177,  ...,  0.1498,  0.4010, -0.0943],\n#          [ 0.3235,  0.3555,  0.0294,  ...,  0.3456,  0.2777, -0.0781],\n#          [-0.0850,  0.5020,  0.0024,  ..., -0.2151, -0.2630, -0.0060]],\n#\n#         [[ 0.2720, -0.0620, -0.1884,  ...,  0.1302, -0.1832,  0.1323],\n#          [ 0.3192,  0.0632, -0.2786,  ...,  0.0827,  0.0978,  0.2504],\n#          [-0.6221,  0.2501, -0.0375,  ..., -0.0087, -0.5015,  0.1330],\n#          [ 0.0438,  0.0248, -0.2094,  ..., -0.3010, -0.1458,  0.2819]]])\n'})}),"\n",(0,i.jsx)(n.p,{children:"\xc0 noter ici que les textes d'une minibatch doivent \xeatre de m\xeame longueur. On revient sur ce point dans la section 5 sur le padding."}),"\n",(0,i.jsx)(n.p,{children:"On peut par la suite soumettre cette minibatch \xe0 une couche r\xe9currente."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'output, hidden = spacy_rnn_layer(minibatch_embeddings)\nprint("Tous les \xe9tats cach\xe9s des 2 exemples:\\n", output)\nprint("\\nL\'\xe9tat cach\xe9 du dernier mot de chacun des 2 exemples:\\n", hidden)\n# Tous les \xe9tats cach\xe9s des 2 exemples:\n#  tensor([[[ 0.9280,  0.9665, -0.4408, -0.9572],\n#          [-0.9912,  0.9825, -0.9495, -0.8383],\n#          [-0.9938, -0.0011, -0.9939,  0.9142],\n#          [ 0.8921,  0.8652, -0.1503, -0.9503]],\n#\n#         [[ 0.9163,  0.9763, -0.4424, -0.6474],\n#          [ 0.9873,  0.9823,  0.3500,  0.6399],\n#          [ 0.1647,  0.9123, -0.8965, -0.7907],\n#          [ 0.2726,  0.9968, -0.5654, -0.8592]]], grad_fn=<TransposeBackward1>)\n#\n# L\'\xe9tat cach\xe9 du dernier mot de chacun des 2 exemples:\n#  tensor([[[ 0.8921,  0.8652, -0.1503, -0.9503],\n#          [ 0.2726,  0.9968, -0.5654, -0.8592]]], grad_fn=<StackBackward>)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"45-ajout-dun-jeton-de-mot-inconnu-unknown-au-vocabulaire",children:"4.5 Ajout d'un jeton de mot inconnu (unknown) au vocabulaire"}),"\n",(0,i.jsx)(n.p,{children:"Lorsqu'on consid\xe8re que notre vocabulaire est ouvert (c.-\xe0-d. qu'on peut avoir de nouveaux mots\nlorsqu'on utilise un mod\xe8le), il est important de tenir compte de ce mot. L'approche de base consiste\n\xe0 ajouter un mot inconnu () dans notre vocabulaire et d'assigner tous les nouveaux mots \xe0 ce jeton."}),"\n",(0,i.jsx)(n.p,{children:"Cela peut \xeatre fait simplement en ajouter le jeton au vocabulaire et en modifiant l\xe9g\xe8rement la conversion des mots en identifiants."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"unk_id = 1\nunk_token = '<UNK>'\npadding_token = '<PAD>'\n\nvocab = [padding_token, unk_token, 'one', 'two', 'three']\nword2id = {token:id for id, token in enumerate(vocab)}\nword2id\n{'<PAD>': 0, '<UNK>': 1, 'one': 2, 'two': 3, 'three': 4}\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"tokens = ['one', 'two', 'three', 'tchatchatcha']\n# get(token, 1) retourne l'identifiant 1 par d\xe9faut\nsequence = [word2id.get(token, unk_id) for token in tokens]\nsequence\n[2, 3, 4, 1]\n"})}),"\n",(0,i.jsx)(n.h2,{id:"5-padding-des-exemples-dune-minibatch",children:"5. Padding des exemples d'une minibatch"}),"\n",(0,i.jsx)(n.p,{children:"Comme mentionn\xe9 pr\xe9c\xe9demment, tous les exemples d'une minibatch soumise \xe0 un r\xe9seau r\xe9current devrait avoir le m\xeame\nnombre de jetons. Comme les textes ont habituellement diff\xe9rentes longueurs, on fait un rembourrage en ajoutant des\njetons de padding aux textes plus courts."}),"\n",(0,i.jsx)(n.p,{children:"Voici un exemple comment proc\xe9der avec la fonction pad_sequence de PyTorch."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'input_texts = ["not a question", "To be a test question", "This is not"]\ninput_sequences = [convert_to_id(x) for x in input_texts]\ninput_sequences\n# [[2, 1, 10], [6, 9, 1, 4, 10], [12, 7, 2]]\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from torch.nn.utils.rnn import pad_sequence\n\n# on convertit les listes de Ids en tenseur\nminibatch_seqs = [torch.LongTensor(seq) for seq in input_sequences]\nx_padded = pad_sequence(minibatch_seqs, batch_first=True, padding_value=0)\nx_padded\n# tensor([[ 2,  1, 10,  0,  0],\n#         [ 6,  9,  1,  4, 10],\n#         [12,  7,  2,  0,  0]])\n"})}),"\n",(0,i.jsx)(n.p,{children:"On peut par la suite passer cette minibatch d'identifiants dans une couche d'embeddings pour obtenir les s\xe9quences\nsous forme de plongements. On reprend ici la couche embedding_layer de la section pr\xe9c\xe9dente pour faire cette conversion."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"embeddings = embedding_layer(x_padded)\nembeddings.size()\n# torch.Size([3, 5, 300])\nembeddings\n# tensor([[[ 0.3235,  0.3555,  0.0294,  ...,  0.3456,  0.2777, -0.0781],\n#          [-0.6221,  0.2501, -0.0375,  ..., -0.0087, -0.5015,  0.1330],\n#          [-0.0850,  0.5020,  0.0024,  ..., -0.2151, -0.2630, -0.0060],\n#          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n#          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n#\n#         [[ 0.2720, -0.0620, -0.1884,  ...,  0.1302, -0.1832,  0.1323],\n#          [ 0.3192,  0.0632, -0.2786,  ...,  0.0827,  0.0978,  0.2504],\n#          [-0.6221,  0.2501, -0.0375,  ..., -0.0087, -0.5015,  0.1330],\n#          [ 0.0438,  0.0248, -0.2094,  ..., -0.3010, -0.1458,  0.2819],\n#          [-0.0850,  0.5020,  0.0024,  ..., -0.2151, -0.2630, -0.0060]],\n#\n#         [[-0.0592,  0.1065, -0.2161,  ..., -0.4275,  0.0250,  0.0247],\n#          [ 0.0611,  0.2672,  0.2177,  ...,  0.1498,  0.4010, -0.0943],\n#          [ 0.3235,  0.3555,  0.0294,  ...,  0.3456,  0.2777, -0.0781],\n#          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n#          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n"})}),"\n",(0,i.jsxs)(n.h2,{id:"6-packing-des-exemples-dune-minibatch",children:["6. Packing des exemples d'une ",(0,i.jsx)(n.em,{children:"minibatch"})]}),"\n",(0,i.jsx)(n.p,{children:"L'utilisation du padding pour obtenir des s\xe9quences de m\xeame longueur peut mener \xe0 un traitement inefficace. Par exemple,\nsupposons qu'une minibatch contient 3 s\xe9quences de longueurs 3, 5 et 10. Apr\xe8s le rembourrage, on se retrouve\navec 3 s\xe9quences de longueurs 10."}),"\n",(0,i.jsx)(n.p,{children:"Pour mieux le visualiser:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"seqs = [[1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]]\nseq_tensors = [torch.LongTensor(seq) for seq in seqs]\nx_padded = pad_sequence(seq_tensors, batch_first=True, padding_value=0)\nx_padded\n# tensor([[ 1,  2,  3,  0,  0,  0,  0,  0,  0,  0],\n#         [ 4,  5,  6,  7,  8,  0,  0,  0,  0,  0],\n#         [ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]])\n"})}),"\n",(0,i.jsx)(n.p,{children:"Un RNN qui traitent ces 3 s\xe9quences devra faire 30 r\xe9currences, alors que seulement 18 sont requises.\nLe probl\xe8me se complique encore plus si on utilise des r\xe9seaux bidirectionnels."}),"\n",(0,i.jsxs)(n.p,{children:["Une mani\xe8re de contourner ce probl\xe8me avec PyTorch est de faire un ",(0,i.jsx)(n.em,{children:"packing"})," (un emballage) qui consiste \xe0\nr\xe9organiser les s\xe9quences par position (tous les \xe9l\xe9ments des s\xe9quences en position 1, suivi des \xe9l\xe9ments en\nposition 2, suivi...) afin d'\xe9liminer les 0 et d'\xe9viter les calculs inutiles."]}),"\n",(0,i.jsxs)(n.p,{children:["On peut obtenir une repr\xe9sentation emball\xe9e (packed) d'une minibatch avec la fonction ",(0,i.jsx)(n.code,{children:"pack_padded_sequences"}),"\nqui utilise les s\xe9quences rembourr\xe9es ainsi que leurs longeurs originales avant emballage."]}),"\n",(0,i.jsx)(n.p,{children:"Pour notre exemple, on obtient le packing suivant:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from torch.nn.utils.rnn import pack_padded_sequence\n\noriginal_lengths = [len(seq) for seq in seqs]\nprint("Les longueurs des s\xe9quences originales:", original_lengths)\nx_packed = pack_padded_sequence(x_padded, torch.LongTensor(original_lengths), batch_first=True, enforce_sorted=False)\nx_packed\n# Les longueurs des s\xe9quences originales: [3, 5, 10]\n# PackedSequence(data=tensor([ 9,  4,  1, 10,  5,  2, 11,  6,  3, 12,  7, 13,  8, 14, 15, 16, 17, 18]),\n# batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1, 1, 1, 1]), sorted_indices=tensor([2, 1, 0]),\n# unsorted_indices=tensor([2, 1, 0]))\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'print("Les s\xe9quences emball\xe9es:", x_packed.data.tolist())\n# Les s\xe9quences emball\xe9es: [9, 4, 1, 10, 5, 2, 11, 6, 3, 12, 7, 13, 8, 14, 15, 16, 17, 18]\n'})}),"\n",(0,i.jsx)(n.p,{children:"On note ici que les s\xe9quences ont \xe9t\xe9 r\xe9ordonnanc\xe9es de la plus longue \xe0 la plus courte avant d'\xeatre emball\xe9es.\nCe tri en ordre d\xe9croissant est avantageux dans certaines conditions (par ex. si vous roulez votre code sur un GPU)."}),"\n",(0,i.jsx)(n.p,{children:"On peut facilement retrouver les s\xe9quences rembourr\xe9es \xe0 partir des s\xe9quences emball\xe9es comme suit:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from torch.nn.utils.rnn import pad_packed_sequence\n\nx_padded_reconstructed = pad_packed_sequence(x_packed, batch_first=True)\nx_padded_reconstructed\n# (tensor([[ 1,  2,  3,  0,  0,  0,  0,  0,  0,  0],\n#          [ 4,  5,  6,  7,  8,  0,  0,  0,  0,  0],\n#          [ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]]),\n#  tensor([ 3,  5, 10]))\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Cette fonction pour r\xe9cup\xe9rer les s\xe9quences rembourr\xe9es est \xe9galement utilis\xe9e pour r\xe9cup\xe9rer l'output d'un r\xe9seau r\xe9current qui traite en entr\xe9e un input ",(0,i.jsx)(n.em,{children:"packed"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"7-exemple-de-r\xe9seau-r\xe9current-simple-pour-un-probl\xe8me-de-classification",children:"7. Exemple de r\xe9seau r\xe9current simple pour un probl\xe8me de classification"}),"\n",(0,i.jsx)(n.p,{children:"Un exemple simple pour illustrer 3 parties : les donn\xe9es, le r\xe9seau et l'entra\xeenement."}),"\n",(0,i.jsx)(n.p,{children:"Donn\xe9es: des questions de 3 cat\xe9gories - LOCATION, DEFINITION et TEMPORAL"}),"\n",(0,i.jsxs)(n.p,{children:["R\xe9seau: Il contient une couche GRU ainsi qu'une ",(0,i.jsx)(n.em,{children:"embedding layer"})," pour convertir les textes en plongements de mots.\nPour faire la classification des textes, on utilise une couche lin\xe9aire dont la sortie contient une neurone par classe."]}),"\n",(0,i.jsx)(n.p,{children:"Entra\xeenement: Avec Poutyne comme dans les exemples pr\xe9c\xe9dents du cours."}),"\n",(0,i.jsx)(n.h3,{id:"71-la-cr\xe9ation-du-jeu-de-donn\xe9es-et-des-fonctions-utilitaires",children:"7.1 La cr\xe9ation du jeu de donn\xe9es et des fonctions utilitaires"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import json\n\ndef load_dataset(filename):\n    with open(filename, 'r') as infile:\n        content = json.load(infile)\n    return content[\"questions\"], content[\"labels\"]\n\ntrain_dataset_path = \"./data/questions_small.json\"\nquestions, labels = load_dataset(train_dataset_path)\n\nprint(\"{} exemples de cat\xe9gories {}.\".format(len(labels), set(labels)))\n# 1616 exemples de cat\xe9gories {'LOCATION', 'DEFINITION', 'TEMPORAL'}.\n"})}),"\n",(0,i.jsx)(n.p,{children:"On convertit les classes en identifiants."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"label_list = list(set(labels))\nid2label = {label_id: value for label_id, value in enumerate(label_list)}\nlabel2id = {value: label_id for label_id, value in enumerate(label_list)}\nnb_classes = len(label_list)\nprint(label2id)\n# {'LOCATION': 0, 'DEFINITION': 1, 'TEMPORAL': 2}\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"y = [label2id[label] for label in labels]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(questions, y, test_size=0.1, shuffle=True,random_state=42)\nprint(\"Nb exemples d'entra\xeenement: {}, Nb d'exemples de validation: {}.\".format(len(y_train), len(y_valid)))\n# Nb exemples d'entra\xeenement: 1454, Nb d'exemples de validation: 162.\n"})}),"\n",(0,i.jsx)(n.p,{children:"On construit le vocabulaire et les fonctions utilitaires pour faire la conversion des mots en identifiant."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import spacy\n\nnlp = spacy.load('en_core_web_md')\nembedding_size = nlp.meta['vectors']['width']\n"})}),"\n",(0,i.jsxs)(n.p,{children:["On d\xe9bute par ajouter les jetons de ",(0,i.jsx)(n.em,{children:"padding"})," et de mot inconnu au vocabulaire pour travailler \xe0 vocabulaire ouvert."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\npadding_token = "<PAD>"   # mot 0\nunk_token = "<UNK>"    # mot 1\nzero_vec_embedding = np.zeros(embedding_size, dtype=np.float64)\n\nid2word = {}\nid2word[0] = padding_token\nid2word[1] = unk_token\n\nword2id = {}\nword2id[padding_token] = 0\nword2id[unk_token] = 1\n\nid2embedding = {}\nid2embedding[0] = zero_vec_embedding\nid2embedding[1] = zero_vec_embedding\n\nword_index = 2\nvocab = word2id.keys()\nfor question in X_train:\n    for word in nlp(question):\n        if word.text not in vocab:\n            word2id[word.text] = word_index\n            id2word[word_index] = word.text\n            id2embedding[word_index] = word.vector\n            word_index += 1\n'})}),"\n",(0,i.jsx)(n.p,{children:"On termine avec les classes de Dataset qui g\xe8rent nos donn\xe9es d'entra\xeenement et de validation."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\n\nfrom torch import LongTensor\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\n\nclass QuestionDataset(Dataset):\n    def __init__(self, data , targets, word_to_id, spacy_model):\n        self.data = data\n        self.sequences = [None for _ in range(len(data))]\n        self.targets = targets\n        self.word2id = word_to_id\n        self.tokenizer = spacy_model\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        if self.sequences[index] is None:\n            self.sequences[index] = self.tokenize(self.data[index])\n        return LongTensor(self.sequences[index]), LongTensor([self.targets[index]]).squeeze(0)\n\n    def tokenize(self, sentence):\n        tokens = [word.text for word in self.tokenizer(sentence)]\n        return [self.word2id.get(token, 1) for token in tokens]  # get(token, 1) retourne 1 par d\xe9faut si mot inconnu\n\n\ntrain_dataset = QuestionDataset(X_train, y_train, word2id, nlp)\nvalid_dataset = QuestionDataset(X_valid, y_valid, word2id, nlp)\n\nvalid_dataset[20]\n# (tensor([  59,    3,    4,    1,    1, 1318,    7]), tensor(2))\n"})}),"\n",(0,i.jsx)(n.h3,{id:"72-le-r\xe9seau-gru",children:"7.2 Le r\xe9seau GRU"}),"\n",(0,i.jsxs)(n.p,{children:["Un structure standard avec une couche d'embeddings, un couche GRU et une couche lin\xe9aire qui sert de t\xeate de pr\xe9diction\nsur le dernier \xe9tat cach\xe9 du r\xe9seau r\xe9current. La couche d'embeddings est cr\xe9\xe9e \xe0 partir d'une matrice de poids\nconstruite obtenues avant la cr\xe9ation du r\xe9seau. \xc0 noter qu'on fait de l'emballage (",(0,i.jsx)(n.em,{children:"packing"}),") qui permet d'\xe9viter les\nr\xe9currences superflues. Le padding des batchs d'entra\xeenement est effectu\xe9 par les ",(0,i.jsx)(n.em,{children:"dataloaders"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass GRUClassifier(nn.Module):\n    def __init__(self, embeddings, hidden_state_size, nb_classes) :\n        super(GRUClassifier, self).__init__()\n        self.embedding_layer = nn.Embedding.from_pretrained(embeddings)\n        self.embedding_size = embeddings.size()[1]\n        self.rnn = nn.GRU(self.embedding_size, hidden_state_size, 1, batch_first=True)\n        self.classification_layer = nn.Linear(hidden_state_size, nb_classes)\n\n    def forward(self, x, x_lengths):\n        x = self.embedding_layer(x)\n        packed_batch = pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n        _, last_hidden_state = self.rnn(packed_batch)  # On utilise le hidden state de la derni\xe8re cellule\n        x = last_hidden_state.squeeze()  # Le GRU a une seule couche, on retire cette dimension\n        x = self.classification_layer(x)\n        return x\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Comme nous avons une t\xe2che de classification de texte, nous n'avons besoin que de l'\xe9tat cach\xe9 du dernier mot.\nCet \xe9tat est disponible en sortie du r\xe9seau r\xe9current (2e valeur de sortie). L'approche serait l\xe9g\xe8rement diff\xe9rente\npour une t\xe2che d'\xe9tiquetage de mots o\xf9 on doit r\xe9cup\xe9rer les \xe9tats cach\xe9s de tous les mots apr\xe8s avoir d\xe9semball\xe9\nla sortie du r\xe9seau (avec la fonction ",(0,i.jsx)(n.code,{children:"pad_packed_sequence"}),"). Pour un exemple de token classification (\xe9tiquetage de mots),\nvoir le tutoriel suivant sur l'analyse d'adresses postales ",(0,i.jsx)(n.a,{href:"https://www.dotlayer.org/en/categories/address-parsing/",children:"lien"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["On cr\xe9e maintenant les classes de Dataloaders. Cette classe a la responsabilit\xe9 de faire le padding des exemples de la\nbatch. On utilise une fonction utilitaire ",(0,i.jsx)(n.code,{children:"pad_batch"})," qui est pass\xe9 en argument \xe0 la cr\xe9ation du dataloader."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def pad_batch(batch):\n    x = [x for x,y in batch]\n    x_true_length = [len(x) for x,y in batch]\n    y = torch.stack([y for x,y in batch], dim=0)\n    return ((pad_sequence(x, batch_first=True), x_true_length), y)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n\nnext(iter(train_dataloader))  # Un exemple de batch\n# ((tensor([[ 224,   91,  105,    4,   86, 2143,  567,   26, 2144,   44, 2145, 2146,\n#               7,    0,    0],\n#           [   2,    8,    4, 1926, 1927, 1033, 1550,    7,    0,    0,    0,    0,\n#               0,    0,    0],\n#           [   2,   33,  101,  101, 2059,  104,  100,    7,    0,    0,    0,    0,\n#               0,    0,    0],\n#           [   2,    8,  101,  101,  147,  148,  149,  104,    7,    0,    0,    0,\n# ...\n#             403,  404,    7],\n#           [   2,    8, 2200,    7,    0,    0,    0,    0,    0,    0,    0,    0,\n#               0,    0,    0],\n#           [   2,    8,   26, 2043, 2044,    7,    0,    0,    0,    0,    0,    0,\n#               0,    0,    0],\n#           [  59,    3,  320,  321,    7,    0,    0,    0,    0,    0,    0,    0,\n#               0,    0,    0]]),\n#   [13, 8, 8, 9, 5, 8, 4, 7, 6, 6, 9, 4, 15, 4, 6, 5]),\n#  tensor([0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 0, 1, 1, 2]))\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Maintenant on construit la table d'",(0,i.jsx)(n.em,{children:"embeddings"})," qui sera pass\xe9e en argument au mod\xe8le lors de sa cr\xe9ation."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"vocab_size = len(id2embedding)\nembedding_layer = np.zeros((vocab_size, embedding_size), dtype=np.float32)\nfor token_id, embedding in id2embedding.items():\n    embedding_layer[token_id,:] = embedding\nembedding_layer = torch.from_numpy(embedding_layer)\n\nprint(\"Taille de la couche d'embeddings:\", embedding_layer.shape)\n# Taille de la couche d'embeddings: torch.Size([3022, 300])\n"})}),"\n",(0,i.jsx)(n.p,{children:"Finalement on construit notre mod\xe8le de classification de questions."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from poutyne import set_seeds\n\nset_seeds(42)\nhidden_size = 100  # choisi arbitrairement\nmodel = GRUClassifier(embedding_layer, hidden_size, nb_classes)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"73-lentra\xeene-avec-poutyne",children:"7.3 L'entra\xeene avec Poutyne"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from poutyne.framework import Experiment\n\nexperiment = Experiment(\'model/gru_classification\', model, optimizer = "SGD", task="classification")\nlogging = experiment.train(train_dataloader, valid_dataloader, epochs=25, disable_tensorboard=True)\n# Epoch:  1/25 Train steps: 91 Val steps: 11 10.94s loss: 1.004985 acc: 50.687758 fscore_macro: 0.249522 val_loss: 0.952887 val_acc: 56.172840 val_fscore_macro: 0.239789\n# Epoch 1: val_acc improved from -inf to 56.17284, saving file to model/gru_classification/checkpoint_epoch_1.ckpt\n# Epoch:  2/25 Train steps: 91 Val steps: 11 0.97s loss: 0.956208 acc: 52.613480 fscore_macro: 0.229833 val_loss: 0.937381 val_acc: 56.172840 val_fscore_macro: 0.239789\n# Epoch:  3/25 Train steps: 91 Val steps: 11 1.02s loss: 0.935598 acc: 52.819807 fscore_macro: 0.234294 val_loss: 0.918388 val_acc: 56.172840 val_fscore_macro: 0.239789\n# Epoch:  4/25 Train steps: 91 Val steps: 11 1.14s loss: 0.912273 acc: 54.470426 fscore_macro: 0.269421 val_loss: 0.900670 val_acc: 59.876543 val_fscore_macro: 0.320687\n# ...\n# Epoch: 24/25 Train steps: 91 Val steps: 11 0.72s loss: 0.275926 acc: 90.646492 fscore_macro: 0.858318 val_loss: 0.272842 val_acc: 90.740741 val_fscore_macro: 0.868657\n# Epoch 24: val_acc improved from 90.12346 to 90.74074, saving file to model/gru_classification/checkpoint_epoch_24.ckpt\n# Epoch: 25/25 Train steps: 91 Val steps: 11 0.84s loss: 0.257278 acc: 91.471802 fscore_macro: 0.872796 val_loss: 0.269933 val_acc: 89.506173 val_fscore_macro: 0.862955\n# Restoring data from model/gru_classification/checkpoint_epoch_24.ckpt\n'})}),"\n",(0,i.jsx)(n.h3,{id:"74-quelques-pr\xe9dictions-pour-tester-le-tout",children:"7.4 Quelques pr\xe9dictions pour tester le tout"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from numpy import argmax\n\ndef obtain_prediction(sentence, label=None):\n    tokens = [word.text for word in nlp(sentence)]\n    sequence = [word2id.get(token, 1) for token in tokens]\n    sentence_length = len(sequence)\n    class_scores = model(LongTensor(sequence).unsqueeze(0), LongTensor([sentence_length])).detach().numpy()\n    best_score_id = argmax(class_scores)\n    return id2label[best_score_id]\n\nquestions = ["Where is Milan ?",\n             "What is fibromyalgia ?",\n             "When did Elvis Presley die ?"]\n\nfor question in questions:\n    prediction = obtain_prediction(question)\n    print("Question: {} \\tPrediction: {}".format(question, prediction))\n# Question: Where is Milan ?  Prediction: LOCATION\n# Question: What is fibromyalgia ?  Prediction: DEFINITION\n# Question: When did Elvis Presley die ?   Prediction: TEMPORAL\n'})})]})}function u(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},11151:(e,n,s)=>{s.d(n,{Z:()=>a,a:()=>d});var i=s(67294);const t={},r=i.createContext(t);function d(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:d(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);