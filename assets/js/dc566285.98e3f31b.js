"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[1780],{64970:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var a=t(85893),i=t(11151);const r={sidebar_label:"Retrieval Augmented Generation for Production with LangChain & LlamaIndex",sidebar_position:7},s="Retrieval Augmented Generation for Production with LangChain & LlamaIndex",o={id:"courses/activeloop/rag-for-production/index",title:"Retrieval Augmented Generation for Production with LangChain & LlamaIndex",description:"Module 1 LangChain: Basic Concepts Recap",source:"@site/docs/courses/activeloop/rag-for-production/index.md",sourceDirName:"courses/activeloop/rag-for-production",slug:"/courses/activeloop/rag-for-production/",permalink:"/docs/courses/activeloop/rag-for-production/",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/activeloop/rag-for-production/index.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_label:"Retrieval Augmented Generation for Production with LangChain & LlamaIndex",sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"Build LLM Apps with LangChain.js",permalink:"/docs/courses/deeplearning-ai/sp09-langchain-js"},next:{title:"Training and fine-tuning LLMs",permalink:"/docs/courses/fine-tuning-llms"}},l={},d=[{value:"Module 1 LangChain: Basic Concepts Recap",id:"module-1-langchain-basic-concepts-recap",level:2},{value:"Chat Model",id:"chat-model",level:3},{value:"Embedding Model",id:"embedding-model",level:3},{value:"LLMChain",id:"llmchain",level:3},{value:"Sequential",id:"sequential",level:3},{value:"See also",id:"see-also",level:3},{value:"LlamaIndex Introduction: Precision and Simplicity in Information Retrieval",id:"llamaindex-introduction-precision-and-simplicity-in-information-retrieval",level:2},{value:"Data Connectors",id:"data-connectors",level:3},{value:"Nodes",id:"nodes",level:3},{value:"Indices",id:"indices",level:3},{value:"Query Engines",id:"query-engines",level:3},{value:"Routers",id:"routers",level:3},{value:"Saving and Loading Indexes Locally",id:"saving-and-loading-indexes-locally",level:3},{value:"Challenges with Naive RAG",id:"challenges-with-naive-rag",level:3},{value:"Evaluation",id:"evaluation",level:3},{value:"Chat with Chat with Your Code: LlamaIndex and Activeloop Deep Lake for GitHub Repositories \ud83e\udd99\ud83c\udf0a",id:"chat-with-chat-with-your-code-llamaindex-and-activeloop-deep-lake-for-github-repositories-",level:3},{value:"What are LlamaIndex and Activeloop Deep Lake?",id:"what-are-llamaindex-and-activeloop-deep-lake",level:4},{value:"LlamaIndex: Your Data Framework for LLMs",id:"llamaindex-your-data-framework-for-llms",level:5},{value:"Activeloop Deep Lake: Optimized Data Lake for ML",id:"activeloop-deep-lake-optimized-data-lake-for-ml",level:5},{value:"The Synergy",id:"the-synergy",level:5},{value:"Requirements",id:"requirements",level:4},{value:"Getting Started",id:"getting-started",level:4},{value:"Install the required packages",id:"install-the-required-packages",level:4},{value:"1. llama-index",id:"1-llama-index",level:5},{value:"2. deeplake",id:"2-deeplake",level:5},{value:"3. OpenAI",id:"3-openai",level:5},{value:"4. python-dotenv",id:"4-python-dotenv",level:5},{value:"How does LLamaIndex work?",id:"how-does-llamaindex-work",level:4},{value:"Let&#39;s code",id:"lets-code",level:4},{value:"Diving Deeper: LlamaIndex&#39;s Low-Level API for customizations",id:"diving-deeper-llamaindexs-low-level-api-for-customizations",level:4},{value:"Building the Index",id:"building-the-index",level:5},{value:"Configuring the Retriever",id:"configuring-the-retriever",level:5},{value:"Customize the query engine",id:"customize-the-query-engine",level:5},{value:"Exploring Different Response Modes",id:"exploring-different-response-modes",level:5},{value:"How does LLamaIndex compare to LangChain?",id:"how-does-llamaindex-compare-to-langchain",level:4},{value:"Key Features",id:"key-features",level:5},{value:"Use Cases",id:"use-cases",level:5},{value:"More info on LlamaIndex",id:"more-info-on-llamaindex",level:3},{value:"Module 2 Introduction - Advanced Retrieval Augmented Generation",id:"module-2-introduction---advanced-retrieval-augmented-generation",level:2},{value:"Fine-tuning vs RAG; Introduction to Activeloop&#39;s Deep Memory",id:"fine-tuning-vs-rag-introduction-to-activeloops-deep-memory",level:3},{value:"Overview of RAG Enhancement Techniques",id:"overview-of-rag-enhancement-techniques",level:4},{value:"Prompt engineering",id:"prompt-engineering",level:5},{value:"Fine-tuning",id:"fine-tuning",level:5},{value:"Retrieval-Augmented Generation",id:"retrieval-augmented-generation",level:5},{value:"RAG + Fine-tuning",id:"rag--fine-tuning",level:5},{value:"Enhanced RAG with Deep Memory",id:"enhanced-rag-with-deep-memory",level:4},{value:"Comparison to Lexical search",id:"comparison-to-lexical-search",level:5},{value:"Overview of Deep Memory",id:"overview-of-deep-memory",level:4},{value:"Step by Step - Training a Deep Memory Model",id:"step-by-step---training-a-deep-memory-model",level:4}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"retrieval-augmented-generation-for-production-with-langchain--llamaindex",children:"Retrieval Augmented Generation for Production with LangChain & LlamaIndex"}),"\n",(0,a.jsx)(n.h2,{id:"module-1-langchain-basic-concepts-recap",children:"Module 1 LangChain: Basic Concepts Recap"}),"\n",(0,a.jsx)(n.h3,{id:"chat-model",children:"Chat Model"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage\n\nchat = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)\nmessages = [\n    SystemMessage(\n        content="You are a helpful assistant."\n    ),\n    HumanMessage(\n        content="What is the capital of France?"\n    ),\n]\nanswer = chat(messages)\nprint(answer)\n# AIMessage(content=\'The capital of France is Paris.\')\n'})}),"\n",(0,a.jsx)(n.h3,{id:"embedding-model",children:"Embedding Model"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.embeddings import OpenAIEmbeddings\n\n# Initialize the model\nembeddings_model = OpenAIEmbeddings()\n\n# Embed a list of texts\nembeddings = embeddings_model.embed_documents(\n    ["Hi there!", "Oh, hello!", "What\'s your name?", "My friends call me World", "Hello World!"]\n)\n\nprint("Number of documents embedded:", len(embeddings))\nprint("Dimension of each embedding:", len(embeddings[0]))\n# Number of documents embedded: 5\n# Dimension of each embedding: 1536\n'})}),"\n",(0,a.jsx)(n.h3,{id:"llmchain",children:"LLMChain"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.llmchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import StrOutputParser\n\ntemplate = """List all the colors in a rainbow"""\nprompt = PromptTemplate(\n    template=template, input_variables=[], output_parser=StrOutputParser()\n)\n\nchat = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)\nllm_chain = LLMChain(prompt=prompt, llm=chat)\n\nllm_chain.predict()\n# The colors in a rainbow are:\\n\\n1. Red\\n2. Orange\\n3. Yellow\\n4. Green\\n5. Blue\\n6. Indigo\\n7. Violet\n'})}),"\n",(0,a.jsx)(n.p,{children:"The same code but with LangChain Expression Language (LCEL)."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import StrOutputParser\n\nprompt = PromptTemplate.from_template(\n    "List all the colors in a {item}."\n)\nrunnable = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\nrunnable.invoke({"item": "rainbow"})\n# The colors in a rainbow are:\\n\\n1. Red\\n2. Orange\\n3. Yellow\\n4. Green\\n5. Blue\\n6. Indigo\\n7. Violet\n'})}),"\n",(0,a.jsx)(n.h3,{id:"sequential",children:"Sequential"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.prompts import PromptTemplate\n\npost_prompt = PromptTemplate.from_template(\n    """You are a business owner. Given the theme of a post, write a social media post to share on my socials.\n\nTheme: {theme}\nContent: This is social media post based on the theme above:"""\n)\n\nreview_prompt = PromptTemplate.from_template(\n    """You are an expert social media manager. Given the presented social media post, it is your job to write a review for the post.\n\nSocial Media Post:\n{post}\nReview from a Social Media Expert:"""\n)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import StrOutputParser\n\nllm = ChatOpenAI(temperature=0.0)\n\nchain = (\n    {"post": post_prompt | llm | StrOutputParser()}\n    | review_prompt\n    | llm\n    | StrOutputParser()\n)\nchain.invoke({"theme": "Having a black friday sale with 50% off on everything."})\n'})}),"\n",(0,a.jsx)(n.p,{children:"The previous code returns something like this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"This social media post is highly effective in promoting the Black Friday sale. The use of emojis and exclamation marks adds excitement and grabs the attention of the audience. The post clearly states the offer - a 50% off on everything in-store and online, which is a great deal. It also highlights the variety of products available, from trendy fashion pieces to must-have accessories, appealing to a wide range of customers.\n\nThe post encourages urgency by mentioning that the sale is for a limited time only and advises customers to arrive early to catch the best deals. The call to action is clear, directing customers to visit the website or head to the store to explore the products. The post also encourages customers to spread the word and tag their shopping buddies, which can help increase the reach and engagement of the post.\n\nOverall, this social media post effectively communicates the details of the Black Friday sale, creates excitement, and encourages customers to take advantage of the unbeatable offer. It is well-written, visually appealing, and likely to generate interest and engagement from the audience.\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.schema.runnable import RunnablePassthrough\n\nllm = ChatOpenAI(temperature=0.0)\n\npost_chain = post_prompt | llm | StrOutputParser()\nreview_chain = review_prompt | llm | StrOutputParser()\nchain = {"post": post_chain} | RunnablePassthrough.assign(review=review_chain)\nchain.invoke({"theme": "Having a black friday sale with 50% off on everything."})\n'})}),"\n",(0,a.jsx)(n.p,{children:"The previous code returns something like this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"{'post': \"\ud83c\udf89 BLACK FRIDAY SALE ALERT! \ud83c\udf89\\n\\nGet ready to shop till you drop because we're bringing you an EPIC Black Friday Sale! \ud83d\udecd\ufe0f\ud83d\udd25 Enjoy a whopping 50% OFF on EVERYTHING in-store and online! \ud83c\udf81\u2728\\n\\nFrom trendy fashion pieces to must-have accessories, we've got you covered! Whether you're looking to revamp your wardrobe or find the perfect holiday gifts, now is the time to snag incredible deals! \ud83d\udc83\ud83d\udcbc\\n\\nDon't miss out on this incredible opportunity to save big and treat yourself or your loved ones! \ud83d\ude4c Mark your calendars and set those reminders because this sale is going to be LEGENDARY! \ud83d\udcc5\ud83d\udca5\\n\\nVisit our website or head to our store to explore our wide range of products and take advantage of this unbeatable offer. Remember, the early bird catches the best deals, so be sure to arrive early! \u23f0\ud83c\udfc3\\u200d\u2640\ufe0f\\n\\nSpread the word and tag your shopping buddies below! Let's make this Black Friday the most unforgettable shopping experience ever! \ud83d\uded2\ud83d\udc83\\n\\n#BlackFridaySale #50PercentOff #ShopTillYouDrop #UnbeatableDeals #TreatYourself #HolidayShopping #MustHaveItems #FashionFinds #SaveBig #LimitedTimeOffer\",\n 'review': 'This social media post is highly effective in promoting the Black Friday sale. The use of emojis and exclamation marks adds excitement and grabs the attention of the audience. The post clearly states the offer - a 50% off on everything in-store and online, which is a great deal. It also highlights the variety of products available, from trendy fashion pieces to must-have accessories, appealing to a wide range of customers.\\n\\nThe post encourages urgency by mentioning the limited time offer and the need to arrive early to catch the best deals. It also encourages engagement by asking followers to tag their shopping buddies, which can help spread the word and increase the reach of the post.\\n\\nThe use of relevant hashtags, such as #BlackFridaySale and #ShopTillYouDrop, helps the post to be easily discoverable by users searching for Black Friday deals. Overall, this post effectively communicates the sale details, creates excitement, and encourages engagement, making it a successful social media promotion for the Black Friday sale.'}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"see-also",children:"See also"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://python.langchain.com/docs/expression_language/",children:"LangChain Expression Language (LCEL)"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://medium.com/aimonks/simple-guide-to-text-chunking-for-your-llm-applications-bddfe8ad7892",children:"Simple guide to Text Chunking for Your LLM Applications"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://medium.com/@ashu.goel_9925/understanding-text-chunking-for-the-llm-application-da59cbc2855b",children:"Understanding Text Chunking for the LLM Application"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://nanonets.com/blog/langchain/#module-ii-retrieval",children:"A Complete LangChain Guide"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://www.davidgentile.net/langchain-models/",children:"LangChain Models: Simple and Consistent Interfaces for LLMs, Chat, and Text Embeddings"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://docs.kanaries.net/articles/langchain-openai#chains-combining-llms-and-prompts",children:"Unleash the Potential of LangChain in Web Application Development"})}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"llamaindex-introduction-precision-and-simplicity-in-information-retrieval",children:"LlamaIndex Introduction: Precision and Simplicity in Information Retrieval"}),"\n",(0,a.jsx)(n.h3,{id:"data-connectors",children:"Data Connectors"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://llamahub.ai/",children:"LlamaHub"})," is an open-source project that hosts data connectors."]}),"\n",(0,a.jsxs)(n.p,{children:["Before testing loaders, we must install the required packages and set the OpenAI API key for LlamaIndex.\nYou can get the API key on ",(0,a.jsx)(n.a,{href:"https://platform.openai.com/playground",children:"OpenAI's website"})," and set the\nenvironment variable with ",(0,a.jsx)(n.code,{children:"OPENAI_API_KEY"}),". Please note that LlamaIndex defaults to using OpenAI's ",(0,a.jsx)(n.code,{children:"gpt-3.5-turbo"}),"\nfor text generation and ",(0,a.jsx)(n.code,{children:"text-embedding-ada-002"})," model for embedding generation."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install -q llama-index==0.9.14.post3 openai==1.3.8 cohere==4.37\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import os\nimport logging\nimport sys\n\nfrom llama_index import download_loader\n\n# You can set the logging level to DEBUG for more verbose output,\n# or use level=logging.INFO for less detailed information.\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nos.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_API_KEY>'\n\nWikipediaReader = download_loader(\"WikipediaReader\")\nloader = WikipediaReader()\n\ndocuments = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\nprint(len(documents))\n# 2\n"})}),"\n",(0,a.jsx)(n.h3,{id:"nodes",children:"Nodes"}),"\n",(0,a.jsxs)(n.p,{children:["In LlamaIndex, once data is ingested as documents, it passes through a processing structure that transforms these\ndocuments into ",(0,a.jsx)(n.code,{children:"Node"})," objects. Nodes are smaller, more granular data units created from the original\ndocuments. Besides their primary content, these nodes also contain metadata and contextual information."]}),"\n",(0,a.jsxs)(n.p,{children:["LlamaIndex features a ",(0,a.jsx)(n.code,{children:"NodeParser"})," class designed to convert the content of documents into structured\nnodes automatically. The ",(0,a.jsx)(n.code,{children:"SimpleNodeParser"})," converts a list of document objects into nodes."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from llama_index.node_parser import SimpleNodeParser\n\n# Assuming documents have already been loaded\n\n# Initialize the parser\nparser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=20)\n\n# Parse documents into nodes\nnodes = parser.get_nodes_from_documents(documents)\nprint(len(nodes))\n# 48\n"})}),"\n",(0,a.jsx)(n.p,{children:"The code above splits the two retrieved documents from the Wikipedia page into 48 smaller chunks with slight overlap."}),"\n",(0,a.jsx)(n.h3,{id:"indices",children:"Indices"}),"\n",(0,a.jsx)(n.p,{children:"At the heart of LlamaIndex is the capability to index and search various data formats like documents,\nPDFs, and database queries. Indexing is an initial step for storing information in a database; it\nessentially transforms the unstructured data into embeddings that capture semantic meaning and\noptimize the data format so it can be easily accessed and queried."}),"\n",(0,a.jsx)(n.p,{children:"LlamaIndex has a variety of index types, each fulfills a specific role. We have highlighted some\nof the popular index types in the following subsections."}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/DocSummary.html",children:"Summary Index"}),"\nextracts a summary from each document and stores it with all the nodes in that document. Since it\u2019s not always\neasy to match small node embeddings with a query, sometimes having a document summary helps."]}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_guide.html",children:"Vector Store Index"}),"\ngenerates embeddings during index construction to identify the top-k most similar nodes in response to a query.\nIt\u2019s suitable for small-scale applications and easily scalable to accommodate larger datasets using high-performance vector databases.\nSee also ",(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/community/integrations/vector_stores.html",children:"Vector Store"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["The crawled Wikipedia documents can be stored in a Deep Lake vector store, and an index object can be created based on\nits data. We can create the dataset in ",(0,a.jsx)(n.a,{href:"https://www.activeloop.ai/",children:"Activeloop"})," and append documents to it by\nemploying the ",(0,a.jsx)(n.code,{children:"DeepLakeVectorStore"})," class. First, we need to set the Activeloop and OpenAI API keys in the environment using the following code."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import os\n\nos.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_API_KEY>'\nos.environ['ACTIVELOOP_TOKEN'] = '<YOUR_ACTIVELOOP_KEY>'\n"})}),"\n",(0,a.jsxs)(n.p,{children:["To connect to the platform, use the ",(0,a.jsx)(n.code,{children:"DeepLakeVectorStore"})," class and provide the ",(0,a.jsx)(n.code,{children:"dataset_path"})," as an argument.\nTo save the dataset on your workspace, you can replace the ",(0,a.jsx)(n.code,{children:"genai360"})," name with your organization ID\n(which defaults to your Activeloop username). Running the following code will create an empty dataset."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from llama_index.vector_stores import DeepLakeVectorStore\n\nmy_activeloop_org_id = "genai360"\nmy_activeloop_dataset_name = "LlamaIndex_intro"\ndataset_path = f"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}"\n\n# Create an index over the documnts\nvector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=False)\n# Your Deep Lake dataset has been successfully created!\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Now, we need to create a storage context using the ",(0,a.jsx)(n.code,{children:"StorageContext"})," class and the Deep Lake dataset as the source.\nPass this storage to a ",(0,a.jsx)(n.code,{children:"VectorStoreIndex"})," class to create the index (generate embeddings) and store the results on the defined dataset."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from llama_index.storage.storage_context import StorageContext\nfrom llama_index import VectorStoreIndex\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)\n"})}),"\n",(0,a.jsx)(n.p,{children:"The previous code should return something like this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"Uploading data to deeplake dataset.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:00<00:00, 69.43it/s]\nDataset(path='hub://genai360/LlamaIndex_intro', tensors=['text', 'metadata', 'embedding', 'id'])\n\n  tensor      htype      shape      dtype  compression\n  -------    -------    -------    -------  ------- \n   text       text      (23, 1)      str     None   \n metadata     json      (23, 1)      str     None   \n embedding  embedding  (23, 1536)  float32   None   \n    id        text      (23, 1)      str     None\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Learn more about other ",(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html#querying",children:"Index types"})," from LlamaIndex documentation."]}),"\n",(0,a.jsx)(n.h3,{id:"query-engines",children:"Query Engines"}),"\n",(0,a.jsxs)(n.p,{children:["The next step is to leverage the generated indexes to query through the information. The Query Engine is a wrapper that\ncombines a Retriever and a Response Synthesizer into a pipeline. The pipeline uses the query string to fetch nodes and\nthen sends them to the LLM to generate a response. A query engine can be created by calling\nthe ",(0,a.jsx)(n.code,{children:"as_query_engine()"})," method on an already-created index."]}),"\n",(0,a.jsxs)(n.p,{children:["The code below uses the documents fetched from the Wikipedia page to construct a Vector Store Index using\nthe ",(0,a.jsx)(n.code,{children:"GPTVectorStoreIndex"})," class. The ",(0,a.jsx)(n.code,{children:".from_documents()"})," method simplifies building indexes on these processed documents.\nThe created index can then be utilized to generate a ",(0,a.jsx)(n.code,{children:"query_engine"})," object, allowing us to ask questions\nbased on the documents using the ",(0,a.jsx)(n.code,{children:".query()"})," method."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from llama_index import GPTVectorStoreIndex\n\nindex = GPTVectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query("What does NLP stands for?")\nprint(response.response)\n# NLP stands for Natural Language Processing.\n'})}),"\n",(0,a.jsxs)(n.p,{children:["See also ",(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/examples/query_engine/custom_query_engine.html",children:"Defining a Custom Query Engine"})]}),"\n",(0,a.jsx)(n.h3,{id:"routers",children:"Routers"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/querying/router/root.html#routers",children:"Routers"}),"\nplay a role in determining the most appropriate retriever for extracting context from the knowledge base.\nThe routing function selects the optimal query engine for each task, improving performance and accuracy."]}),"\n",(0,a.jsx)(n.p,{children:"These functions are beneficial when dealing with multiple data sources, each holding unique information. Consider an application that employs a SQL database and a Vector Store as its knowledge base. In this setup, the router can determine which data source is most applicable to the given query."}),"\n",(0,a.jsxs)(n.p,{children:["You can see a working example of incorporating the\nrouters ",(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/querying/router/root.html#routers",children:"in this tutorial"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"saving-and-loading-indexes-locally",children:"Saving and Loading Indexes Locally"}),"\n",(0,a.jsxs)(n.p,{children:["All the examples we explored involved storing indexes on cloud-based vector stores like Deep Lake.\nHowever, there are scenarios where saving the data on a disk might be necessary for rapid testing.\nThe concept of storing refers to saving the index data, which includes the nodes and their associated\nembeddings, to disk. This is done using the ",(0,a.jsx)(n.code,{children:"persist()"})," method from the ",(0,a.jsx)(n.code,{children:"storage_context"}),"\nobject related to the index."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# store index as vector embeddings on the disk\nindex.storage_context.persist()\n# This saves the data in the 'storage' by default\n# to minimize repetitive processing\n"})}),"\n",(0,a.jsx)(n.p,{children:"If the index already exists in storage, you can load it directly instead of recreating it. We simply\need to determine whether the index already exists on disk and proceed accordingly; here is how to do it:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Index Storage Checks\nimport os.path\nfrom llama_index import (\n    VectorStoreIndex,\n    StorageContext,\n    load_index_from_storage,\n)\nfrom llama_index import download_loader\n\n# Let's see if our index already exists in storage.\nif not os.path.exists(\"./storage\"):\n    # If not, we'll load the Wikipedia data and create a new index\n    WikipediaReader = download_loader(\"WikipediaReader\")\n    loader = WikipediaReader()\n    documents = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\n    index = VectorStoreIndex.from_documents(documents)\n    # Index storing\n    index.storage_context.persist()\n\nelse:\n    # If the index already exists, we'll just load it:\n    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n    index = load_index_from_storage(storage_context)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"challenges-with-naive-rag",children:"Challenges with Naive RAG"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bad Retrieval"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Low Precision: Not all chunks in retrieved set are relevant","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Hallucination + Lost in the Middle Problems"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Low Recall: Now all relevant chunks are retrieved.","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Lacks enough context for LLM to synthesize an answer"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:"Outdated information: The data is redundant or out of date."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bad Response Generation"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Hallucination: Model makes up an answer that isn\u2019t in the context."}),"\n",(0,a.jsx)(n.li,{children:"Irrelevance: Model makes up an answer that doesn\u2019t answer the question."}),"\n",(0,a.jsx)(n.li,{children:"Toxicity/Bias: Model makes up an answer that\u2019s harmful/offensive."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Questions"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Data: Can we store additional information beyond raw text chunks? (e.g., with multi-modal data store like Activeloop's Deep Lake)"}),"\n",(0,a.jsx)(n.li,{children:"Embeddings: Can we optimize our embedding representations?"}),"\n",(0,a.jsx)(n.li,{children:"Retrieval: Can we do better than top-k embedding lookup? (with systems like Activeloop's Deep Memory)"}),"\n",(0,a.jsx)(n.li,{children:"Synthesis: Can we use LLMs for more than generation?"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"evaluation",children:"Evaluation"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"How do we properly evaluate a RAG system?"}),"\n",(0,a.jsxs)(n.li,{children:["Evaluate in isolation (retrieval, synthesis)","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Evaluate quality of retrieved chunks given user query"}),"\n",(0,a.jsxs)(n.li,{children:["Steps","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Create Deep Lake Dataset"}),"\n",(0,a.jsx)(n.li,{children:"Run retriever over dataset"}),"\n",(0,a.jsx)(n.li,{children:"Measure ranking metrics (e.g., MRR, Precision@K, NDCG)"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Evaluate end-to-end (retrieval + synthesis)","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Evaluation of final generated response given input"}),"\n",(0,a.jsxs)(n.li,{children:["Steps","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Create Deep Lake Dataset"}),"\n",(0,a.jsx)(n.li,{children:"Run through full RAG pipeline"}),"\n",(0,a.jsx)(n.li,{children:"Collect evaluation metrics"}),"\n",(0,a.jsx)(n.li,{children:"Generated Response vs [Optional] Context (Label-free Evaluator: Faithfulness, Relevancy, Adheres to Guidelines, Toxicity-free, etc.)"}),"\n",(0,a.jsx)(n.li,{children:"Generated Response vs Actual Response (Label-based Evaluator: Correctness, etc.)"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"chat-with-chat-with-your-code-llamaindex-and-activeloop-deep-lake-for-github-repositories-",children:"Chat with Chat with Your Code: LlamaIndex and Activeloop Deep Lake for GitHub Repositories \ud83e\udd99\ud83c\udf0a"}),"\n",(0,a.jsx)(n.p,{children:"This guide is your quickstart toolkit for integrating LlamaIndex with Activeloop Deep Lake. You'll learn how to\neffortlessly index GitHub repositories into Deep Lake and interact with your code through natural language queries."}),"\n",(0,a.jsx)(n.h4,{id:"what-are-llamaindex-and-activeloop-deep-lake",children:"What are LlamaIndex and Activeloop Deep Lake?"}),"\n",(0,a.jsx)(n.h5,{id:"llamaindex-your-data-framework-for-llms",children:"LlamaIndex: Your Data Framework for LLMs"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://gpt-index.readthedocs.io/en/stable/",children:"LlamaIndex"})," is a bridge between your data and Language Language\nModels (LLMs). Whether your data resides in APIs, SQL databases, or PDFs, LlamaIndex ingests and structures it into\na format easily consumable by LLMs. It offers features like data connectors for various data sources, indexing capabilities\nfor quick retrieval, and natural language query engines that make your data not just accessible but also interactive."]}),"\n",(0,a.jsxs)(n.p,{children:["It also offers ",(0,a.jsx)(n.a,{href:"https://llamahub.ai/",children:"Llama Hub"}),", a platform that aggregates custom plugins for all data types."]}),"\n",(0,a.jsx)(n.h5,{id:"activeloop-deep-lake-optimized-data-lake-for-ml",children:"Activeloop Deep Lake: Optimized Data Lake for ML"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://www.activeloop.ai/",children:"Activeloop"})," Deep Lake is a data lake solution specifically designed for machine learning\nworkflows. Unlike traditional data lakes, it's optimized for quick data retrieval and manipulation, making it an ideal\nchoice for machine learning projects that require efficient data access. It supports various data types and formats,\nfrom images and videos to more complex data structures, while maintaining high performance. You can create local\nvector stores or use the managed serverless service."]}),"\n",(0,a.jsx)(n.h5,{id:"the-synergy",children:"The Synergy"}),"\n",(0,a.jsx)(n.p,{children:"When LlamaIndex and Activeloop Deep Lake are combined, they offer a robust, efficient, and interactive data management\nsolution. LlamaIndex takes care of ingesting and structuring your data, while Activeloop Deep Lake provides\noptimized storage and retrieval capabilities."}),"\n",(0,a.jsx)(n.p,{children:"In this guide, we'll see how to store and interact with your code repositories through natural language queries, offering\na unique and powerful way to manage and understand your codebase."}),"\n",(0,a.jsx)(n.h4,{id:"requirements",children:"Requirements"}),"\n",(0,a.jsx)(n.p,{children:"Before getting started, make sure you have the following:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Python - Version 3.7 or newer is required."}),"\n",(0,a.jsx)(n.li,{children:"An active account on OpenAI, along with an OpenAI API key."}),"\n",(0,a.jsx)(n.li,{children:"An Activeloop Deep Lake account, complete with a Deep Lake API key."}),"\n",(0,a.jsx)(n.li,{children:"A 'classic' personal token from GitHub."}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"getting-started",children:"Getting Started"}),"\n",(0,a.jsx)(n.p,{children:"Creating a new Python virtual environment for this project is strongly advised. It helps maintain a tidy workspace by keeping dependencies in one place."}),"\n",(0,a.jsx)(n.p,{children:"Create a Python virtual environment with:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python3 -m venv repo-aiCopy\n"})}),"\n",(0,a.jsx)(n.p,{children:"Then activate it with:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"source repo-ai/bin/activateCopy\n"})}),"\n",(0,a.jsx)(n.h4,{id:"install-the-required-packages",children:"Install the required packages"}),"\n",(0,a.jsx)(n.p,{children:"This project will need Python packages, including LLamaIndex and Deep Lake. Run the following pip command:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install llama-index deeplake openai python-dotenv\n"})}),"\n",(0,a.jsx)(n.p,{children:"Let's understand what we are installing and why."}),"\n",(0,a.jsx)(n.h5,{id:"1-llama-index",children:"1. llama-index"}),"\n",(0,a.jsx)(n.p,{children:"LlamaIndex is a data framework that works with Language Learning Models (LLMs). It helps ingest, structure, and make data accessible through natural language queries."}),"\n",(0,a.jsx)(n.p,{children:"Key Features:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Data Connectors: Ingest data from various sources like APIs, SQL databases, and PDFs."}),"\n",(0,a.jsx)(n.li,{children:"Data Indexing: Structure the ingested data for quick and efficient retrieval."}),"\n",(0,a.jsx)(n.li,{children:"Query Engines: Enable natural language queries to interact with your data."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Use-Case in the guide: In the context of this guide, LlamaIndex will be used to index GitHub repositories and make them queryable through natural language interfaces."}),"\n",(0,a.jsx)(n.h5,{id:"2-deeplake",children:"2. deeplake"}),"\n",(0,a.jsx)(n.p,{children:"Activeloop Deep Lake is a specialized data lake optimized for machine learning workflows. It allows for efficient storage and retrieval of various data types."}),"\n",(0,a.jsx)(n.p,{children:"Key Features:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Optimized Storage: Designed for quick data retrieval, ideal for machine learning applications."}),"\n",(0,a.jsx)(n.li,{children:"Data Type Support: Handles multiple data types like images, videos, and complex data structures."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Use-Case in the guide: Deep Lake is the storage layer where the GitHub repositories indexed by LlamaIndex will be stored."}),"\n",(0,a.jsx)(n.h5,{id:"3-openai",children:"3. OpenAI"}),"\n",(0,a.jsx)(n.p,{children:"The OpenAI Python package provides an interface to OpenAI's GPT models and other services. It allows you to make API calls to interact with these models."}),"\n",(0,a.jsx)(n.p,{children:"Key Features:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"API Integration: Easy integration with OpenAI's GPT models."}),"\n",(0,a.jsx)(n.li,{children:"Text Generation: Generate text based on the model's training data and capabilities."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Use-Case in the guide: LLamaIndex uses this package to interact with the OpenAI models."}),"\n",(0,a.jsx)(n.h5,{id:"4-python-dotenv",children:"4. python-dotenv"}),"\n",(0,a.jsx)(n.p,{children:"Python-dotenv is a library that allows you to specify environment variables in a .env file, making it easier to manage configurations."}),"\n",(0,a.jsx)(n.p,{children:"Key Features:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Environment Variable Management: Store configuration variables in a .env file."}),"\n",(0,a.jsx)(n.li,{children:"Easy Import: Automatically import variables from .env into your Python environment."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Use-Case in the guide: This package manages the API keys."}),"\n",(0,a.jsx)(n.h4,{id:"how-does-llamaindex-work",children:"How does LLamaIndex work?"}),"\n",(0,a.jsx)(n.p,{children:"In the context of leveraging LlamaIndex for data-driven applications, the underlying logic and workflow are pretty simple. Here's a breakdown:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Load Documents"}),": The first step involves loading your raw data into the system. You can do this manually, directly inputting the data,\nor through a data loader that automates the process. LlamaIndex offers specialized data loaders that can ingest data from various sources,\ntransforming them into Document objects, and you can find many plugins on Llama Hub. This is a crucial step as it sets the stage for the\nsubsequent data manipulation and querying functionalities."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Parse the Documents into Nodes"}),": Once the documents are loaded, they are parsed into Nodes, essentially structured data units. These\nNodes contain chunks of the original documents and carry valuable metadata and relationship information. This parsing process is vital\nas it organizes the raw data into a structured format, making it easier and more efficient for the system to handle."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Construct an Index from Nodes or Documents"}),": After the Nodes are prepared, an index is constructed to make the data searchable and\nqueryable. Depending on your needs, this index can be built directly from the original documents or the parsed Nodes. The index is often\nstored in structures like VectorStoreIndex, optimized for quick data retrieval. This step is the system's heart, turning your structured\ndata into a robust, queryable database."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Query the Index"}),": With the index in place, the final step is to query it. A query engine is initialized, allowing you to make natural\nlanguage queries against the indexed data. This is where the magic happens: you can conversationally ask the system questions, and it will\nsift through the indexed data to provide accurate and relevant answers."]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"lets-code",children:"Let's code"}),"\n",(0,a.jsx)(n.p,{children:"Now that the environment is ready and all the explanations are out of the way let's start with some code.\nIn your project's directory, create a new file named .env, paste the following, and add your API keys:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'GITHUB_TOKEN="YOUR_GH_CLASSIC_TOKEN"\nOPENAI_API_KEY="YOUR_OPENAI_KEY"\nACTIVELOOP_TOKEN="YOUR_ACTIVELOOP_TOKEN"\nDATASET_PATH="hub://YOUR_ORG/repository_vector_store"\n'})}),"\n",(0,a.jsxs)(n.p,{children:["After this, create a new file named ",(0,a.jsx)(n.code,{children:"main.py"})," and paste the following code:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\nimport textwrap\nfrom dotenv import load_dotenv\nfrom llama_index import download_loader\nfrom llama_hub.github_repo import GithubRepositoryReader, GithubClient\nfrom llama_index import VectorStoreIndex\nfrom llama_index.vector_stores import DeepLakeVectorStore\nfrom llama_index.storage.storage_context import StorageContext\nimport re\n\n# Load environment variables\nload_dotenv()\n\n# Fetch and set API keys\nopenai_api_key = os.getenv("OPENAI_API_KEY")\nactive_loop_token = os.getenv("ACTIVELOOP_TOKEN")\ndataset_path = os.getenv("DATASET_PATH")\n\n# This function takes a GitHub URL and extracts the repository owner and name using regular expressions.\ndef parse_github_url(url):\n    pattern = r"https://github\\.com/([^/]+)/([^/]+)"\n    match = re.match(pattern, url)\n    return match.groups() if match else (None, None)\n\n# Validates that both the repository owner and name are present.\ndef validate_owner_repo(owner, repo):\n    return bool(owner) and bool(repo)\n\n# Initializes the GitHub client using the token fetched from the environment variables.\ndef initialize_github_client():\n    github_token = os.getenv("GITHUB_TOKEN")\n    return GithubClient(github_token)\n\n\ndef main():\n    # Check for OpenAI API key\n    openai_api_key = os.getenv("OPENAI_API_KEY")\n    if not openai_api_key:\n        raise EnvironmentError("OpenAI API key not found in environment variables")\n\n    # Check for GitHub Token\n    github_token = os.getenv("GITHUB_TOKEN")\n    if not github_token:\n        raise EnvironmentError("GitHub token not found in environment variables")\n\n    # Check for Activeloop Token\n    active_loop_token = os.getenv("ACTIVELOOP_TOKEN")\n    if not active_loop_token:\n        raise EnvironmentError("Activeloop token not found in environment variables")\n\n    github_client = initialize_github_client()\n    download_loader("GithubRepositoryReader")\n\n    github_url = input("Please enter the GitHub repository URL: ")\n    owner, repo = parse_github_url(github_url)\n\n    while True:\n        owner, repo = parse_github_url(github_url)\n        if validate_owner_repo(owner, repo):\n            loader = GithubRepositoryReader(\n                github_client,\n                owner=owner,\n                repo=repo,\n                filter_file_extensions=(\n                    [".py", ".js", ".ts", ".md"],\n                    GithubRepositoryReader.FilterType.INCLUDE,\n                ),\n                verbose=False,\n                concurrent_requests=5,\n            )\n            print(f"Loading {repo} repository by {owner}")\n            docs = loader.load_data(branch="main")\n            print("Documents uploaded:")\n            for doc in docs:\n                print(doc.metadata)\n            break  # Exit the loop once the valid URL is processed\n        else:\n            print("Invalid GitHub URL. Please try again.")\n            github_url = input("Please enter the GitHub repository URL: ")\n\n    print("Uploading to vector store...")\n\n    # ====== Create vector store and upload data ======\n\n    vector_store = DeepLakeVectorStore(\n        dataset_path=dataset_path,\n        overwrite=True,\n        runtime={"tensor_db": True},\n    )\n\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n    query_engine = index.as_query_engine()\n\n    # Include a simple question to test.\n    intro_question = "What is the repository about?"\n    print(f"Test question: {intro_question}")\n    print("=" * 50)\n    answer = query_engine.query(intro_question)\n\n    print(f"Answer: {textwrap.fill(str(answer), 100)} \\n")\n    while True:\n        user_question = input("Please enter your question (or type \'exit\' to quit): ")\n        if user_question.lower() == "exit":\n            print("Exiting, thanks for chatting!")\n            break\n\n        print(f"Your question: {user_question}")\n        print("=" * 50)\n\n        answer = query_engine.query(user_question)\n        print(f"Answer: {textwrap.fill(str(answer), 100)} \\n")\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,a.jsx)(n.p,{children:"Run the command to start:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python3 main.py\n"})}),"\n",(0,a.jsx)(n.h4,{id:"diving-deeper-llamaindexs-low-level-api-for-customizations",children:"Diving Deeper: LlamaIndex's Low-Level API for customizations"}),"\n",(0,a.jsx)(n.p,{children:"While the high-level API of LlamaIndex offers a seamless experience for most use cases, there might be situations where\nyou need more granular control over the query logic. This is where the low-level API shines, offering customizations to\nfine-tune your interactions with the indexed data."}),"\n",(0,a.jsx)(n.h5,{id:"building-the-index",children:"Building the Index"}),"\n",(0,a.jsx)(n.p,{children:"To start, you'll need to build an index from your documents, the same as we have done so far."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create an index of the documents\ntry:\n    vector_store = DeepLakeVectorStore(\n        dataset_path=dataset_path,\n        overwrite=True,\n        runtime={"tensor_db": True},\n)\nexcept Exception as e:\n    print(f"An unexpected error occurred while creating or fetching the vector store: {str(e)}")\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n'})}),"\n",(0,a.jsx)(n.h5,{id:"configuring-the-retriever",children:"Configuring the Retriever"}),"\n",(0,a.jsx)(n.p,{children:"The retriever is responsible for fetching relevant nodes from the index. LlamaIndex supports various retrieval modes, allowing you to choose the one that best fits your needs:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from llama_index.retrievers import VectorIndexRetriever\nretriever = VectorIndexRetriever(index=index, similarity_top_k=4)\n"})}),"\n",(0,a.jsx)(n.h5,{id:"customize-the-query-engine",children:"Customize the query engine"}),"\n",(0,a.jsx)(n.p,{children:"In LLamaIndex, passing extra parameters and customizations to the query engine is possible."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from llama_index import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer()\nquery_engine = RetrieverQueryEngine.from_args(\n        retriever=retriever,\n        response_mode='default',\n        response_synthesizer=response_synthesizer,\n        node_postprocessors=[\n            SimilarityPostprocessor(similarity_cutoff=0.7)]\n    )\n"})}),"\n",(0,a.jsx)(n.h5,{id:"exploring-different-response-modes",children:"Exploring Different Response Modes"}),"\n",(0,a.jsx)(n.p,{children:"The RetrieverQueryEngine offers various response modes to tailor the synthesis of responses based on the retrieved nodes."}),"\n",(0,a.jsx)(n.p,{children:"Here's a breakdown of some of the available modes:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"response_mode='default'"}),": In the default mode, the system processes each retrieved node sequentially, making a separate LLM call for each one. This mode is suitable for generating detailed answers."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"response_mode='compact'"}),": The compact mode fits as many node text chunks as possible within the maximum prompt size during each LLM call. If there are too many chunks, it refines the answer by processing multiple prompts."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"response_mode='tree_summarize'"}),": This mode constructs a tree from a set of node objects, and the query then returns the root node as the response. It's beneficial for summarization tasks."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"response_mode='no_text'"}),": In the no-text mode, the retriever fetches the nodes that would have been sent to the LLM but doesn't send them. This mode allows for inspecting the retrieved nodes without generating a synthesized response."]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"how-does-llamaindex-compare-to-langchain",children:"How does LLamaIndex compare to LangChain?"}),"\n",(0,a.jsx)(n.p,{children:"Now that you understand how LLamaIndex works comparing it with the big-name LangChain is an excellent time."}),"\n",(0,a.jsx)(n.p,{children:"LlamaIndex and Langchain are both frameworks/libraries designed to enhance the capabilities of large language models by allowing them to interact with external data. While they serve similar overarching goals, their approaches and features differ."}),"\n",(0,a.jsx)(n.h5,{id:"key-features",children:"Key Features"}),"\n",(0,a.jsx)(n.p,{children:"Llama Index:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Connectors"}),": Offers a variety of connectors to ingest data, making it versatile for different data sources. LLama Hub is an excellent source of community-made tools."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Structuring"}),": Allows users to structure data using various index types, such as list index, tree index, and even the ability to compose indices."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Query Abstraction"}),": Provides layers of abstraction, enabling both simple and complex data querying."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Langchain:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Modular Design"}),": Comes with modules for tools, agents, and chains, each serving a distinct purpose."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Chains"}),": A series of steps or actions the language model takes, ideal for tasks requiring multiple interactions."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Agents"}),": Autonomous entities that can decide the next steps in a process, adding a layer of decision-making to the model's interactions."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Tools"}),": Utility agents are used to perform specific tasks, such as searching or querying an index."]}),"\n"]}),"\n",(0,a.jsx)(n.h5,{id:"use-cases",children:"Use Cases"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["The ",(0,a.jsx)(n.strong,{children:"Llama Index"})," is best suited for applications that require complex data structures and querying. Its strength lies in handling and querying structured data."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"LangChain"}),", on the other hand, excels in scenarios that require multiple interactions with a language model, especially when those interactions involve decision-making or a series of steps."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Of course, you can combine the two; the potential of combining LlamaIndex and Langchain is promising. By integrating LlamaIndex's\nstructured data querying capabilities with the multi-step interaction and decision-making features of Langchain, developers can create\nrobust and versatile applications. As mentioned in the conclusion, This combination can offer the best of both worlds."}),"\n",(0,a.jsx)(n.p,{children:"So which one to use? Use the tool that makes it easier to take care of your use cases; this is usually the leading factor in deciding which one I want to use."}),"\n",(0,a.jsx)(n.h3,{id:"more-info-on-llamaindex",children:"More info on LlamaIndex"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/",children:"LlamaIndex Documentation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://cookbook.openai.com/examples/third_party/financial_document_analysis_with_llamaindex",children:"Financial Document Analysis with LlamaIndex"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://www.datacamp.com/tutorial/llama-index-adding-personal-data-to-llms",children:"LlamaIndex: Adding Personal Data to LLMs"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://www.datacamp.com/blog/the-pros-and-cons-of-using-llm-in-the-cloud-versus-running-llm-locally",children:"The Pros and Cons of Using LLMs in the Cloud Versus Running LLMs Locally"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://howaibuildthis.substack.com/p/llamaindex-how-to-use-index-correctly",children:"LlamaIndex: How to use Index correctly"})}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"module-2-introduction---advanced-retrieval-augmented-generation",children:"Module 2 Introduction - Advanced Retrieval Augmented Generation"}),"\n",(0,a.jsx)(n.h3,{id:"fine-tuning-vs-rag-introduction-to-activeloops-deep-memory",children:"Fine-tuning vs RAG; Introduction to Activeloop's Deep Memory"}),"\n",(0,a.jsx)(n.h4,{id:"overview-of-rag-enhancement-techniques",children:"Overview of RAG Enhancement Techniques"}),"\n",(0,a.jsx)(n.h5,{id:"prompt-engineering",children:"Prompt engineering"}),"\n",(0,a.jsxs)(n.p,{children:["Prompt engineering is often the first step in enhancing the performance of an LLM for specific tasks. This approach alone\ncan be sufficient, especially for simpler or well-defined tasks. Techniques like ",(0,a.jsx)(n.a,{href:"https://www.promptingguide.ai/techniques/fewshot",children:"few-shot prompting"})," can notably improve\ntask performance. This method involves providing small task-specific examples to guide the LLM. ",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2201.11903",children:"Chain of Thought (CoT)"})," prompting\ncan also improve reasoning capabilities and encourage the model to generate more detailed responses."]}),"\n",(0,a.jsx)(n.p,{children:"Combining Few-shot with RAG\u2014using a tailored dataset of examples to retrieve the most relevant information for each query\u2014can be more effective."}),"\n",(0,a.jsx)(n.h5,{id:"fine-tuning",children:"Fine-tuning"}),"\n",(0,a.jsx)(n.p,{children:"Fine-tuning enhances LLM\u2019s capabilities in the following areas:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Modifying the structure or tone of responses."}),"\n",(0,a.jsx)(n.li,{children:"Teaching the model to follow complex instructions."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"For example, fine-tuning enables models to perform tasks like extracting JSON-formatted data from text, translating natural language into SQL queries, or adopting a specific writing style."}),"\n",(0,a.jsx)(n.p,{children:"Fine-tuning demands a large, high-quality, task-specific dataset for effective training. You can start with a small dataset and training to see if the method works for your task."}),"\n",(0,a.jsx)(n.p,{children:"Fine-tuning is less effective in adapting to new, rapidly changing data or unfamiliar queries beyond the training dataset. It's also not the best choice for incorporating new information into the model. Alternative methods, such as Retrieval-Augmented Generation, are more suitable."}),"\n",(0,a.jsx)(n.h5,{id:"retrieval-augmented-generation",children:"Retrieval-Augmented Generation"}),"\n",(0,a.jsxs)(n.p,{children:["RAG specializes in incorporating ",(0,a.jsx)(n.strong,{children:"external knowledge"}),", enabling the model to access current and varied information."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Real-Time Updates"}),": It is more adept at dealing with evolving datasets and can provide more up-to-date responses."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Complexity in Integration"}),": Setting up a RAG system is more complex than basic prompting, requiring extra components like a Vector Database and retrieval algorithms."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Data Management"}),": Managing and updating the external data sources is crucial for maintaining the accuracy and relevance of its outputs."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Retrieval accuracy"}),": Ensuring precise embedding retrieval is crucial in RAG systems to guarantee reliable and\ncomprehensive responses to user queries. For that, we will demonstrate how Activeloop\u2019s Deep Memory method can greatly\nincrease the recall of embedding retrieval."]}),"\n",(0,a.jsx)(n.h5,{id:"rag--fine-tuning",children:"RAG + Fine-tuning"}),"\n",(0,a.jsx)(n.p,{children:"Fine-tuning and RAGs are not mutually exclusive techniques. Fine-tuning brings the advantage of customizing models for a specific style or format,\nwhich can be useful when using LLMs for specific domains such as medical, financial, or legal, requiring a highly specialized tone of writing."}),"\n",(0,a.jsx)(n.p,{children:"When combined with RAG, the model becomes adept in its specialized area and gains access to a vast range of external information.\nThe resulting model provides accurate responses in the niche area."}),"\n",(0,a.jsx)(n.p,{children:"Implementing these two methods can demand considerable resources for setup and ongoing upkeep.\nIt involves multiple training runs of fine-tuning with the data handling requirements inherent to RAG."}),"\n",(0,a.jsx)(n.h4,{id:"enhanced-rag-with-deep-memory",children:"Enhanced RAG with Deep Memory"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://docs.activeloop.ai/performance-features/deep-memory",children:"Deep Memory"})," is a method developed by Activeloop to boost the\naccuracy of embedding retrieval for RAG systems integrated into the Deep Lake vector store database."]}),"\n",(0,a.jsx)(n.p,{children:"Central to its functionality is an embedding transformation process. Deep Memory trains a model that transforms embeddings\ninto a space optimized for your use case. This reconfiguration significantly improves vector search accuracy."}),"\n",(0,a.jsx)(n.p,{children:"Deep Memory is effective where query reformulation, query transformation, or document re-ranking might cause latency\nand increased token usage. It boosts retrieval capabilities without negatively impacting the system's performance."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Recall@1"}),": This measures whether the top result (i.e., the first result) returned by the retrieval system is relevant to the query."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Recall@10"}),": This metric assesses whether the relevant document is within the top 10 results returned by the retrieval system."]}),"\n"]}),"\n",(0,a.jsx)(n.h5,{id:"comparison-to-lexical-search",children:"Comparison to Lexical search"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Okapi_BM25",children:"BM25"}),' is considered a state-of-the-art approach for "lexical search," based on the\nexplicit presence of words (or lexicons) from the query in the documents. It\'s particularly effective for applications where the relevance\nof documents depends heavily on the presence of specific terms, such as in traditional search engines. However, BM25 does\nnot account for the semantic relationships between words, where more advanced techniques like vector search with neural embeddings and\nsemantic search come into play.']}),"\n",(0,a.jsx)(n.h4,{id:"overview-of-deep-memory",children:"Overview of Deep Memory"}),"\n",(0,a.jsx)(n.p,{children:"In the figure above, we see the Inference and Training workflow:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Embeddings"}),": Vector representation of a text sentence or set of words. We can create them using embedding models\nsuch as OpenAI\u2019s ",(0,a.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/embeddings/embedding-models",children:"text-embedding-ada-002"})," or open-source models."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Deep Memory Training"}),": A dataset of ",(0,a.jsx)(n.strong,{children:"query and context pairs"})," trains the Deep Memory model. This training process\nruns in Deep Lake Cloud, which provides the computational resources and infrastructure for handling the training."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Deep Memory Inference"}),": The model enters the inference phase after training, which transforms query embeddings.\nWe can use the ",(0,a.jsx)(n.a,{href:"https://docs.activeloop.ai/performance-features/querying-datasets",children:"Tensor Query Language (TQL)"})," when\nrunning an inference/querying in the Vector Store."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Transformed Embeddings"}),": The result of the inference process is a set of transformed embeddings optimized for a\nspecific use case. This optimization means that the embeddings are now in a more conducive space for returning accurate results."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Vector Search"}),": These optimized embeddings are used in a vector search, utilizing standard similarity search techniques\n(e.g., cosine similarity). The vector search is retrieving information, leveraging the refined embeddings to find and\nretrieve the most relevant data points for a given query."]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"step-by-step---training-a-deep-memory-model",children:"Step by Step - Training a Deep Memory Model"}),"\n",(0,a.jsxs)(n.p,{children:["Je suis rendu ici... ",(0,a.jsx)(n.a,{href:"https://learn.activeloop.ai/courses/take/rag/multimedia/51320353-brief-overview-of-available-techniques-fine-tuning-rag-activeloop-s-deep-memory",children:"https://learn.activeloop.ai/courses/take/rag/multimedia/51320353-brief-overview-of-available-techniques-fine-tuning-rag-activeloop-s-deep-memory"})]}),"\n",(0,a.jsx)(n.p,{children:"Moving forward in our lesson, let's implement Deep Memory within our workflow to see firsthand how it impacts retrieval recall."}),"\n",(0,a.jsx)(n.p,{children:"You can follow along with this Colab notebook."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'!pip3 install deeplake langchain openai tiktoken llama-index\n\nimport os, getpass\nos.environ[\'ACTIVELOOP_TOKEN\'] = getpass.getpass()\nos.environ[\'OPENAI_API_KEY\'] = getpass.getpass()\n\n!mkdir -p \'data/paul_graham/\'\n!curl \'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\' -o \'data/paul_graham/paul_graham_essay.txt\'\n\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader("./data/paul_graham/").load_data()\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)\n\n# By default, the node/chunks ids are set to random uuids. To ensure same id\'s per run, we manually set them.\nfor idx, node in enumerate(nodes):\n    node.id_ = f"node_{idx}"\n\nprint(f"Number of Documents: {len(documents)}")\nprint(f"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}")\n\nfrom llama_index import VectorStoreIndex, ServiceContext, StorageContext\nfrom llama_index.vector_stores import DeepLakeVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms import OpenAI\n\n# Create a DeepLakeVectorStore locally to store the vectors\ndataset_path = "./data/paul_graham/deep_lake_db"\nvector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True, exec_option="compute_engine")\n\n# LLM that will answer questions with the retrieved context\nllm = OpenAI(model="gpt-3.5-turbo-1106")\nembed_model = OpenAIEmbedding()\n\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nvector_index = VectorStoreIndex(nodes, service_context=service_context, storage_context=storage_context, show_progress=True)\n'})}),"\n",(0,a.jsx)(n.p,{children:"You should see something like this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"Uploading data to deeplake dataset.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:00<00:00, 274.94it/s]Dataset(path='./data/paul_graham/deep_lake_db', tensors=['text', 'metadata', 'embedding', 'id'])\n\n  tensor      htype      shape      dtype  compression\n  -------    -------    -------    -------  ------- \n   text       text      (58, 1)      str     None   \n metadata     json      (58, 1)      str     None   \n embedding  embedding  (58, 1536)  float32   None   \n    id        text      (58, 1)      str     None\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import deeplake\nlocal = "./data/paul_graham/deep_lake_db"\nhub_path = "hub://genai360/LlamaIndex_paulgraham_essay"\nhub_managed_path = "hub://genai360/LlamaIndex_paulgraham_essay_managed"\n\n# First upload our local vector store\ndeeplake.deepcopy(local, hub_path, overwrite=True)\n# Create a managed vector store under a different name\ndeeplake.deepcopy(hub_path, hub_managed_path, overwrite=True, runtime={"tensor_db": True})\n\ndb = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, exec_option="compute_engine", read_only=True,)\n\n# Fetch dataset docs and ids \ndocs = db.vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)[\'value\']\nids = db.vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)[\'value\']\nprint(len(docs))\n\nfrom openai import OpenAI\nclient = OpenAI()\n\ndef generate_question(text):\n    try:\n        response = client.chat.completions.create(\n            model="gpt-3.5-turbo-1106",\n            messages=[\n                {"role": "system", "content": "You are a world class expert for generating questions based on provided context. \\\n                        You make sure the question can be answered by the text."},\n                {\n                    "role": "user",\n                    "content": text,\n                },\n            ],\n        )\n        return response.choices[0].message.content\n    except:\n        question_string = "No question generated"\n        return question_string\n\nimport random\nfrom tqdm import tqdm\n\ndef generate_queries(docs: list[str], ids: list[str], n: int):\n\n    questions = []\n    relevances = []\n    pbar = tqdm(total=n)\n    while len(questions) < n:\n        # 1. randomly draw a piece of text and relevance id\n        r = random.randint(0, len(docs)-1)\n        text, label = docs[r], ids[r]\n\n        # 2. generate queries and assign and relevance id\n        generated_qs = [generate_question(text)]\n        if generated_qs == ["No question generated"]:\n            print("No question generated")\n            continue\n\n        questions.extend(generated_qs)\n        relevances.extend([[(label, 1)] for _ in generated_qs])\n        pbar.update(len(generated_qs))\n\n    return questions[:n], relevances[:n]\n\nquestions, relevances = generate_queries(docs, ids, n=40)\nprint(len(questions)) #40\nprint(questions[0])\n\n# Launch Deep Memory Training\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nopenai_embeddings = OpenAIEmbeddings()\n\njob_id = db.vectorstore.deep_memory.train(\n    queries=questions,\n    relevance=relevances,\n    embedding_function=openai_embeddings.embed_documents,\n)\n'})})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>o,a:()=>s});var a=t(67294);const i={},r=a.createContext(i);function s(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);