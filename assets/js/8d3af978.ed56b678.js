"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[3178],{41517:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var t=a(85893),i=a(11151);const s={},r="\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain",o={id:"references/llms/langchain/langchain",title:"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain",description:"HuggingFaceHub",source:"@site/docs/references/llms/langchain/langchain.md",sourceDirName:"references/llms/langchain",slug:"/references/llms/langchain/",permalink:"/docs/references/llms/langchain/",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/references/llms/langchain/langchain.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"GPT",permalink:"/docs/references/llms/gpt"},next:{title:"LlamaIndex",permalink:"/docs/references/llms/llama-index"}},l={},c=[{value:"HuggingFaceHub",id:"huggingfacehub",level:2},{value:"Langchain with Llama2 GGML",id:"langchain-with-llama2-ggml",level:3},{value:"Agents",id:"agents",level:2},{value:"Querying a PDF File with ChatGPT",id:"querying-a-pdf-file-with-chatgpt",level:2},{value:"Another example",id:"another-example",level:3},{value:"Add Conversational Memory To The ChatGPT API With LangChain",id:"add-conversational-memory-to-the-chatgpt-api-with-langchain",level:2},{value:"Building a Research Assistant from Scratch",id:"building-a-research-assistant-from-scratch",level:2},{value:"Install the required libraries",id:"install-the-required-libraries",level:3},{value:"Set your API key and LangChain project",id:"set-your-api-key-and-langchain-project",level:3},{value:"Running LangChain and a MistralAI 7B Model in Google Colab",id:"running-langchain-and-a-mistralai-7b-model-in-google-colab",level:2},{value:"Microsoft Word Document QA",id:"microsoft-word-document-qa",level:2},{value:"LangChain with OLlama",id:"langchain-with-ollama",level:2},{value:"See also",id:"see-also",level:2}];function d(e){const n={a:"a",admonition:"admonition",annotation:"annotation",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msqrt:"msqrt",msup:"msup",p:"p",path:"path",pre:"pre",semantics:"semantics",span:"span",svg:"svg",ul:"ul",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"\ufe0f-langchain",children:"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain"}),"\n",(0,t.jsx)(n.h2,{id:"huggingfacehub",children:"HuggingFaceHub"}),"\n",(0,t.jsxs)(n.p,{children:["We need a ",(0,t.jsx)(n.a,{href:"https://huggingface.co/settings/tokens",children:"Hugging Face account and API key"})," to use these endpoints."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"!pip install langchain\n!pip install huggingface_hub\n\nimport os\nfrom langchain import PromptTemplate\nfrom langchain import HuggingFaceHub, LLMChain\n\nos.environ['HUGGINGFACEHUB_API_TOKEN'] = 'HF_API_KEY'\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: \"\"\"\nprompt = PromptTemplate(\n    template=template,\n    input_variables=['question']\n)\n\n# user question\nquestion = \"Which NFL team won the Super Bowl in the 2010 season?\"\n\n# initialize Hub LLM\nhub_llm = HuggingFaceHub(\n    repo_id='google/flan-t5-xl',\n    model_kwargs={'temperature':1e-10}\n)\n\n# create prompt template > LLM chain\nllm_chain = LLMChain(\n    prompt=prompt,\n    llm=hub_llm\n)\n\n# ask the user question about NFL 2010\nprint(llm_chain.run(question))\n"})}),"\n",(0,t.jsx)(n.p,{children:"To ask multiple questions, we can try two approaches:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Iterate through all questions using the ",(0,t.jsx)(n.code,{children:"generate"})," method, answering them one at a time."]}),"\n",(0,t.jsx)(n.li,{children:"Place all questions into a single prompt for the LLM; this will only work for more advanced LLMs."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"qs = [\n    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n    {'question': \"Who was the 12th person on the moon?\"},\n    {'question': \"How many eyes does a blade of grass have?\"}\n]\nres = llm_chain.generate(qs)\nres\n"})}),"\n",(0,t.jsx)(n.h3,{id:"langchain-with-llama2-ggml",children:"Langchain with Llama2 GGML"}),"\n",(0,t.jsx)(n.p,{children:"This code can run on Google Colab."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'!pip install -q ctransformers langchain\n\nfrom langchain.llms import CTransformers\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\nllm = CTransformers(\n        model="TheBloke/Llama-2-7B-Chat-GGML", \n        model_file=\'llama-2-7b-chat.ggmlv3.q2_K.bin\', \n        callbacks=[StreamingStdOutCallbackHandler()])\n\ntemplate = """\n[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Your answers are always brief.\n<</SYS>>\n{text}[/INST]\n"""\n\nprompt = PromptTemplate(template=template, input_variables=["text"])\n\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nresponse = llm_chain.run("Why some days are terrible?")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"agents",children:"Agents"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pip install langchain\npip install wikipedia\npip install openai\npip install openai\n\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.llms import OpenAI\n\nimport os\n\nos.environ[\'OPENAI_API_KEY\'] = "sk-..."  # insert your API_TOKEN here\n\nllm = OpenAI(temperature=0,model_name=\'gpt-4-0314\')\n\ntools = load_tools(["wikipedia"], llm=llm)\n\nagent = initialize_agent(tools, llm, agent="zero-shot-react-description", verbose=True)\n\nagent.run("""What is the square root of the year the founder of SpaceX and Tesla born \nand what is the name of the first company he founded?""")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"querying-a-pdf-file-with-chatgpt",children:"Querying a PDF File with ChatGPT"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"2023-08-30"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Here is a guide for querying a PDF file using ChatGPT."}),"\n",(0,t.jsxs)(n.p,{children:["To get started, open a new ",(0,t.jsx)(n.a,{href:"https://colab.google/",children:"Colab"})," notebook and follow these steps:"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://colab.research.google.com/drive/1KvZyC81-lhzNJiBDbRAHZLJ5K9KXZfWn?usp=drive_link",children:(0,t.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})})}),"\n",(0,t.jsx)(n.p,{children:"Install the required libraries."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"!pip3 install langchain pypdf chromadb openai tiktoken --quiet\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Enter your ",(0,t.jsx)(n.a,{href:"https://platform.openai.com/account/api-keys",children:"OpenAI API key"})," as shown below:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import os\nimport getpass\n\nos.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n"})}),"\n",(0,t.jsx)(n.p,{children:"Import the necessary libraries:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from langchain.document_loaders import PyPDFLoader \nfrom langchain.embeddings import OpenAIEmbeddings \nfrom langchain.vectorstores import Chroma \nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chat_models import ChatOpenAI\n"})}),"\n",(0,t.jsx)(n.p,{children:"Download the PDF document you want to query. In this example, we will use a 472-page document, the Budgetary Plan for the 2023-2024 Budget:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"!curl -o paper.pdf http://www.finances.gouv.qc.ca/Budget_et_mise_a_jour/budget/documents/Budget2324_PlanBudgetaire.pdf\n"})}),"\n",(0,t.jsx)(n.p,{children:"Specify the path to the PDF file and load the pages of the document:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pdf_path = "./paper.pdf"\nloader = PyPDFLoader(pdf_path)\npages = loader.load_and_split()\n'})}),"\n",(0,t.jsx)(n.p,{children:"Create embeddings using OpenAIEmbeddings:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"embeddings = OpenAIEmbeddings()\n"})}),"\n",(0,t.jsx)(n.p,{children:"Generate a vector database from the document pages using Chroma:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'vectordb = Chroma.from_documents(pages, embedding=embeddings, persist_directory=".")\nvectordb.persist()\n'})}),"\n",(0,t.jsx)(n.p,{children:"Set up a conversational memory using ConversationBufferMemory:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)\n'})}),"\n",(0,t.jsx)(n.p,{children:"Create a conversational retrieval chain using ConversationalRetrievalChain:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'llm = ChatOpenAI(model_name="gpt-4", temperature=0.8) \npdf_qa = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), memory=memory)\n'})}),"\n",(0,t.jsx)(n.p,{children:"Define a function to query the PDF document:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def query(query: str):\n    memory.clear()\n    result = pdf_qa({"question": query})\n    print(result["answer"])\n'})}),"\n",(0,t.jsx)(n.p,{children:"Ask specific questions and get answers from the document:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'query("What are the highlights of the budget plan of the government of Quebec?")\nquery("What is the financial situation of the government of Quebec evolving?")\nquery("How is the net debt evolving?")\n\nquery("Quels sont les faits saillants du plan budg\xe9taire du gouvernement du Qu\xe9bec?")\nquery("Quelle est l\'\xe9volution de la situation financi\xe8re du gouvernement du Qu\xe9bec?")\nquery("Comment \xe9volue la dette nette?")\n'})}),"\n",(0,t.jsx)(n.p,{children:'The code above will generate responses like these.\nPlease note that large language models do not always produce the same results.\nWhen running the code in your notebook, you may get slightly different responses."'}),"\n",(0,t.jsx)(n.p,{children:"IN FRENCH:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-txt",children:"Le plan budg\xe9taire du Qu\xe9bec pour 2023-2024 comporte plusieurs points saillants. \nTout d'abord, le gouvernement pr\xe9voit un retour \xe0 l'\xe9quilibre budg\xe9taire en \n2027-2028. Depuis mars 2022, la situation budg\xe9taire du Qu\xe9bec s'est am\xe9lior\xe9e \ngr\xe2ce \xe0 une forte croissance \xe9conomique, malgr\xe9 l'inflation et l'augmentation \nrapide des taux d'int\xe9r\xeat. \n\nLe cadre financier pr\xe9sente des r\xe9visions favorables de 5,5 milliards de dollars\nen 2022-2023, de 6,5 milliards de dollars en 2023-2024, et de 7,7 milliards de \ndollars en 2024-2025. Ces r\xe9visions sont principalement attribuables \xe0 une \nhausse des revenus du gouvernement, en particulier des revenus fiscaux. \n\nToutefois, ces augmentations sont en partie compens\xe9es par une augmentation des\nd\xe9penses de 1,8 milliard de dollars en 2022-2023, de 2,5 milliards de dollars en\n2023-2024, et de 2,4 milliards de dollars en 2024-2025. \n\nCes am\xe9liorations permettent de nouvelles initiatives de 6,6 milliards de \ndollars en 2022-2023 et en 2023-2024, puis de 7,4 milliards de dollars en \n2024-2025. Ces sommes seront utilis\xe9es pour accro\xeetre la richesse du Qu\xe9bec, \nd\xe9velopper le potentiel des jeunes et rendre le r\xe9seau de la sant\xe9 plus \nperformant et plus humain. \n\nEnfin, le gouvernement pr\xe9voit d'\xe9liminer graduellement le d\xe9ficit budg\xe9taire de\n5,0 milliards de dollars pr\xe9vu en 2022-2023 \xe0 raison de 1 milliard de dollars \npar ann\xe9e. \xc0 partir de 2025-2026, le solde budg\xe9taire devrait afficher des \nsurplus.\n\nL'\xe9volution r\xe9cente de la situation financi\xe8re du gouvernement du Qu\xe9bec est \nmarqu\xe9e par des r\xe9visions favorables des revenus. Ces r\xe9visions rendent possible\nle retour \xe0 l'\xe9quilibre budg\xe9taire en 2027-2028, tout en soutenant la croissance\n\xe9conomique et la cr\xe9ation de richesse. Cependant, un ralentissement de \nl'activit\xe9 \xe9conomique au Qu\xe9bec est attendu en 2023. \n\nDes am\xe9liorations de 6,6 milliards de dollars en moyenne de 2022-2023 \xe0 \n2024-2025 ont \xe9t\xe9 pr\xe9sent\xe9esdans le budget 2022-2023. Pour cette p\xe9riode, le \ngouvernement pr\xe9voit de nouvelles initiatives de 6,6 milliards de dollars en \n2022-2023 et en 2023-2024, puis de 7,4 milliards de dollars en 2024-2025. Ces \nfonds seront utilis\xe9s pour accro\xeetre la richesse du Qu\xe9bec, d\xe9velopper le \npotentiel des jeunes et rendre le r\xe9seau de la sant\xe9 plus performant et plus \nhumain. Ces montants incluent le d\xe9ploiement du Bouclier anti-inflation annonc\xe9 \n\xe0 l'automne 2022.\n\nLes versements au Fonds des g\xe9n\xe9rations conna\xeetront une croissance plus mod\xe9r\xe9e, \nprincipalement en raison des modifications pr\xe9vues concernant les sources de \nrevenus consacr\xe9s \xe0 ce fonds \xe0 partir de 2023-2024.\n\nLa dette nette du Qu\xe9bec \xe9volue de mani\xe8re d\xe9croissante. Au 31 mars 2023, la \ndette nette du Qu\xe9bec s'\xe9tablira \xe0 206,8 milliards de dollars, soit \xe0 37,4 % du \nPIB. Une diminution \xe0 35,8 % du PIB est pr\xe9vue d'ici 2027-2028. Le retour \xe0 \nl'\xe9quilibre budg\xe9taire, les versements au Fonds des g\xe9n\xe9rations et la \nprogression de l'\xe9conomie contribuent \xe0 cette r\xe9duction de la dette nette.\n"})}),"\n",(0,t.jsx)(n.p,{children:"I hope this helps!"}),"\n",(0,t.jsx)(n.h3,{id:"another-example",children:"Another example"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"2023-12-23"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Here is another example from ",(0,t.jsx)(n.a,{href:"https://github.com/UmarJawad/RAG/blob/main/RAG-DataScienceDojo.ipynb",children:"UmarJawad"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Dependencies\n!pip install -qU langchain openai datasets transformers sentence-transformers pypdf\n\n# Building the Knowledge Base from pdfs\n# 1. Split and Chunk Documents\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 300, # characters\n    chunk_overlap  = 50, # overlap between chunks in characters\n    length_function = len, # function to determine the length of a chunk\n)\n\n# Make chunks\nfilepath = "./TRIMFR_SIFIN_2T_23-24.pdf"\nloader = PyPDFLoader(filepath)\nchunks = loader.load_and_split(text_splitter=text_splitter)\n\n# 2. Display chunks\nfor chunk in chunks[12:14]:\n    print("Page_content:\\n",chunk.page_content)\n    print("Page_metadata:\\n",chunk.metadata)\n    print("----------------------------------------------------------------")\n\n# Vector DB\n# 1. Load dependencies\xb6\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n\n# 2. Use an open source embedding model to embed the chunks\n# create the open-source embedding function\nembedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")\n\n# Explain Embeddings\nembedding = embedding_function.embed_documents("This is a test")\n\nprint(embedding[0])\nprint("Dimensions of Embedding:",len(embedding[0]))\n\n# 3. Upload to Vector DB\n# load it into Chroma\ndb = Chroma.from_documents(chunks, embedding_function) #, persist_directory="./chroma_db")\nprint("Chunks in DB:",db._collection.count())\n\n# 4. Retrieval from Vector DB\xb6\n# Using a retriever object\nquery = "Quelle est la dette du Qu\xe9bec?"\nretriever = db.as_retriever()\nretriever.get_relevant_documents(query)\n\n# Setting up RAG\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQAWithSourcesChain\n\n# 1. Connect to LLM (OpenAI)\nOPENAI_API_KEY = "sk-xyz..."\n\n# Setup LLM connection\nllm = ChatOpenAI(\n    openai_api_key=OPENAI_API_KEY,\n    model_name=\'gpt-3.5-turbo\',\n    # model_name = "gpt-4",\n    temperature=0.0,\n    max_tokens=100,\n    # streaming=True,\n)\n\n# 2. Connect Vector DB to LLM\nqa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n    llm=llm,\n    chain_type="stuff",\n    retriever=retriever,\n)\n\n# 3. RAG QA\nquery = "Quelle est la dette du Qu\xe9bec?"\nqa_with_sources(query)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"add-conversational-memory-to-the-chatgpt-api-with-langchain",children:"Add Conversational Memory To The ChatGPT API With LangChain"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"2023-09-30"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'!pip install langchain openai tiktoken --quiet\n\nimport openai\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationSummaryBufferMemory\n\n# openai.api_key = os.getenv("OPENAI_API_KEY")\n# os.environ[\'OPENAI_API_KEY\'] = getpass.getpass(\'OpenAI API Key:\')\nopenai.api_key_path = "api_key.txt"\n\nchat = ChatOpenAI(model_name="gpt-4", temperature=0.2)\nconversation = ConversationChain(\n    llm=chat,\n    memory=ConversationSummaryBufferMemory(\n        llm=ChatOpenAI(), max_token_limit=2048\n    ),\n    verbose=False,\n)\n\nconversation.predict(input="Hello world!")\nconversation.predict(input="What is the capital of France?")\nconversation.predict(input="What was my previous question?")\n'})}),"\n",(0,t.jsx)(n.p,{children:"The previous code should output something like this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-txt",children:'"Hello! It\'s great to connect with you. How can I assist you today?"\n\n"The capital of France is Paris. It\'s known for its beautiful architecture, rich history, \nand iconic landmarks like the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral."\n\n\'Your previous question was "What is the capital of France?"\'\n'})}),"\n",(0,t.jsx)(n.p,{children:"Here is another question."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-txt",children:'from IPython.display import HTML\n\nquestion = """\nUn triangle ayant deux c\xf4t\xe9 $a$ et $b$. Quelle est la longueur de l\'hypot\xe9nuse \n$c$ en LaTeX?\n"""\nhtml_string = conversation.predict(input=question)\n\nprint(html_string)\nprint("-----")\ndisplay(HTML(html_string))\n'})}),"\n",(0,t.jsx)(n.p,{children:"The previous code should output something like this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-txt",children:"La longueur de l'hypot\xe9nuse $c$ d'un triangle avec des c\xf4t\xe9s $a$ et $b$ en \nLaTeX est la suivante :\n\n\\[\nc = \\sqrt{{a}^2 + {b}^2}\n\\]\n"})}),"\n",(0,t.jsxs)(n.p,{children:["La longueur de l'hypot\xe9nuse ",(0,t.jsxs)(n.span,{className:"katex",children:[(0,t.jsx)(n.span,{className:"katex-mathml",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsx)(n.mrow,{children:(0,t.jsx)(n.mi,{children:"c"})}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"c"})]})})}),(0,t.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(n.span,{className:"base",children:[(0,t.jsx)(n.span,{className:"strut",style:{height:"0.4306em"}}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"c"})]})})]})," d'un triangle avec des c\xf4t\xe9s ",(0,t.jsxs)(n.span,{className:"katex",children:[(0,t.jsx)(n.span,{className:"katex-mathml",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsx)(n.mrow,{children:(0,t.jsx)(n.mi,{children:"a"})}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"a"})]})})}),(0,t.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(n.span,{className:"base",children:[(0,t.jsx)(n.span,{className:"strut",style:{height:"0.4306em"}}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"a"})]})})]})," et ",(0,t.jsxs)(n.span,{className:"katex",children:[(0,t.jsx)(n.span,{className:"katex-mathml",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsx)(n.mrow,{children:(0,t.jsx)(n.mi,{children:"b"})}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"b"})]})})}),(0,t.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(n.span,{className:"base",children:[(0,t.jsx)(n.span,{className:"strut",style:{height:"0.6944em"}}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"b"})]})})]})," en\nLaTeX est la suivante :"]}),"\n",(0,t.jsx)(n.span,{className:"katex-display",children:(0,t.jsxs)(n.span,{className:"katex",children:[(0,t.jsx)(n.span,{className:"katex-mathml",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mi,{children:"c"}),(0,t.jsx)(n.mo,{children:"="}),(0,t.jsx)(n.msqrt,{children:(0,t.jsxs)(n.mrow,{children:[(0,t.jsxs)(n.msup,{children:[(0,t.jsx)(n.mi,{children:"a"}),(0,t.jsx)(n.mn,{children:"2"})]}),(0,t.jsx)(n.mo,{children:"+"}),(0,t.jsxs)(n.msup,{children:[(0,t.jsx)(n.mi,{children:"b"}),(0,t.jsx)(n.mn,{children:"2"})]})]})})]}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"c = \\sqrt{{a}^2 + {b}^2}"})]})})}),(0,t.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(n.span,{className:"base",children:[(0,t.jsx)(n.span,{className:"strut",style:{height:"0.4306em"}}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"c"}),(0,t.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(n.span,{className:"mrel",children:"="}),(0,t.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(n.span,{className:"base",children:[(0,t.jsx)(n.span,{className:"strut",style:{height:"1.24em",verticalAlign:"-0.1777em"}}),(0,t.jsx)(n.span,{className:"mord sqrt",children:(0,t.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(n.span,{className:"vlist-r",children:[(0,t.jsxs)(n.span,{className:"vlist",style:{height:"1.0623em"},children:[(0,t.jsxs)(n.span,{className:"svg-align",style:{top:"-3.2em"},children:[(0,t.jsx)(n.span,{className:"pstrut",style:{height:"3.2em"}}),(0,t.jsxs)(n.span,{className:"mord",style:{paddingLeft:"1em"},children:[(0,t.jsxs)(n.span,{className:"mord",children:[(0,t.jsx)(n.span,{className:"mord",children:(0,t.jsx)(n.span,{className:"mord mathnormal",children:"a"})}),(0,t.jsx)(n.span,{className:"msupsub",children:(0,t.jsx)(n.span,{className:"vlist-t",children:(0,t.jsx)(n.span,{className:"vlist-r",children:(0,t.jsx)(n.span,{className:"vlist",style:{height:"0.7401em"},children:(0,t.jsxs)(n.span,{style:{top:"-2.989em",marginRight:"0.05em"},children:[(0,t.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(n.span,{className:"mord mtight",children:"2"})})]})})})})})]}),(0,t.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,t.jsx)(n.span,{className:"mbin",children:"+"}),(0,t.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,t.jsxs)(n.span,{className:"mord",children:[(0,t.jsx)(n.span,{className:"mord",children:(0,t.jsx)(n.span,{className:"mord mathnormal",children:"b"})}),(0,t.jsx)(n.span,{className:"msupsub",children:(0,t.jsx)(n.span,{className:"vlist-t",children:(0,t.jsx)(n.span,{className:"vlist-r",children:(0,t.jsx)(n.span,{className:"vlist",style:{height:"0.7401em"},children:(0,t.jsxs)(n.span,{style:{top:"-2.989em",marginRight:"0.05em"},children:[(0,t.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(n.span,{className:"mord mtight",children:"2"})})]})})})})})]})]})]}),(0,t.jsxs)(n.span,{style:{top:"-3.0223em"},children:[(0,t.jsx)(n.span,{className:"pstrut",style:{height:"3.2em"}}),(0,t.jsx)(n.span,{className:"hide-tail",style:{minWidth:"1.02em",height:"1.28em"},children:(0,t.jsx)(n.svg,{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"1.28em",viewBox:"0 0 400000 1296",preserveAspectRatio:"xMinYMin slice",children:(0,t.jsx)(n.path,{d:"M263,681c0.7,0,18,39.7,52,119\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\nc340,-704.7,510.7,-1060.3,512,-1067\nl0 -0\nc4.7,-7.3,11,-11,19,-11\nH40000v40H1012.3\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\nM1001 80h400000v40h-400000z"})})})]})]}),(0,t.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(n.span,{className:"vlist-r",children:(0,t.jsx)(n.span,{className:"vlist",style:{height:"0.1777em"},children:(0,t.jsx)(n.span,{})})})]})})]})]})]})}),"\n",(0,t.jsx)(n.h2,{id:"building-a-research-assistant-from-scratch",children:"Building a Research Assistant from Scratch"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"2023-11-19"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The code below is from ",(0,t.jsx)(n.a,{href:"https://www.youtube.com/watch?v=DjuXACWYkkU",children:"Building a Research Assistant from Scratch"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"See also:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://app.tavily.com/chat",children:"Tavily Chat"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.tavily.com/docs/tavily-api/introduction",children:"Tavily Introduction"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher",children:"GPT Researcher"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.langchain.com/langsmith",children:"Langsmith"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://blog.langchain.dev/announcing-langsmith/",children:"Announcing Langsmith"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.smith.langchain.com/overview",children:"Langsmith Overview"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/langchain-ai/langserve",children:"Langserve"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://platform.openai.com/docs/models/gpt-3-5",children:"GPT-3.5 models"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c",children:"This code"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://api.python.langchain.com/en/latest/utilities/langchain.utilities.duckduckgo_search.DuckDuckGoSearchAPIWrapper.html",children:"LangChain DuckDuckGoSearchAPIWrapper"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://python.langchain.com/docs/get_started/quickstart#serving-with-langserve",children:"Serving with LangServe"})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"install-the-required-libraries",children:"Install the required libraries"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"!pip install -U langchain openai duckduckgo-search --quiet\n!pip show langchain\n!pip install requests beautifulsoup4 --quiet\n"})}),"\n",(0,t.jsx)(n.h3,{id:"set-your-api-key-and-langchain-project",children:"Set your API key and LangChain project"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\nimport openai\n\nopenai.api_key = os.environ[\'OPENAI_API_KEY\']\nos.environ["LANGCHAIN_TRACING_V2"] = "true"\nos.environ["LANGCHAIN_PROJECT"] = "research-assistant"\nos.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"\n# os.environ["LANGCHAIN_API_KEY"] = "..." \n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain.schema.runnable import RunnablePassthrough, RunnableLambda\nfrom langchain.utilities import DuckDuckGoSearchAPIWrapper\nimport json\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'MODEL = "gpt-3.5-turbo-1106"\nRESULTS_PER_QUESTION = 3\n\n# See https://python.langchain.com/docs/integrations/tools/ddg\n# See https://api.python.langchain.com/en/latest/utilities/langchain.utilities.duckduckgo_search.DuckDuckGoSearchAPIWrapper.htm\nddg_search = DuckDuckGoSearchAPIWrapper()\n\ndef web_search(query: str, num_results: int = RESULTS_PER_QUESTION):\n    # Perform a web search using DuckDuckGo\'s API and return a list of result links\n    results = ddg_search.results(query, num_results)\n    return [r["link"] for r in results]\n\nSUMMARY_TEMPLATE = """\n{text} \n\n-----------\n\nUsing the above text, answer in short the following question: \n\n> {question}\n\n-----------\nif the question cannot be answered using the text, imply summarize the text. Include \nall factual information, numbers, stats etc if available.\n"""  \n\n# See https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/\nSUMMARY_PROMPT = ChatPromptTemplate.from_template(SUMMARY_TEMPLATE)\n\ndef scrape_text(url: str):\n    # Send a GET request to the webpage\n    try:\n        response = requests.get(url)\n\n        # Check if the request was successful\n        if response.status_code == 200:\n            # Parse the content of the request with BeautifulSoup\n            soup = BeautifulSoup(response.text, "html.parser")\n\n            # Extract all text from the webpage\n            page_text = soup.get_text(separator=" ", strip=True)\n\n            # Print the extracted text\n            return page_text\n        else:\n            return f"Failed to retrieve the webpage: Status code {response.status_code}"\n    except Exception as e:\n        print(e)\n        return f"Failed to retrieve the webpage: {e}"\n\nurl = "https://blog.langchain.dev/announcing-langsmith/"\n\nscrape_and_summarize_chain = RunnablePassthrough.assign(\n    summary = RunnablePassthrough.assign(\n        text=lambda x: scrape_text(x["url"])[:10000]\n    ) | SUMMARY_PROMPT | ChatOpenAI(model=MODEL) | StrOutputParser()\n) | (lambda x: f"URL: {x[\'url\']}\\n\\nSUMMARY: {x[\'summary\']}")\n\nweb_search_chain = RunnablePassthrough.assign(\n    urls = lambda x: web_search(x["question"])\n) | (lambda x: [{"question": x["question"], "url": u} for u in x["urls"]]) | scrape_and_summarize_chain.map()\n\nSEARCH_PROMPT = ChatPromptTemplate.from_messages(\n    [\n        (\n            "user",\n            "Write 3 google search queries to search online that form an "\n            "objective opinion from the following: {question}\\n"\n            "You must respond with a list of strings in the following format: "\n            \'["query 1", "query 2", "query 3"].\',\n        ),\n    ]\n)\n\nsearch_question_chain = SEARCH_PROMPT | ChatOpenAI(temperature=0) | StrOutputParser() | json.loads\n\nfull_research_chain = search_question_chain | (lambda x: [{"question": q} for q in x]) | web_search_chain.map()\n\nWRITER_SYSTEM_PROMPT = """\nYou are an AI critical thinker research assistant. Your sole purpose is to write well \nwritten, critically acclaimed, objective and structured reports on given text."""\n\n# Report prompts from \n# https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py\nRESEARCH_REPORT_TEMPLATE = """Information:\n--------\n{research_summary}\n--------\n\nUsing the above information, answer the following question or topic: "{question}" in a detailed report -- \\\nThe report should focus on the answer to the question, should be well structured, informative, \\\nin depth, with facts and numbers if available and a minimum of 1,200 words.\n\nYou should strive to write the report as long as you can using all relevant and necessary information provided.\nYou must write the report with markdown syntax.\nYou MUST determine your own concrete and valid opinion based on the given information. \nDo NOT deter to general and meaningless conclusions.\nWrite all used source urls at the end of the report, and make sure to not add duplicated sources, but only \none reference for each.\nYou must write the report in apa format.\nPlease do your best, this is very important to my career."""\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        ("system", WRITER_SYSTEM_PROMPT),\n        ("user", RESEARCH_REPORT_TEMPLATE),\n    ]\n)\n\ndef collapse_list_of_lists(list_of_lists):\n    """\n    Collapse a list of lists into a single string with double line breaks.\n    """\n    content = []\n    for l in list_of_lists:\n        content.append("\\n\\n".join(l))\n    return "\\n\\n".join(content)\n\nchain = RunnablePassthrough.assign(\n    research_summary = full_research_chain | collapse_list_of_lists\n) | prompt | ChatOpenAI(model=MODEL) | StrOutputParser()\n\nresult = chain.invoke(\n    {\n        "question": "what is the difference between langsmith and langchain"\n    }\n)\n\nfrom IPython.display import Markdown, display\ndisplay(Markdown(result))\n'})}),"\n",(0,t.jsx)(n.p,{children:"LangSmith is a developer platform that lets you debug, test, evaluate, and monitor chains built\non any LLM framework and seamlessly integrates with LangChain. With LangSmith, you should see this."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"s01",src:a(21774).Z+"",width:"4006",height:"2334"})}),"\n",(0,t.jsx)(n.p,{children:"The previous code should return something like this."}),"\n",(0,t.jsx)(n.admonition,{title:"Response",type:"note",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-txt",children:"# Report on the Difference Between LangSmith and LangChain\n\n## Introduction\n\nIn the rapidly evolving field of artificial intelligence (AI), frameworks and tools play a crucial role in the development and deployment of AI applications. LangSmith and LangChain are two such frameworks developed to address specific challenges in working with large language models (LLMs). This report aims to provide a detailed analysis of the differences between LangSmith and LangChain based on the information available from various sources.\n\n## LangChain: A Framework for LLM-Powered Applications\n\nLangChain is a framework designed to facilitate the development of applications powered by language models. It offers high-level APIs for working with LLMs and provides off-the-shelf chains for quick start. The primary focus of LangChain is on prototyping and building intelligent agents capable of performing multiple tasks. As an open-source Python package, LangChain addresses the challenges of building LLM-powered applications, providing speed of iteration, choice, flexibility, and the latest cognitive architectures and interaction patterns out-of-the-box. Additionally, LangChain offers deeper product integrations with Microsoft, including ease of procurement in the Azure Marketplace and data security peace-of-mind with deployments within the customer\u2019s Azure VPC ([LangChain Blog](https://blog.langchain.dev/langchain-expands-collaboration-with-microsoft/)).\n\n## LangSmith: A Unified Platform for LLM-Powered Applications\n\nOn the other hand, LangSmith is positioned as a complementary Software as a Service (SaaS) product that aims to bridge the gap between prototyping and production for LLM-powered applications. It addresses key challenges in production environments by providing features for debugging, testing, evaluating, and monitoring LLM applications. LangSmith also offers a simple and intuitive user interface to reduce the barrier to entry for developers without a software background. Additionally, it integrates seamlessly with open-source evaluation modules for heuristic and LLM evaluations. LangSmith is designed to improve developer productivity, decrease time to production, and harden applications to be more reliable at scale ([LangSmith Blog](https://blog.langchain.dev/announcing-langsmith/)).\n\n## Key Differences\n\n### 1. Focus and Purpose\n\nLangChain is primarily focused on prototyping and building intelligent agents capable of performing multiple tasks, while LangSmith is geared towards addressing production challenges and aims to bridge the gap between prototype and production for LLM-powered applications ([Medium - What is LangSmith and why should I care as a developer](https://logankilpatrick.medium.com/what-is-langsmith-and-why-should-i-care-as-a-developer-e5921deb54b5)).\n\n### 2. Features and Capabilities\n\nLangChain offers high-level APIs to work with LLMs and off-the-shelf chains for quick start, making it ideal for building intelligent agents. On the other hand, LangSmith provides features for debugging, testing, evaluating, and monitoring LLM applications, with a focus on providing a simple and intuitive user interface to reduce the barrier to entry for developers without a software background. LangSmith also integrates seamlessly with open-source evaluation modules for heuristic and LLM evaluations ([LogRocket - LangSmith: Test LLMS AI Applications](https://blog.logrocket.com/langsmith-test-llms-ai-applications/)).\n\n### 3. Development Stage\n\nLangSmith is described as the production counterpart to LangChain, indicating that LangChain is used for prototyping, while LangSmith is designed to tackle production challenges and provide a unified platform for LLM-powered applications ([Dev.to - What is LangSmith and why should I care as a developer](https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k)).\n\n### 4. Integration and Extensibility\n\nLangSmith aims to connect with as many tools as possible and is working on integrations with OpenAI evals and multiple fine-tuning providers. Additionally, it offers extensibility and the ability to integrate with other applications and services, which is a key feature that the authors would like to see ([Dev.to - What is LangSmith and why should I care as a developer](https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k)).\n\n### 5. User Interface and Usability\n\nLangSmith is specifically designed to provide a simple and intuitive user interface to facilitate tasks such as debugging, testing, evaluating, monitoring, and usage metrics, making it easier for developers to work with large language models. This emphasis on usability sets LangSmith apart from LangChain, which is focused more on the development and prototyping aspect of LLM-powered applications ([Medium - What is LangSmith and why should I care as a developer](https://logankilpatrick.medium.com/what-is-langsmith-and-why-should-i-care-as-a-developer-e5921deb54b5)).\n\n## Conclusion\n\nIn conclusion, LangChain and LangSmith serve different purposes in the development and deployment of LLM-powered applications. LangChain is primarily focused on prototyping and building intelligent agents, offering high-level APIs and off-the-shelf chains for quick start. On the other hand, LangSmith is designed to address production challenges by providing features for debugging, testing, evaluating, and monitoring LLM applications, with an emphasis on usability and integration with other tools and services. Both frameworks play significant roles in different stages of the development lifecycle of LLM-powered applications, and their distinct features and capabilities cater to different needs of developers in the AI space.\n\nOverall, the choice between LangSmith and LangChain depends on the specific requirements of the development project, with LangChain being suitable for prototyping and building intelligent agents, while LangSmith is more suitable for addressing production challenges and providing a unified platform for LLM-powered applications.\n\n## References\n\n* LangChain vs LlamaIndex: Difference and Which One to Choose. (n.d.). Retrieved from [Getting Started AI](https://www.gettingstarted.ai/langchain-vs-llamaindex-difference-and-which-one-to-choose/)\n* Kilpatrick, L. (n.d.). What is LangSmith and Why Should I Care as a Developer? Retrieved from [Medium](https://logankilpatrick.medium.com/what-is-langsmith-and-why-should-i-care-as-a-developer-e5921deb54b5)\n* Kilpatrick, L. (n.d.). What is LangSmith and Why Should I Care as a Developer? Retrieved from [DEV Community](https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k)\n* Announcing LangSmith. (n.d.). Retrieved from [LangChain Blog](https://blog.langchain.dev/announcing-langsmith/)\n* LangSmith: Test LLMS AI Applications. (n.d.). Retrieved from [LogRocket](https://blog.logrocket.com/langsmith-test-llms-ai-applications/)\n* LangChain Expands Collaboration with Microsoft. (n.d.). Retrieved from [LangChain Blog](https://blog.langchain.dev/langchain-expands-collaboration-with-microsoft/)\n"})})}),"\n",(0,t.jsx)(n.p,{children:"To search in ArXiv, you just need to replace a part of the code with this."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"!pip install arxiv --quiet\n!pip install langserve fastapi unicorn --quiet\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain.retrievers import ArxivRetriever\n\nretriever = ArxivRetriever()\nSUMMARY_TEMPLATE = """{doc} \n\n-----------\n\nUsing the above text, answer in short the following question: \n\n> {question}\n\n-----------\nif the question cannot be answered using the text, imply summarize the text. Include all factual information, numbers, stats etc if available."""  # noqa: E501\nSUMMARY_PROMPT = ChatPromptTemplate.from_template(SUMMARY_TEMPLATE)\n\n\nscrape_and_summarize_chain = RunnablePassthrough.assign(\n    summary =  SUMMARY_PROMPT | ChatOpenAI(model="gpt-3.5-turbo-1106") | StrOutputParser()\n) | (lambda x: f"Title: {x[\'doc\'].metadata[\'Title\']}\\n\\nSUMMARY: {x[\'summary\']}")\n\nweb_search_chain = RunnablePassthrough.assign(\n    docs = lambda x: retriever.get_summaries_as_docs(x["question"])\n)| (lambda x: [{"question": x["question"], "doc": u} for u in x["docs"]]) | scrape_and_summarize_chain.map()\n\nweb_search_chain.invoke({"question": "what papers did noam shazeer write?"})\n'})}),"\n",(0,t.jsx)(n.h2,{id:"running-langchain-and-a-mistralai-7b-model-in-google-colab",children:"Running LangChain and a MistralAI 7B Model in Google Colab"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'!pip install bitsandbytes accelerate xformers einops langchain faiss-cpu transformers sentence-transformers\n\nfrom typing import List\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\nimport torch\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains import ConversationalRetrievalChain, RetrievalQA\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain_core.vectorstores import VectorStoreRetriever\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.vectorstores import FAISS\n\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nprint("Device:", device)\nif device == \'cuda\':\n    print(torch.cuda.get_device_name(0))\n\norig_model_path = "mistralai/Mistral-7B-Instruct-v0.1"\nmodel_path = "filipealmeida/Mistral-7B-Instruct-v0.1-sharded"\nbnb_config = BitsAndBytesConfig(\n                                load_in_4bit=True,\n                                bnb_4bit_use_double_quant=True,\n                                bnb_4bit_quant_type="nf4",\n                                bnb_4bit_compute_dtype=torch.bfloat16,\n                               )\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, \n    trust_remote_code=True, \n    quantization_config=bnb_config, \n    device_map="auto")\ntokenizer = AutoTokenizer.from_pretrained(orig_model_path)\n\ntext_generation_pipeline = transformers.pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task="text-generation",\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    repetition_penalty=1.1,\n    return_full_text=True,\n    max_new_tokens=100)\nmistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n\ntext = "What is Mistral? Write a short answer."\nanswer = mistral_llm.invoke(text)\nprint(answer)\n\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template(\n    "Tell me a {adjective} joke about {content}."\n)\n\nllm_chain = prompt | mistral_llm\nanswer = llm_chain.invoke({"adjective": "funny", "content": "chickens"})\nprint(answer)\n\nfrom langchain.prompts import ChatPromptTemplate\n\nchat_prompt = ChatPromptTemplate.from_messages(\n    [\n        ("system", "You are a helpful AI bot. Your name is {name}. Answer with short sentences."),\n    ]\n)\n\nllm_chain = chat_prompt | mistral_llm\nanswer = llm_chain.invoke({"name": "Mistral", "user_input": "What is your name?"})\nprint(answer)\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\nembeddings = HuggingFaceEmbeddings(\n    model_name="sentence-transformers/all-MiniLM-l6-v2",\n    model_kwargs={"device": "cuda"},\n)\n\ndb_docs = [\n    "Airbus\'s registered headquarters is located in Leiden, Netherlands.",\n]\n\nfrom langchain.vectorstores import FAISS\nfrom langchain_core.vectorstores import VectorStoreRetriever\n\n\nvector_db = FAISS.from_texts(db_docs, embeddings)\nretriever = VectorStoreRetriever(vectorstore=vector_db)\n\ntemplate = """You are a helpful AI assistant. Use the following pieces of context to answer the question at the end.\n              {context}\n              If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer.\n              Chat history: {history}\n              Question: {question}\n              Write your answers short. Helpful Answer:"""\n\nprompt = PromptTemplate(\n        template=template, input_variables=["history", "context", "question"]\n    )\nqa = RetrievalQA.from_chain_type(\n        llm=mistral_llm,\n        chain_type="stuff",\n        retriever=retriever,\n        chain_type_kwargs={\n            "verbose": False,\n            "prompt": prompt,\n            "memory": ConversationBufferMemory(\n                memory_key="history",\n                input_key="question"),\n        }\n    )\n\nanswer = qa.run("Hi, who are you?")\nprint(answer)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"microsoft-word-document-qa",children:"Microsoft Word Document QA"}),"\n",(0,t.jsxs)(n.p,{children:["See ",(0,t.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/document_loaders/microsoft_word#using-docx2txt",children:"Docx2txt"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain import OpenAI, VectorDBQA\nimport pickle\nimport textwrap\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.llms import OpenAI\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import PyPDFLoader, Docx2txtLoader, DirectoryLoader\nfrom langchain.document_loaders import UnstructuredWordDocumentLoader, UnstructuredFileLoader\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set up the environment variable for the OpenAI API key\nos.environ[\"OPENAI_API_KEY\"] = \"\"\n\ndef get_documents(folder_path, file_extension):\n    documents = []\n    if file_extension == 'pdf':\n        pdf_loader = DirectoryLoader(folder_path, glob=\"./*.pdf\", loader_cls=PyPDFLoader)  # Select PDF files\n        documents += pdf_loader.load()\n    elif file_extension == 'txt':\n        txt_loader = DirectoryLoader(folder_path, glob=\"./*.txt\")  # Select TXT files\n        documents += txt_loader.load()\n    elif file_extension == 'docx':\n        docx_loader = DirectoryLoader(folder_path, glob=\"./*.docx\", loader_cls=UnstructuredWordDocumentLoader)\n        documents += docx_loader.load()\n    elif file_extension == 'combined':\n        pdf_loader = DirectoryLoader(folder_path, glob=\"./*.pdf\", loader_cls=PyPDFLoader)  # Select PDF files\n        documents += pdf_loader.load()\n        txt_loader = DirectoryLoader(folder_path, glob=\"./*.txt\")  # Select TXT files\n        documents += txt_loader.load()\n    else:\n        return None\n\n    return documents\n\ndef get_query_result(query, documents):\n    # Split documents\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n    texts = text_splitter.split_documents(documents)\n\n    # Query documents\n    embeddings = OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])\n    docsearch = Chroma.from_documents(texts, embeddings)\n    qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=docsearch, return_source_documents=True)\n    result = qa({\"query\": query})\n\n    result_text = result['result'].strip()\n    source = result.get('source_documents', [{}])[0].metadata.get('source', '')\n    page = result.get('source_documents', [{}])[0].metadata.get('page', '')\n\n    return result_text, source, page\n\ndef chat_loop(file_extension, folder_path):\n    documents = get_documents(folder_path, file_extension)\n    if documents is None:\n        print(\"Invalid folder path or no supported files found.\")\n        return\n\n    while True:\n        query = input(\"Enter your query (type 'exit' to end): \")\n        if query.lower() == 'exit':\n            break\n\n        result = get_query_result(query, documents)\n\n        if result is not None:\n            result_text, source, page = result\n            print(\"Result:\", result_text)\n            if source:\n                print(\"Source:\", source)\n                print(\"Page:\", page)\n        else:\n            print(\"No answer found for the query.\")\n\n        print()  # Print an empty line for separation\n\n# Get the selected file extension and folder path from the webpage\nselected_file_extension = 'docx'  # Replace with the value obtained from the dropdown\nfolder_path = 'Documents'  # Replace with the folder path obtained from the user input on the webpage\n\n# Start the chat loop\nchat_loop(selected_file_extension, folder_path)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"langchain-with-ollama",children:"LangChain with OLlama"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain.lms import OLlama\n\nllm = OLlama(model="llama2", \n             temperature=0.9,\n            )\n\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate(\n    input_variables=["subject"],\n    template="List 3 interesting facts about {subject}?"\n)\n\nfrom langchain.chains import LLMChain\n\nchain = LLMChain(llm=llm,\n                 prompt=prompt,\n                 verbose=False)\n\nprint(chain.invoke("Python Programming"))\n'})}),"\n",(0,t.jsx)(n.h2,{id:"see-also",children:"See also"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c",children:"Getting Started with LangChain: A Beginner\u2019s Guide to Building LLM-Powered Applications"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},21774:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/s01-b7574a4841e0af7848636dbadda32415.png"},11151:(e,n,a)=>{a.d(n,{Z:()=>o,a:()=>r});var t=a(67294);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);