"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[1373],{68909:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var i=n(85893),r=n(11151),t=n(2828);const a={sidebar_label:"Quiz 2 du 11 octobre 2023",sidebar_position:32},l="Quiz 2 du 11 octobre 2023",o={id:"courses/university/gif-7005/quiz-2",title:"Quiz 2 du 11 octobre 2023",description:"Question 1",source:"@site/docs/courses/university/gif-7005/quiz-2.mdx",sourceDirName:"courses/university/gif-7005",slug:"/courses/university/gif-7005/quiz-2",permalink:"/docs/courses/university/gif-7005/quiz-2",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/university/gif-7005/quiz-2.mdx",tags:[],version:"current",sidebarPosition:32,frontMatter:{sidebar_label:"Quiz 2 du 11 octobre 2023",sidebar_position:32},sidebar:"tutorialSidebar",previous:{title:"Quiz 1 du 27 septembre 2023",permalink:"/docs/courses/university/gif-7005/quiz-1"},next:{title:"Quiz 3 du 25 octobre 2023",permalink:"/docs/courses/university/gif-7005/quiz-3"}},d={},c=[{value:"Question 1",id:"question-1",level:2},{value:"Question 2",id:"question-2",level:2},{value:"Question 3",id:"question-3",level:2},{value:"Question 4",id:"question-4",level:2}];function u(e){const s={h1:"h1",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.h1,{id:"quiz-2-du-11-octobre-2023",children:"Quiz 2 du 11 octobre 2023"}),"\n","\n","\n",(0,i.jsxs)(t.Z,{children:[(0,i.jsx)(s.h2,{id:"question-1",children:"Question 1"}),(0,i.jsx)(s.p,{children:"Peux-tu r\xe9pondre aux questions suivantes?"}),(0,i.jsx)(s.p,{children:"Parmi les \xe9l\xe9ments ci-dessous, s\xe9lectionnez ceux qui sont vrais en ce qui a trait aux discriminants lin\xe9aires."}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"A."})," Le crit\xe8re d'erreur du perceptron est obtenu \xe0 partir de la somme des valeurs des donn\xe9es mal class\xe9es dans l'ensemble de donn\xe9es. | The perceptron error criterion is derived from the sum of misclassified data points in the dataset."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," Le crit\xe8re d'erreur du perceptron est bas\xe9 sur la somme des vecteurs des donn\xe9es mal class\xe9es."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"B."})," Le perceptron assigne une classe sur la base de la valeur absolue de la fonction discriminante. | The perceptron assigns a class based on the absolute value of the discriminant function."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," Le perceptron assigne une classe sur la base du signe de la fonction discriminante et non de sa valeur absolue."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"C."})," Un discriminant avec une fonction de base implique une transformation non lin\xe9aire trait\xe9e sous une forme lin\xe9aire. | A discriminant with a basis function involves a non-linear transformation processed in a linear form."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," L'utilisation d'une fonction de base permet de transformer des donn\xe9es non lin\xe9aires en une forme qui peut \xeatre trait\xe9e lin\xe9airement."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"D."})," La r\xe9gression logistique exige toujours que les variables ind\xe9pendantes soient normalement distribu\xe9es. | Logistic regression always requires the independent variables to be normally distributed."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," La r\xe9gression logistique ne n\xe9cessite pas que les variables ind\xe9pendantes soient normalement distribu\xe9es."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"E."})," L'objectif principal de la r\xe9gression logistique est de d\xe9terminer la matrice de covariance entre diff\xe9rentes caract\xe9ristiques. | The main goal of logistic regression is to determine the covariance matrix between different features."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," L'objectif principal de la r\xe9gression logistique n'est pas de d\xe9terminer la matrice de covariance, mais de pr\xe9dire la probabilit\xe9 qu'une observation appartienne \xe0 une cat\xe9gorie particuli\xe8re."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"F."})," Les mod\xe8les discriminants visent \xe0 r\xe9soudre uniquement le probl\xe8me de la discrimination, consid\xe9rant l'estimation des densit\xe9s comme une \xe9tape inutile. | Discriminative models aim to solve only the problem of discrimination, considering the estimation of densities as an unnecessary step."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," Les mod\xe8les discriminants, contrairement aux mod\xe8les g\xe9n\xe9ratifs, se concentrent directement sur la discrimination sans estimer les densit\xe9s de probabilit\xe9 des classes."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"G."})," La r\xe9gression logistique est un mod\xe8le lin\xe9aire incapable de traiter des donn\xe9es de classement non lin\xe9airement s\xe9parables. | Logistic regression is a linear model unable to handle non-linearly separable classification data."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," Bien que la r\xe9gression logistique soit un mod\xe8le lin\xe9aire, elle peut \xeatre \xe9tendue pour traiter des donn\xe9es non lin\xe9aires en utilisant des fonctions de base ou d'autres transformations."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"H."})," Un discriminant lin\xe9aire est d\xe9crit par une combinaison de poids associ\xe9s aux caract\xe9ristiques et d'un terme de biais. | A linear discriminant is described by a combination of weights associated with features and a bias term."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," Un discriminant lin\xe9aire est d\xe9crit par une combinaison lin\xe9aire des caract\xe9ristiques (avec des poids associ\xe9s) et d'un terme de biais."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"I."})," En classement param\xe9trique, le concept de matrice de covariance partag\xe9e implique que les classes ne peuvent pas \xeatre s\xe9par\xe9es lin\xe9airement. | In parametric classification, the concept of a shared covariance matrix implies that the classes cannot be linearly separated."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," Une matrice de covariance partag\xe9e ne signifie pas n\xe9cessairement que les classes ne peuvent pas \xeatre s\xe9par\xe9es lin\xe9airement. Elle signifie simplement que toutes les classes sont suppos\xe9es avoir la m\xeame matrice de covariance."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"J."}),' La strat\xe9gie \xab un contre tous \xbb pour les mod\xe8les multiclasse utilise une fonction discriminante par classe. | The "one versus all" (OvA) strategy for multi-class models uses one discriminating function per class.']}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"}),' La strat\xe9gie "un contre tous" (ou "one versus all") pour les mod\xe8les multiclasse entra\xeene un classificateur par classe, o\xf9 cette classe est trait\xe9e comme positive et toutes les autres classes sont trait\xe9es comme n\xe9gatives.']}),"\n"]}),(0,i.jsx)(s.p,{children:"Donc, les \xe9nonc\xe9s A, C, F, H, et J sont vrais."}),(0,i.jsx)(s.h2,{id:"question-2",children:"Question 2"}),(0,i.jsx)(s.p,{children:"Nous avons 5 choix de mesures de distance :"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Distance de Manhattan"}),"\n",(0,i.jsx)(s.li,{children:"Distance euclidienne"}),"\n",(0,i.jsx)(s.li,{children:"Distance D-infini"}),"\n",(0,i.jsx)(s.li,{children:"Distance de Minkowsky"}),"\n",(0,i.jsx)(s.li,{children:"Distance de Mahalanobis"}),"\n"]}),(0,i.jsx)(s.p,{children:"Pour chaque proposition suivante, choisissez la mesure de distance correspondante parmi les 5 choix pr\xe9c\xe9dents."}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q1."})," Une m\xe9trique g\xe9n\xe9ralis\xe9e qui peut englober une gamme de mesures de distance. | A generalized metric that can capture a range of distance measures."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Distance de Minkowsky"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q2."})," Inclut un param\xe8tre afin de faire varier l'importance des variables dans la m\xe9trique en fonction de leur magnitude.| Includes a parameter to vary the importance of the variables in the metric according to their magnitude."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Distance de Minkowsky"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q3."})," Sa particularit\xe9 r\xe9side dans sa capacit\xe9 \xe0 prendre en compte la variance et la structure de covariance des donn\xe9es. | Its particularity lies in its ability to account for the variance and covariance structure of the data."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Distance de Mahalanobis"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q4."}),' \xc9galement appel\xe9e \xab norme L2 \xbb. | Also referred to as the "L2 norm."']}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Distance euclidienne"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q5."})," Calcul\xe9e comme la somme des diff\xe9rences absolues entre les coordonn\xe9es de deux points. | Calculated as the sum of the absolute differences between the coordinates of two points."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Distance de Manhattan"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q6."})," Ne prend en compte que la variable pour laquelle la diff\xe9rence est la plus grande. | Takes into account only the variable for which the difference is the largest."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Distance D-infini"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q7."})," Dans un parcours en grille, cette distance repr\xe9sente le chemin le plus court qu'une voiture emprunterait en naviguant uniquement dans des rues orthogonales. | In a grid-based path, this distance represents the shortest path a car would take navigating only orthogonal streets."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Distance de Manhattan"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q8."})," Type de distance mesur\xe9e \xe0 l'aide d'une r\xe8gle entre deux points d'un plan en 2D. | The type of distance you'd measure with a ruler between two points in a 2D plane."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Distance euclidienne"}),"\n"]}),(0,i.jsx)(s.h2,{id:"question-3",children:"Question 3"}),(0,i.jsx)(s.p,{children:"Nous avons 7 choix possibles de m\xe9thodes sur le classement avec discriminants lin\xe9aires:"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"R\xe8gle du perceptron"}),"\n",(0,i.jsx)(s.li,{children:"M\xe9thode des moindres carr\xe9s"}),"\n",(0,i.jsx)(s.li,{children:"R\xe9gression logistique"}),"\n",(0,i.jsx)(s.li,{children:"R\xe9gression d'ar\xeate (Ridge regression)"}),"\n",(0,i.jsx)(s.li,{children:"LASSO"}),"\n",(0,i.jsx)(s.li,{children:"Mod\xe8les de lois normales avec covariance partag\xe9es"}),"\n",(0,i.jsx)(s.li,{children:"Descente du gradient stochastique"}),"\n"]}),(0,i.jsx)(s.p,{children:"Pour chacune des propositions suivantes, s\xe9lectionnez la m\xe9thode correspondante sur le classement avec discriminants lin\xe9aires."}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q1."})," Utilise la fonction sigmo\xefde pour resserrer la sortie entre 0 et 1. | Uses the sigmoid function to squeeze the output between 0 and 1."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"R\xe9gression logistique"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q2."})," Impl\xe9mente la r\xe9gularisation L2 pour \xe9viter le sur-apprentissage en p\xe9nalisant les grands coefficients. | Implements L2 regularization to prevent overfitting by penalizing large coefficients."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"R\xe9gression d'ar\xeate (Ridge regression)"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q3."})," Minimise la somme des carr\xe9s des diff\xe9rences entre les valeurs observ\xe9es et estim\xe9es. | Minimizes the sum of the squared differences between observed and estimated values."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"M\xe9thode des moindres carr\xe9s"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q4."})," Permet de pr\xe9dire la probabilit\xe9 d'appartenance \xe0 une classe sans estimer directement les param\xe8tres de sa distribution. | Allows predicting the probability of belonging to a class without directly estimating its distribution parameters."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"R\xe9gression logistique"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q5."})," Sensible aux valeurs aberrantes qui peuvent affecter le r\xe9sultat de l'ajustement de mani\xe8re disproportionn\xe9e. | Sensitive to outliers as they can disproportionately affect the fit."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"M\xe9thode des moindres carr\xe9s"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q6."})," Souvent utilis\xe9 dans la s\xe9lection des caract\xe9ristiques car il permet de ramener certains coefficients \xe0 z\xe9ro. | Often used in feature selection because it can shrink some coefficients to zero."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"LASSO"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q7."})," Ne converge que si les donn\xe9es sont lin\xe9airement s\xe9parables et que le taux d'apprentissage est suffisamment faible. | Converges only if the data is linearly separable and the learning rate is sufficiently small."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"R\xe8gle du perceptron"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Q8."})," Permet le traitement de tr\xe8s grands ensembles de donn\xe9es, en une seule passe sur celles-ci. | Can process very large datasets, in only one pass over them."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Descente du gradient stochastique"}),"\n"]}),(0,i.jsx)(s.h2,{id:"question-4",children:"Question 4"}),(0,i.jsx)(s.p,{children:"Parmi les \xe9l\xe9ments ici-bas, s\xe9lectionnez ceux qui sont vrais \xe0 propos de l'estimation non param\xe9trique de densit\xe9."}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"A."})," L'application de la m\xe9thode des k-PPV dans un contexte donn\xe9 est d\xe9finie principalement par le nombre de voisins, la mesure de la distance et l'ensemble de donn\xe9es de r\xe9f\xe9rence. | The application of the k-NN method in a given context is defined mainly by the number of neighbors, the distance measurement, and the reference dataset."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," Le k-NN d\xe9pend du nombre de voisins, de la mesure de la distance et de l'ensemble de donn\xe9es de r\xe9f\xe9rence."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"B."})," La distance de Minkowsky accorde toujours la m\xeame importance \xe0 toutes les dimensions. | The Minkowsky distance always gives equal weight to all dimensions."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," La distance de Minkowsky ne donne pas n\xe9cessairement un poids \xe9gal \xe0 toutes les dimensions. Elle d\xe9pend de la valeur de l'ordre p. Par exemple, lorsque p=1, c'est la distance de Manhattan, et lorsque p=2, c'est la distance euclidienne."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"C."})," L'un des inconv\xe9nients des histogrammes en tant que m\xe9thode d'estimation de la densit\xe9 est qu'ils peuvent \xeatre sensibles \xe0 l'emplacement de d\xe9part des compartiments. | One of the drawbacks of histograms as a density estimation method is that they can be sensitive to the starting location of the bins."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," L'emplacement de d\xe9part des compartiments dans les histogrammes peut influencer l'apparence de l'histogramme."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"D."})," L'estimation de densit\xe9 par noyau peut \xeatre \xe9tendue au classement, o\xf9 la fonction discriminante est d\xe9termin\xe9e sur la base des densit\xe9s estim\xe9es pour chaque classe. | Kernel density estimation can be extended to classification, where the discriminant function is determined based on the estimated densities for each class."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," L'estimation de densit\xe9 par noyau peut \xeatre utilis\xe9e pour le classement."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"E."})," Le taux d'erreur bay\xe9sien optimal repr\xe9sente le taux d'erreur lorsque les vraies densit\xe9s de probabilit\xe9 de classe sont connues ; il est alors impossible d'obtenir une meilleure g\xe9n\xe9ralisation. | The optimal Bayesian error rate represents the error rate when the true class probability densities are known; it is then impossible to achieve better generalization."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," Si les vraies densit\xe9s de probabilit\xe9 sont connues, le taux d'erreur bay\xe9sien optimal est le meilleur possible."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"F."})," Des m\xe9thodes telles que l'\xe9dition de Wilson et la condensation de Hart peuvent \xeatre utilis\xe9es pour s\xe9lectionner un sous-ensemble repr\xe9sentatif de donn\xe9es dans les k-PPV. | Methods like Wilson's editing and Hart's condensing can be used to select a representative subset of data in k-NN."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," Les m\xe9thodes d'\xe9dition de Wilson et de condensation de Hart aident \xe0 s\xe9lectionner un sous-ensemble de donn\xe9es pour les k-NN."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"G."})," Le noyau d'Epanechnikov a un support infini. | The Epanechnikov kernel has an infinite support range."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," Le noyau d'Epanechnikov a un support fini."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"H."})," Dans l'estimation de densit\xe9 par noyau, le choix de la largeur de fen\xeatre affecte le compromis biais-variance : une petite largeur de fen\xeatre peut conduire \xe0 une variance \xe9lev\xe9e (sur-apprentissage) tandis qu'une grande largeur de fen\xeatre peut entra\xeener un biais \xe9lev\xe9 (sous-apprentissage). | In kernel density estimation, the choice of bandwidth affects the bias-variance trade-off: a small bandwidth can lead to high variance (overfitting) while a large bandwidth can cause high bias (underfitting)."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," La largeur de la fen\xeatre (ou la bande passante) dans l'estimation de densit\xe9 par noyau affecte le compromis entre biais et variance."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"I."})," L'estimation de densit\xe9 par noyau fournit une estimation plus souple que l'estimateur na\xeff d'histogramme. | Kernel density estimation provides a softer estimate compared to the naive histogram estimator."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," L'estimation de densit\xe9 par noyau est g\xe9n\xe9ralement plus souple et continue que l'estimation bas\xe9e sur des histogrammes."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"J."})," Les m\xe9thodes non param\xe9triques ne posent pas d'hypoth\xe8ses fortes sur la nature de la distribution sous-jacente des donn\xe9es. | Nonparametric methods make no strong assumptions about the form of the underlying distribution of the data."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," Les m\xe9thodes non param\xe9triques n'assument pas de forme sp\xe9cifique pour la distribution sous-jacente."]}),"\n"]}),(0,i.jsx)(s.p,{children:"Donc, les \xe9nonc\xe9s A, C, D, E, F, H, I, et J sont vrais."})]})]})}function h(e={}){const{wrapper:s}={...(0,r.a)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}},2828:(e,s,n)=>{n.d(s,{Z:()=>r});n(67294);var i=n(85893);const r=e=>{let{children:s}=e;return(0,i.jsx)("p",{children:"Sorry, this content is protected and not available."})}},11151:(e,s,n)=>{n.d(s,{Z:()=>l,a:()=>a});var i=n(67294);const r={},t=i.createContext(r);function a(e){const s=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(t.Provider,{value:s},e.children)}}}]);