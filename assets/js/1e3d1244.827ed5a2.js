"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[6448],{91998:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var a=s(85893),t=s(11151);const i={},r="Best practices CPU and GPU",o={id:"courses/python-gpu/index",title:"Best practices CPU and GPU",description:"\x3c!--",source:"@site/docs/courses/python-gpu/index.md",sourceDirName:"courses/python-gpu",slug:"/courses/python-gpu/",permalink:"/docs/courses/python-gpu/",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/python-gpu/index.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Training and fine-tuning LLMs",permalink:"/docs/courses/fine-tuning-llms"},next:{title:"Disposable Income Calculator",permalink:"/docs/models/revdisp"}},l={},c=[{value:"What is a GPU and when should I use one?",id:"what-is-a-gpu-and-when-should-i-use-one",level:2},{value:"CPU",id:"cpu",level:3},{value:"GPU",id:"gpu",level:3},{value:"First set of best practices - when to use a GPU",id:"first-set-of-best-practices---when-to-use-a-gpu",level:3},{value:"Pseudo-Code Examples",id:"pseudo-code-examples",level:3},{value:"Example 1 - Embarassingly parallel problem",id:"example-1---embarassingly-parallel-problem",level:4},{value:"Example 2 - Serial &amp; branching problem",id:"example-2---serial--branching-problem",level:4},{value:"Example 3 - Communication-bound parallel problem",id:"example-3---communication-bound-parallel-problem",level:4},{value:"GPUs and Data Loading",id:"gpus-and-data-loading",level:2},{value:"Second set of best practices - Data Loading",id:"second-set-of-best-practices---data-loading",level:3},{value:"Monitoring &amp; Profiling - How do I know if I&#39;m using a GPU correctly?",id:"monitoring--profiling---how-do-i-know-if-im-using-a-gpu-correctly",level:2},{value:"Nvidia-smi",id:"nvidia-smi",level:3},{value:"Nvtop",id:"nvtop",level:3},{value:"Cluster Portals",id:"cluster-portals",level:3},{value:"Third set of best practices - Monitoring and Profiling",id:"third-set-of-best-practices---monitoring-and-profiling",level:3},{value:"When should I use multiple CPUs instead of a GPU?",id:"when-should-i-use-multiple-cpus-instead-of-a-gpu",level:2},{value:"Narval",id:"narval",level:3},{value:"B\xe9luga",id:"b\xe9luga",level:3},{value:"Cedar",id:"cedar",level:3},{value:"Graham",id:"graham",level:3},{value:"Fourth set of best practices - Use multiple CPUs instead of a GPU",id:"fourth-set-of-best-practices---use-multiple-cpus-instead-of-a-gpu",level:3},{value:"Example 1 - Solving Linear Systems",id:"example-1---solving-linear-systems",level:2},{value:"Example 2 - Linear Regression",id:"example-2---linear-regression",level:2},{value:"Example 3 - Deep Learning",id:"example-3---deep-learning",level:2},{value:"Submitting Jobs",id:"submitting-jobs",level:2},{value:"Using Multiple CPUs",id:"using-multiple-cpus",level:2},{value:"See also",id:"see-also",level:2}];function h(e){const n={a:"a",annotation:"annotation",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"best-practices-cpu-and-gpu",children:"Best practices CPU and GPU"}),"\n",(0,a.jsx)(n.p,{children:"Training given by Calcul Qu\xe9bec on November 30, 2023."}),"\n",(0,a.jsx)(n.p,{children:"Over the last decade, GPUs have become an integral part of the High-Performance Computing (HPC) world. Originally designed to efficiently render images on a screen, it turns out that GPUs can also be used to speed up scientific computing workloads, sometimes by orders of magnitude!"}),"\n",(0,a.jsxs)(n.p,{children:["They are not, however, a silver bullet, in the sense that they will not magically accelerate just ",(0,a.jsx)(n.em,{children:"anything"})," that you throw at them. In many cases, using a GPU may even ",(0,a.jsx)(n.em,{children:"slow down"})," your workload. In other cases, a GPU may achieve pretty unimpressive performance gains over a CPU, sometimes no gain at all. Given that GPUs are ",(0,a.jsx)(n.strong,{children:"much"})," more expensive, and much less numerous than CPUs on the Digital Research Alliance of Canada's HPC Clusters, you would be better off using one or more CPUs instead in all of those cases."]}),"\n",(0,a.jsxs)(n.p,{children:["In general, users should be mindful of not wasting resources on the clusters. Whenever your code requests a GPUs, but does not make a reasonable use of them, you are ",(0,a.jsx)(n.strong,{children:"unnecessarily using up your group's priority"}),", which will impact your colleagues' ability to run their jobs! You might also be preventing other users, whose jobs actually require a GPU, from acessing these much needed resources!"]}),"\n",(0,a.jsx)(n.p,{children:"To avoid all these negative impacts, this workshop will introduce you to a set of best practices you can follow when deciding whether or not you need a GPU. There will be many factors that you will need to consider and we will provide concrete steps for you to reason about them."}),"\n",(0,a.jsx)(n.p,{children:"First, let's spend some time looking into the inner-workings of a GPU. Knowing how they work at a very high level will help you make sense out of our first set of recommendations."}),"\n",(0,a.jsx)(n.h2,{id:"what-is-a-gpu-and-when-should-i-use-one",children:"What is a GPU and when should I use one?"}),"\n",(0,a.jsxs)(n.p,{children:["In this section, we will use Nvidia's ",(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Ampere_(microarchitecture)",children:"Ampere GPU architecture"}),"  as a reference to introduce key concepts that will help you reason about what kinds of workload are best suited to run on GPUs. In general, most of these concepts will also apply more-or-less to other Nvidia GPU architectures and to GPUs designed by other vendors than Nvidia."]}),"\n",(0,a.jsx)(n.p,{children:"So what is a GPU anyway?"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"cpu vs gpu",src:s(98596).Z+"",width:"1920",height:"1080"})}),"\n",(0,a.jsxs)(n.p,{children:["Simply put, a GPU is a ",(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Massively_parallel",children:"Massively Parallel Processor (MPP)"}),". While a CPU in an HPC node will typically have between 16 and 32 ",(0,a.jsx)(n.strong,{children:"compute cores"}),", a modern GPU has thousands of them. On both CPUs and GPUs, each one of these cores is a part of the chip that has the ability to perform a number of floating-point operation (Flop) per clock cycle. On modern CPUs, each core may be able to perform up to 16 Flops at a time. On GPUs, each core can perform one Flop at a time (two if we count fused operations)."]}),"\n",(0,a.jsxs)(n.p,{children:["In the diagram above, the right-hand side represents a portion of a GPU component called a ",(0,a.jsx)(n.strong,{children:"Streaming Multiprocessor (SM)"}),". Each SM is made up of 4 of these. A modern A100 GPU has ",(0,a.jsx)(n.strong,{children:"108 SMs"}),", boasting a total of 8192 cores."]}),"\n",(0,a.jsxs)(n.p,{children:["So can we say that GPUs are ",(0,a.jsx)(n.em,{children:"always faster"})," than CPUs because they can perform several thousands of Flops per clock cycle against a CPU's up to a few hundred?"]}),"\n",(0,a.jsx)(n.p,{children:"No. It is not that simple."}),"\n",(0,a.jsx)(n.p,{children:"It turns out that CPUs and GPUs are not designed for the same kinds of tasks. Each has a set of key characteristics that makes it best suited for different computational workloads."}),"\n",(0,a.jsx)(n.h3,{id:"cpu",children:"CPU"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Key Charcteristics","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Higher Clock Frequency"}),"\n",(0,a.jsx)(n.li,{children:"Low count of general purpose cores"}),"\n",(0,a.jsx)(n.li,{children:"Lower memory bandwidth"}),"\n",(0,a.jsx)(n.li,{children:"Designed to minimize memory access latency"}),"\n",(0,a.jsx)(n.li,{children:"Branching prediction"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["They make CPUs good at:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Executing sequences of instructions that require access to different areas of memory at each step"}),"\n",(0,a.jsx)(n.li,{children:"Switching into different tasks that require different sets of instructions fast"}),"\n",(0,a.jsx)(n.li,{children:"Pipelining unordered sequences of instructions over small amounts of data (Out-of-order Execution)"}),"\n",(0,a.jsx)(n.li,{children:"Executing sequences that branch out into different instructions depending on the input data (If-Else)"}),"\n",(0,a.jsx)(n.li,{children:"Parallelizing individual instructions over very small batches of data at a time (SIMD)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"gpu",children:"GPU"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Key Charcteristics","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Lower clock frequency"}),"\n",(0,a.jsx)(n.li,{children:"High count of specialized cores"}),"\n",(0,a.jsx)(n.li,{children:"Higher memory bandwidth"}),"\n",(0,a.jsx)(n.li,{children:"Designed to maximize amount of data fetched per memory access"}),"\n",(0,a.jsx)(n.li,{children:"No branching prediction"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["They make GPUs good at:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Executing sequences of instructions that re-use the same area of memory at each step"}),"\n",(0,a.jsx)(n.li,{children:"Executing different tasks, that require the same sets of instructions, at the same time."}),"\n",(0,a.jsx)(n.li,{children:"Pipelining ordered sequences of instructions over large amounts of data (In-order Execution)"}),"\n",(0,a.jsx)(n.li,{children:"Executing sequences that do not branch out depending on the input data."}),"\n",(0,a.jsx)(n.li,{children:"Parallelizing individual instructions over relatively large batches of data at a time (SIMT)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"cpu-vs-cpu-bandwitdh",src:s(85912).Z+"",width:"1153",height:"499"})}),"\n",(0,a.jsx)(n.h3,{id:"first-set-of-best-practices---when-to-use-a-gpu",children:"First set of best practices - when to use a GPU"}),"\n",(0,a.jsx)(n.p,{children:"GPUs are the best option when:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"What you are doing requires large volumes of data"}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:'What you are doing can be parallelized over thousands of "workers"'}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Your program does not branch out depending on its inputs"}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Your program re-uses large chunks of data often"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Now let's look at a few pseudo-code examples that illustrate these ideas."}),"\n",(0,a.jsx)(n.h3,{id:"pseudo-code-examples",children:"Pseudo-Code Examples"}),"\n",(0,a.jsx)(n.h4,{id:"example-1---embarassingly-parallel-problem",children:"Example 1 - Embarassingly parallel problem"}),"\n",(0,a.jsx)(n.p,{children:"Show/Hide"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"\nfor i in range(data.size):\n\n    data[i] + constant\n\n"})}),"\n",(0,a.jsx)(n.p,{children:"Is this code best suited to run on a CPU or a GPU?"}),"\n",(0,a.jsx)(n.p,{children:"Answer:"}),"\n",(0,a.jsxs)(n.p,{children:["It depends on ",(0,a.jsx)(n.code,{children:"data.size"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["Assuming both ",(0,a.jsx)(n.code,{children:"data"})," and ",(0,a.jsx)(n.code,{children:"constant"})," are ",(0,a.jsx)(n.strong,{children:"available in the GPU's memory"})," when this program runs, then:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["CPU wins for ",(0,a.jsx)(n.strong,{children:"very small"})," arrays"]}),"\n",(0,a.jsx)(n.li,{children:"GPU wins for any thing larger than a few thousands of elements"}),"\n",(0,a.jsxs)(n.li,{children:["Performance gap increases ",(0,a.jsx)(n.strong,{children:"very quickly"})," as array size grows."]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"example-2---serial--branching-problem",children:"Example 2 - Serial & branching problem"}),"\n",(0,a.jsx)(n.p,{children:"Show/Hide"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"for i in range(1,data.size):\n    data[i] = data[i] + data[i-1] if other_data[i]%2 == 0 else another_data[i] + data[i-1]\n"})}),"\n",(0,a.jsx)(n.p,{children:"Is this code best suited to run on a CPU or a GPU?"}),"\n",(0,a.jsx)(n.p,{children:"Answer:"}),"\n",(0,a.jsxs)(n.p,{children:["The short answer here is ",(0,a.jsx)(n.strong,{children:"CPU"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Why does the CPU win?"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Each step depends on the value of the previous step. This is a serial loop."}),"\n",(0,a.jsxs)(n.li,{children:["Assuming ",(0,a.jsx)(n.code,{children:"other_data"})," and ",(0,a.jsx)(n.code,{children:"another_data"})," are completely disjoint, each step fetches values from different memory regions."]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"example-3---communication-bound-parallel-problem",children:"Example 3 - Communication-bound parallel problem"}),"\n",(0,a.jsx)(n.p,{children:"Show/Hide"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"for i in range(1,data.size):\n\n    data[i] = data[i] + constant\n    other_data[i] = other_data[i] + data[i]\n    more_data[i] = more_data[i] + other_data[i]\n\noutput = sum(data + other_data + another_data)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Is this code best suited to run on a CPU or a GPU?"}),"\n",(0,a.jsxs)(n.p,{children:["As in the first example, it depends on ",(0,a.jsx)(n.code,{children:"data.size"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["Assuming both ",(0,a.jsx)(n.code,{children:"data"}),", ",(0,a.jsx)(n.code,{children:"other_data"})," and ",(0,a.jsx)(n.code,{children:"more_data"})," are ",(0,a.jsx)(n.strong,{children:"available in the GPU's memory"})," when this program runs, then:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["CPU wins with ",(0,a.jsx)(n.strong,{children:"very small"})," arrays"]}),"\n",(0,a.jsx)(n.li,{children:"GPU wins for anything larger than a few thousands of elements."}),"\n",(0,a.jsxs)(n.li,{children:["Performance gap increases ",(0,a.jsx)(n.strong,{children:"very quickly"})," as array size grows."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"gpus-and-data-loading",children:"GPUs and Data Loading"}),"\n",(0,a.jsxs)(n.p,{children:["As previously mentioned, we are using Nvidia's ",(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Ampere_(microarchitecture)",children:"Ampere GPU architecture"})," as a stand-in for generic GPUs in this material. In this architecture, as well as in older Nvidia architectures and comptemporary architectures from other vendors, GPUs and CPUs ",(0,a.jsx)(n.strong,{children:"do not share memory"}),". That is to say, each has its own memory and one cannot access data that is stored on the other's memory. It is also often the case in systems equipped with such GPUs that data cannot be loaded directly from disk to GPU memory. Data must first be loaded via CPU into the system's memory, and then from there it can be loaded into the GPU's memory - an approach known as a ",(0,a.jsx)(n.em,{children:"bounce buffer"}),":"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"cpu-vs-cpu-bandwitdh",src:s(47667).Z+"",width:"559",height:"289"})}),"\n",(0,a.jsx)(n.p,{children:"As it turns out, using a bounce buffer comes with a series of negative consequences in terms of performance, which include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Latency due to multiple read/write operations, leaving the GPU's Streaming Multiprocessors idle while waiting for data from disk."}),"\n",(0,a.jsx)(n.li,{children:"CPU might multi-task between data loading and running other processes."}),"\n",(0,a.jsx)(n.li,{children:"Amount of system memory must be taken into consideration."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["This makes ",(0,a.jsx)(n.strong,{children:"loading data"})," into the GPU's memory a critical factor that influences the performance of a GPU program. Also worth noticing is the fact that the negative consequences apply the other way around too! If you need to copy data from GPU memory to system memory, you will also experience these performance setbacks."]}),"\n",(0,a.jsx)(n.h3,{id:"second-set-of-best-practices---data-loading",children:"Second set of best practices - Data Loading"}),"\n",(0,a.jsx)(n.p,{children:"When designing pipelines to load data into GPU memory, always try to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Load all the data you will need in one go, if possible, before starting your computations."}),"\n",(0,a.jsx)(n.li,{children:"Load the maximum amount of data you can get away with if loading the whole dataset is not an option."}),"\n",(0,a.jsx)(n.li,{children:"Use one or more additional CPUs to perform data loading if you must load data iteratively in batches. Avoid using the main process to load data."}),"\n",(0,a.jsx)(n.li,{children:"Avoid moving data back and forth between GPU and CPU!"}),"\n",(0,a.jsx)(n.li,{children:"Generate random values directly on the GPU if your computatations require using them."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"monitoring--profiling---how-do-i-know-if-im-using-a-gpu-correctly",children:"Monitoring & Profiling - How do I know if I'm using a GPU correctly?"}),"\n",(0,a.jsx)(n.p,{children:"So far we have seen two sets of best practices to keep in mind when reasoning about using GPUs in your computations. They are good pointers in theory, but even if you follow them to the letter it is still possible that things will not go as expected. Thus, the best way to make sure you are getting maximum performance out of a GPU is to monitor its usage and to profile your code."}),"\n",(0,a.jsx)(n.h3,{id:"nvidia-smi",children:"Nvidia-smi"}),"\n",(0,a.jsxs)(n.p,{children:["We will start with monitoring tools. The first thing you should always verify is whether your code is using the GPU at all. The simplest way to answer this question is with the command ",(0,a.jsx)(n.code,{children:"nvidia-smi"}),":"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"cpu-vs-cpu-bandwitdh",src:s(84976).Z+"",width:"778",height:"359"})}),"\n",(0,a.jsxs)(n.p,{children:["As you can see above, ",(0,a.jsx)(n.code,{children:"nvidia-smi"})," will tell you whether or not any processes are currently using the GPU and how much GPU memory they have allocated."]}),"\n",(0,a.jsxs)(n.p,{children:["A common way of misreading the ",(0,a.jsx)(n.code,{children:"utilisation %"})," field is to take it as the amount of compute capacity currently being used. That is ",(0,a.jsx)(n.strong,{children:"not"})," what that number is. Rather, ",(0,a.jsx)(n.code,{children:"utilisation %"})," is the proportion of time where there was ",(0,a.jsx)(n.em,{children:"at least one kernel"})," running on the GPU. A low % means that the GPU is either idle, or busy doing other things than running kernels, such as managing memory or scheduling tasks."]}),"\n",(0,a.jsxs)(n.p,{children:["Ideally, this % stays high for the duration of your job, but ",(0,a.jsx)(n.strong,{children:"that is not sufficient"})," to conclude that your code is not wasting GPU capacity. You could after all have a single kernel running 100% of the time using just a few cores, thereby massively wasting GPU capacity."]}),"\n",(0,a.jsx)(n.h3,{id:"nvtop",children:"Nvtop"}),"\n",(0,a.jsxs)(n.p,{children:["Another way of checking whether or not your code is actually using the GPU is the command ",(0,a.jsx)(n.code,{children:"nvtop"}),". This tool will display the same usage statistics as ",(0,a.jsx)(n.code,{children:"nvidia-smi"})," as a line plot that changes over time. You will also get a breakdown of ",(0,a.jsx)(n.code,{children:"utilisation %"})," per process if you have multiple processes sharing the same GPU:"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"cpu-vs-cpu-bandwitdh",src:s(38765).Z+"",width:"1285",height:"402"})}),"\n",(0,a.jsxs)(n.p,{children:["The line plot is useful to identify peaks and cliffs in ",(0,a.jsx)(n.code,{children:"utilisation %"})," as well as any changes in memory allocation during the execution of your code. This can provide helpful clues for you to optimize your code to minimize idle time for example."]}),"\n",(0,a.jsxs)(n.p,{children:["Another interesting number provided by this tool is the ",(0,a.jsx)(n.strong,{children:"CPU % utilisation"}),". In general, CPU usage should be low in a GPU program. A high CPU utilisation % might indicate your program is not using the GPU optimally as a lot of work is being done by the CPU."]}),"\n",(0,a.jsx)(n.h3,{id:"cluster-portals",children:"Cluster Portals"}),"\n",(0,a.jsx)(n.p,{children:"Next we turn to a tool that stradles the line between monitoring and profiling: the Digital Research Alliance of Canada's cluster portals:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"cpu-vs-cpu-bandwitdh",src:s(15244).Z+"",width:"997",height:"682"})}),"\n",(0,a.jsxs)(n.p,{children:["Above you can see a different set of statistics pertaining to the same code example used to illustrate ",(0,a.jsx)(n.code,{children:"nvidia-smi"})," ",(0,a.jsx)(n.code,{children:"nvtop"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"SM Activity:"})," This is the average % of time over all SMs where there was at least one set of concurrent operations active, no matter how many kernels that happens to be. Note that ",(0,a.jsx)(n.em,{children:"active"})," does not mean ",(0,a.jsx)(n.em,{children:"computing"}),". Other activities such as waiting for data and acessing memory also count here. Ideally this number should stay over 80% for the duration of your job, but that ",(0,a.jsx)(n.strong,{children:"does not necessarily mean"})," you are using 80% of the GPUs compute capacity. It means that either 80% of the time your were using 100% of the capcity and stayed idle 20% of the time, or that you used 80% capacity 100% of the time with 20% of vacant capacity."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"SM Occupancy:"}),"  This is the average, over time, of the ratio of active concurrent operations and the maximum number of concurrent operations supported by the GPU. A high % also ",(0,a.jsx)(n.strong,{children:"does not necessarily mean"})," you are using the GPU effectively. For our purposes however, we will say that this number being high is a strong indicator of good usage."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The other metrics we see above - ",(0,a.jsx)(n.code,{children:"Tensor"}),", ",(0,a.jsx)(n.code,{children:"FP64"}),", ",(0,a.jsx)(n.code,{children:"FP32"})," and ",(0,a.jsx)(n.code,{children:"FP16"})," - indicate what type of GPU core is being used and what % of time these specific parts of the card were active. Of note here is the ",(0,a.jsx)(n.code,{children:"Tensor"})," metric. Tensor cores are specialized cores purpose built for speeding-up n-dimensional tensor operations like multiplications and convolutions. If your code makes heavy use of tensor operations, the Cuda compiler can automatically take advantage of these specialized cores if your program satisfies one of these conditions:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["You use ",(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Mixed-precision_arithmetic",children:"mixed precision"})," in your program."]}),"\n",(0,a.jsxs)(n.li,{children:["You explicitly use ",(0,a.jsx)(n.a,{href:"https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/",children:"Nvidia's TF32 data type"})," in your program."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"And additionally:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"In a matrix multiplication, all your matrices' dimensions are multiples of 8."}),"\n",(0,a.jsx)(n.li,{children:"In a convolution, the number of channels and the width are all multiples of 8."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"A high % usage of Tensor cores should correspond to good speed-ups relative to other GPU core types. One caveat is that the first two conditions require sacrificing precision in computations, so you have to be mindful of whether or not high precision is important for your application."}),"\n",(0,a.jsx)(n.p,{children:"Apart from the GPU metrics depicted above, you can also monitor a number of other job statistics on the Alliance's cluster portals such as: CPU and system memory usage; network metrics; IO metrics, disk usage and more."}),"\n",(0,a.jsx)(n.p,{children:"The alliance currently maintains cluster portals for the following clusters:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://portail.beluga.calculquebec.ca",children:"B\xe9luga"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://portail.narval.calculquebec.ca",children:"Narval"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://dashboard.graham.sharcnet.ca",children:"Graham"})}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"third-set-of-best-practices---monitoring-and-profiling",children:"Third set of best practices - Monitoring and Profiling"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Always do a dry run of your job before requesting a GPU for long durations."}),"\n",(0,a.jsx)(n.li,{children:"Make sure your program is even using a GPU in the first place."}),"\n",(0,a.jsx)(n.li,{children:"Make sure your program is able to keep the GPU busy for most of the duration of your job."}),"\n",(0,a.jsxs)(n.li,{children:["Keep ",(0,a.jsx)(n.strong,{children:"SM Activity"})," above 80% most of the time."]}),"\n",(0,a.jsxs)(n.li,{children:["Keep ",(0,a.jsx)(n.strong,{children:"SM Occupancy"})," above 50% most of the time."]}),"\n",(0,a.jsx)(n.li,{children:"Use Tensor cores when applicable."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"when-should-i-use-multiple-cpus-instead-of-a-gpu",children:"When should I use multiple CPUs instead of a GPU?"}),"\n",(0,a.jsx)(n.p,{children:"Let's assume you have a program that fits the bill of the first and second sets of best practices, but you can't for the love of all that is good get it to keep the GPU consistently busy. Maybe you have bursts of high usage followed by long periods of no usage at all. Maybe you can't get activity and/or occupancy above 50%."}),"\n",(0,a.jsx)(n.p,{children:"In all these cases, a GPU still beats a single CPU hands down... but how does it compare to multiple CPUs?"}),"\n",(0,a.jsx)(n.p,{children:"While CPUs are not purpose-built to handle the type and scale of operations that a GPU does, they are still pretty good at it - especially when you can throw lots of them at the problem. In many cases, like linear algebra operators and fourier transforms, highly optimized parallel implementations of these operations for multiple CPUs have been around for ages."}),"\n",(0,a.jsx)(n.p,{children:"Furthermore, unless you are writing Cuda code directly, it is often possible to run the exact same code without any changes on a GPU or one/many CPUs."}),"\n",(0,a.jsxs)(n.p,{children:["This, coupled with the fact that there are ",(0,a.jsx)(n.strong,{children:"many, many, many"})," more CPUs available in the Alliance's clusters than there are GPUs, make using multiple CPUs a great alternative."]}),"\n",(0,a.jsx)(n.h3,{id:"narval",children:"Narval"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"CPUs: 75584"}),"\n",(0,a.jsx)(n.li,{children:"GPUs: 636"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"b\xe9luga",children:"B\xe9luga"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"CPUs: 32080"}),"\n",(0,a.jsx)(n.li,{children:"GPUs: 688"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"cedar",children:"Cedar"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"CPUs: 90752"}),"\n",(0,a.jsx)(n.li,{children:"GPUs: 1352"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"graham",children:"Graham"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"CPUs: 37664"}),"\n",(0,a.jsx)(n.li,{children:"GPUs: 536"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["So even in cases where you ",(0,a.jsx)(n.strong,{children:"can"})," make an effective usage of a GPU, you might get, say, 16 CPUs allocated to you much faster and possibly beat the GPU's performance depending on the nature of your program!"]}),"\n",(0,a.jsx)(n.h3,{id:"fourth-set-of-best-practices---use-multiple-cpus-instead-of-a-gpu",children:"Fourth set of best practices - Use multiple CPUs instead of a GPU"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"When dry-running your GPU code, dry run it with multiple CPUs too. See if there is really an advantage to using a GPU."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Now let's go through some hands-on example to put into practice what we've learned so far!"}),"\n",(0,a.jsx)(n.h2,{id:"example-1---solving-linear-systems",children:"Example 1 - Solving Linear Systems"}),"\n",(0,a.jsxs)(n.p,{children:["In this example, you will simultaneously solve N systems of equations of the type ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mi,{children:"A"}),(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsx)(n.mo,{children:"="}),(0,a.jsx)(n.mi,{children:"b"})]}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"Ax=b"})]})})}),(0,a.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"A"}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(n.span,{className:"mrel",children:"="}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6944em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"b"})]})]})]})," where ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"A"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"A"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"A"})]})})]})," is a ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mi,{children:"M"}),(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsx)(n.mi,{children:"M"})]}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"MxM"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"})]})})]})," matrix and ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"B"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"B"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.05017em"},children:"B"})]})})]})," is ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mi,{children:"M"}),(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsx)(n.mn,{children:"1"})]}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"Mx1"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"mord",children:"1"})]})})]})]}),"\n",(0,a.jsxs)(n.p,{children:["You will be given two functions: one to generate random matrices and one to solve the systems of equations. You will use the magic function ",(0,a.jsx)(n.code,{children:"%timeit"})," to compare the running times for these functions with different combinations of the following parameters:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Using CPU or GPU"}),"\n",(0,a.jsx)(n.li,{children:"The number of CPUs to use"}),"\n",(0,a.jsx)(n.li,{children:"The number and size of the systems of equations to be solved"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"You should be able to see how these parameters affect performance when using CPUs or a GPU to solve the systems."}),"\n",(0,a.jsxs)(n.p,{children:["For the CPU part of teh example, you will use ",(0,a.jsx)(n.code,{children:"numpy"})," - a Python library that contains optimized implementations of a myriad of numerical operations. For the GPU portion, you will use ",(0,a.jsx)(n.code,{children:"cupy"})," - a library that aims to be an analogous of ",(0,a.jsx)(n.code,{children:"numpy"})," but for GPUs."]}),"\n",(0,a.jsxs)(n.p,{children:["On the first cell, you will import all libraries needed for this example and you will choose the number of CPUs that should be used for your computations. The environment variable ",(0,a.jsx)(n.code,{children:"OMP_NUM_THREADS"})," controls how many parallel threads should be spawned by routines written using OpenMP - an API for parallel programming. As it turns out, the low level C code that ",(0,a.jsx)(n.code,{children:"numpy"})," calls to solve linear systems of equations and to generate random numbers are both examples of routines written using OpenMP!"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\nimport timeit\n\nN_CPUS=10\n\nos.environ["OMP_NUM_THREADS"]= str(N_CPUS)\n\nimport numpy as np\nimport cupy as cp\n'})}),"\n",(0,a.jsx)(n.p,{children:"Next you will define functions to generate random data and to solve linear systems:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def generate_data(n_systems,n_rows,n_cols,device):\n    lib = cp if device == "gpu" else np\n\n    rmg = lib.random.rand #random matrix generator\n    a = rmg(n_systems,n_rows,n_cols)\n    b = rmg(n_systems,n_rows,1)\n    return a,b\n\ndef multiple_solve(a,b):\n    lib = cp if type(a) == cp.ndarray else np\n\n    results = lib.linalg.solve(a,b)\n    return results\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Now try changing both the number and size of systems, as well as the device used to generate the data. The magic function ",(0,a.jsx)(n.code,{children:"%timeit"})," will return the average run time of ",(0,a.jsx)(n.code,{children:"generate_data"})," over several repetitions:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'%timeit generate_data(n_systems=1, n_rows=10000, n_cols=10000, device="gpu")\n# 3.63 ms \xb1 12.7 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1,000 loops each)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'%timeit generate_data(n_systems=1, n_rows=10000, n_cols=10000, device="cpu")\n# 884 ms \xb1 21.5 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["You have now got a sense of the time it takes to generate random systems of equations on both CPU and GPU. Now let's store some random data in a variable so we can use it to time the execution time of our second function: ",(0,a.jsx)(n.code,{children:"multiple_solve"}),". Try it out with different numbers and sizes of systems on both the CPU and the GPU. Notice how the difference in performance changes as the size of the problem grows."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'a,b = generate_data(1,10000,10000,"gpu")\n%timeit multiple_solve(a,b)\n# 257 ms \xb1 44.4 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'a,b = generate_data(1,10000,10000,"cpu")\n%timeit multiple_solve(a,b)\n# 3.71 s \xb1 578 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Last, but not least, let's see what happens when we generate data using the CPU, then copy the data to GPU before calling ",(0,a.jsx)(n.code,{children:"multiple solve"}),". This should illustrate the benefits of generating random numbers directly on the GPU, instead of using a ",(0,a.jsx)(n.em,{children:"bounce buffer"})," to load numbers generated on the CPU:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'%%timeit\nc,d = generate_data(10,10,10,"cpu")\nc_gpu, d_gpu = (cp.asarray(c),cp.asarray(d)) # This will load c and d into the GPU\'s memory\nmultiple_solve(c_gpu,d_gpu,compute_batched=False)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"example-2---linear-regression",children:"Example 2 - Linear Regression"}),"\n",(0,a.jsxs)(n.p,{children:["In this next example, you will use ",(0,a.jsx)(n.code,{children:"JAX"})," to solve a linear regression problem. ",(0,a.jsx)(n.code,{children:"JAX"})," is a numerical computing library that, similarly to ",(0,a.jsx)(n.code,{children:"cupy"}),", aims to bring the ",(0,a.jsx)(n.code,{children:"numpy"})," user experience to the world of GPUs. Unlike ",(0,a.jsx)(n.code,{children:"cupy"})," however, JAX is geared towards Machine Learning research, and so it has many useful built-in features to fit models at scale - most notably ",(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Automatic_differentiation",children:"automatic differentiation"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Load cuda/11.4 to use JAX\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom matplotlib import pyplot as plt\n"})}),"\n",(0,a.jsx)(n.p,{children:"First, you will define a function to generate random data for our linear regression problem. The function below takes in a mathematical function - preferably a linear function - and the number of desired samples as arguments. It then generates the desired number of samples by adding random noise to randomly chosen points along the line drawn by the mathematical function."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def generate_regression_data(function,n_samples):\n\n    samples = [(x,function(x) + np.random.normal(0,1) * 0.1) for x in np.sort(np.random.uniform(0,1,n_samples))]\n    X, y = zip(*samples)\n\n    X = np.array(X,dtype=jnp.float32) #Change the numpy array's data type into a JAX data type\n    X = np.c_[X,np.ones(X.shape[0])]\n    y = np.array(y,dtype=jnp.float32)\n    return X,y\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Let's start by generating 50 noisy samples from the function ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mi,{children:"y"}),(0,a.jsx)(n.mo,{children:"="}),(0,a.jsx)(n.mn,{children:"2"}),(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsx)(n.mo,{children:"+"}),(0,a.jsx)(n.mn,{children:"5"})]}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"y = 2x + 5"})]})})}),(0,a.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(n.span,{className:"mrel",children:"="}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,a.jsx)(n.span,{className:"mord",children:"2"}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(n.span,{className:"mbin",children:"+"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6444em"}}),(0,a.jsx)(n.span,{className:"mord",children:"5"})]})]})]}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"f = lambda x: 2 * x + 5\nN_SAMPLES=50\n\nX,y = generate_regression_data(f,N_SAMPLES)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["In our linear regression problem, you will be given only the blue dots and will attempt to recover the red line. In other words, given some data points (the blue dots), you will run an algorithm to ",(0,a.jsx)(n.strong,{children:"learn"})," the function that best describes the data (the red line):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"X_target = np.linspace(0,1,50).reshape(-1,1)\n\nplt.scatter(X[:,0],y,label='Samples')\nplt.plot(X_target,f(X_target),color='r',label='Target')\n\nplt.legend(loc='lower right')\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"s01",src:s(91386).Z+"",width:"1136",height:"826"})}),"\n",(0,a.jsxs)(n.p,{children:["You will solve the linear regression using the iterative ",(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Least_squares",children:"least-squares approach"}),". In the cell below, you are given a set of functions that implement this approach. In a nutshell - you will use ",(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Gradient_descent",children:"gradient descent"})," for ",(0,a.jsx)(n.code,{children:"n_iterations"})," in an attempt to minimize the square of the distance between the blue dots and a linear function, that is initialized as ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mi,{children:"y"}),(0,a.jsx)(n.mo,{children:"="}),(0,a.jsx)(n.mn,{children:"0"}),(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsx)(n.mo,{children:"+"}),(0,a.jsx)(n.mn,{children:"0"})]}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"y = 0x + 0"})]})})}),(0,a.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(n.span,{className:"mrel",children:"="}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,a.jsx)(n.span,{className:"mord",children:"0"}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(n.span,{className:"mbin",children:"+"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6444em"}}),(0,a.jsx)(n.span,{className:"mord",children:"0"})]})]})]}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Notice how this is re-using the same dataset and how it is applying the same operations during each iteration over and over again? And think about the mathematical operations used in the function below - can they be computed in a parallelized manner?"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Matrix multiplication of X and beta"}),"\n",(0,a.jsx)(n.li,{children:"Element-wise square of the element-wise difference between two vectors"}),"\n",(0,a.jsx)(n.li,{children:"Mean of a vector"}),"\n",(0,a.jsx)(n.li,{children:"A vector minus another vector multiplied by a constant"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def model(beta, X):\n    return (X @ beta).T\n\ndef loss(beta, X, y):\n    return jnp.mean((model(beta, X) - y)**2)\n\n@jax.jit\ndef update(beta, X, y, learning_rate):\n    return beta - learning_rate * jax.grad(loss)(beta, X, y)\n\ndef fit_model(X, y, n_iterations, learning_rate=0.1):\n    beta = jnp.zeros([X.shape[1],1])\n    if y.ndim > 1:\n        y = y.reshape(-1)\n    for i in range(n_iterations):\n        beta = update(beta, X, y, learning_rate)\n\n    return beta\n"})}),"\n",(0,a.jsxs)(n.p,{children:["After ",(0,a.jsx)(n.code,{children:"n_iterations"})," you should see the coefficients ",(0,a.jsx)(n.code,{children:"beta"})," of the linear function converge to 2 and 5 - the true values of these coefficients that you had set when you first generated this random dataset:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'beta_fit = fit_model(X, y, 10000)\nprint(f"Estimated f(x) = {beta_fit[0]} * X + {beta_fit[1]}")\n'})}),"\n",(0,a.jsxs)(n.p,{children:["If you plot this ",(0,a.jsx)(n.strong,{children:"learned function"}),", you will see that it is pretty close to that red line we had on the plot a couple cells above:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"plt.scatter(X[:,0],y,label='Samples')\nplt.plot(X_target, np.dot(np.c_[X_target, np.ones(X.shape[0])],beta_fit), color='g',label=\"Linear Fit\")\nplt.legend(loc=\"lower right\")\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"s02",src:s(26606).Z+"",width:"1104",height:"836"})}),"\n",(0,a.jsxs)(n.p,{children:["Now let's see what is the effect on performance when we increase the number of samples. First you will time the execution of ",(0,a.jsx)(n.code,{children:"fit_model()"})," using the CPU. What happens to the execution time when the number of samples grow?"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'N_SAMPLES=10000\nX,y = generate_regression_data(f,N_SAMPLES) #Try this with different values of N_SAMPLES like: 10000, 50000, 100000, 1000000...\n\nwith jax.default_device(jax.devices("cpu")[0]):\n    %timeit fit_model(X, y, N_SAMPLES).block_until_ready()\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-txt",children:"CPU\n 1000: 16.6 ms \xb1 1.01 ms per loop (mean \xb1 std. dev. of 7 runs, 100 loops each)\n10000: 591 ms \xb1 27.5 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n\nGPU\n 1000: 161 ms \xb1 16.2 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n 2.02 s \xb1 104 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Now let's see how long it takes to load data into the GPU using ",(0,a.jsx)(n.code,{children:"JAX"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"%timeit jax.device_put(X)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"%timeit jax.device_put(y)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Finally, let's compare the execution time we obtained for the CPU with the execution time of the same function on the GPU. What can you say about the difference in performance as the number of samples gorw?"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'N_SAMPLES=10000\nX,y = generate_regression_data(f,N_SAMPLES) #Try this with different values of N_SAMPLES like: 10000, 50000, 100000, 1000000...\n\nwith jax.default_device(jax.devices("gpu")[0]):\n    X_gpu = jax.device_put(X)\n    y_gpu = jax.device_put(y)\n\n    %timeit fit_model(X_gpu, y_gpu, N_SAMPLES).block_until_ready()\n\n# CPU: 3.05 s \xb1 149 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n# GPU: 796 ms \xb1 81.6 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["So far you have solved a single-variable linear regression problem, that is, one where the matrix ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"X"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"X"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"X"})]})})]})," has dimensions ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mi,{children:"N"}),(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsx)(n.mn,{children:"2"})]}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"Nx2"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"mord",children:"2"})]})})]})," where ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"N"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"N"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"})]})})]})," is the number of samples and the second column of ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"X"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"X"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"X"})]})})]})," is filled with 1's. As it turns out, the one operation where you use ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"X"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"X"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"X"})]})})]})," in ",(0,a.jsx)(n.code,{children:"fit_model"})," is a matrix multiplication, an operation that GPUs are able to massively parallelize. What do you think will happen if you add more columns to ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"X"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"X"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"X"})]})})]}),"? In other words, what will be the difference in performance between GPU an CPU in a ",(0,a.jsx)(n.strong,{children:"multivariate linear regression problem"}),"?"]}),"\n",(0,a.jsxs)(n.p,{children:["First you will use a slightly altered function to generate multivariate data (i.e, ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"X"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"X"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"X"})]})})]})," with more than 2 columns). This time, the coefficients ",(0,a.jsx)(n.code,{children:"beta"})," of the linear function used to generate the data will be chosen randomly:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def generate_data_multi_regression(n_samples,n_variables):\n\n    beta = np.random.randint(1,10,(n_variables + 1,1))\n\n    f = lambda x: np.dot(x,beta)\n\n    X = np.array(np.random.rand(n_samples,n_variables))\n    X = np.c_[X,np.ones(X.shape[0])]\n\n    y = f(X) + np.random.normal(0,1,(n_samples,1))\n\n    X = np.array(X,dtype=jnp.float32)\n    y = np.array(y,dtype=jnp.float32)\n\n    return X,y\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'N_SAMPLES=10000\nN_VARS=50\n\nX,y = generate_data_multi_regression(N_SAMPLES, N_VARS)#Try this with different values of N_SAMPLES and N_VARS\n\nwith jax.default_device(jax.devices("cpu")[0]):\n    %timeit fit_model(X, y, N_SAMPLES, learning_rate=0.001).block_until_ready()\n# CPU: 2.13 s \xb1 91.7 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'with jax.default_device(jax.devices("gpu")[0]):\n    X_gpu = jax.device_put(X)\n    y_gpu = jax.device_put(y)\n\n    %timeit fit_model(X_gpu, y_gpu, N_SAMPLES, learning_rate=0.001).block_until_ready()\n# GPU: 970 ms \xb1 64.7 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"example-3---deep-learning",children:"Example 3 - Deep Learning"}),"\n",(0,a.jsxs)(n.p,{children:["In this final example you will scale up the ideas from the previous example to fit a more complicated model to a dataset. This time you will use ",(0,a.jsx)(n.code,{children:"PyTorch"})," to fit a ",(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Convolutional_neural_network",children:"Convolutional Neural Network"})," to the CIFAR10 dataset - a collection of 60000 images representing 10 different animals and transportation methods."]}),"\n",(0,a.jsxs)(n.p,{children:["In the linear regression problem, the model being fit to the data was a linear function, which entailed performing two matrix multiplications per iteration of the least squares method. The convolutional neural network you will work with in this example consists of a sequence of two convolutions, followed by 400 concurrent matrix multiplications, then 120 concurrent matrix multiplications and finally 10 concurrent matrix multiplications. Both convolutions and matrix multiplications are operations that can be parallelized. There are also other operations in between each of these convolution and matrix multiplciation ",(0,a.jsx)(n.em,{children:"layers"}),", which can all be parallelized:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import CIFAR10\nfrom torch.utils.data import DataLoader\n\ntorch.set_num_threads(1)\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net().cuda() # Load model on the GPU\n\ncriterion = nn.CrossEntropyLoss().cuda() # Load the loss function on the GPU\noptimizer = optim.SGD(net.parameters(), lr=0.001)\n\ntransform_train = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"})}),"\n",(0,a.jsx)(n.p,{children:"Purely from the perspective of parallelizing a large number of computations, this looks like something that could benefit from a GPU. But, as you've seen in the previous example, another important part of deciding whether or not this should run on a GPU is the amount of data used in these computations."}),"\n",(0,a.jsxs)(n.p,{children:["In Deep Learning, it is a common practice to use a variant of the gradient descent method called ",(0,a.jsx)(n.a,{href:"https://stats.stackexchange.com/questions/488017/understanding-mini-batch-gradient-descent",children:"mini-batch gradient descent"}),". Instead of using the entire matrix of inputs ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"X"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"X"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"X"})]})})]})," at each iteration of the algorithm, you use a small subset of the rows of ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsx)(n.mi,{children:"X"})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"X"})]})})}),(0,a.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"X"})]})})]}),". The amount of rows that make up this subset, called the ",(0,a.jsx)(n.strong,{children:"batch size"}),", is an important factor that affects whether or not as well as how quickly this approach converges."]}),"\n",(0,a.jsx)(n.p,{children:"Next, you will use mini-batch gradient descent to fit our convolutional neural network to the CIFAR10 data. You will use the code below to load one batch of images at a time from disk, then time the execution of 3 iterations of this training algorithm. Try it out with different combinations of the three parameters below and see what the impact is on execution time and on GPU usage:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"BATCH_SIZE"}),": this is the mini-batch gradient descent batch size, i.e., how many images should be loaded from disk and used by the trainig algorith at a time."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"NUM_WORKERS"}),": this controls how many CPUs will be used to load batches of images ",(0,a.jsx)(n.em,{children:"in parallel"}),". The idea is to use multiple processes, with one CPU each, to streamline loading images from disk ",(0,a.jsx)(n.em,{children:"without"})," blocking the main process."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"PRE_FETCH"}),": this controls how many batches of images should be loaded from disk ",(0,a.jsx)(n.em,{children:"before"})," they are needed by the training algorithm. These batches will be stored in the system's memory and loaded on the GPU once its their turn to be used."]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'BATCH_SIZE = 4096\nNUM_WORKERS = 10\nPRE_FETCH = 2\n\ndatadir = "./data" #f"{os.getenv(\'SLURM_TMPDIR\')}/data"\ndataset_train = CIFAR10(root=datadir, train=True, download=False, transform=transform_train)\n\ntrain_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, prefetch_factor=PRE_FETCH)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def train(model, n_iterations, data):\n\n    for iteration in range(n_iterations):\n        for batch_idx, (inputs, targets) in data:\n\n            inputs = inputs.cuda()\n            targets = targets.cuda()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n%timeit train(net,n_iterations=3, data=enumerate(train_loader))\n#     2.24 s \xb1 84.6 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["In the example above, we used the ",(0,a.jsx)(n.code,{children:"DataLoader"})," object to iretatively read bacthes of images from disk, which means 2 sets of read/write operations: ",(0,a.jsx)(n.code,{children:"disk -> system memory"})," and ",(0,a.jsx)(n.code,{children:"system memory -> GPU memory"}),". The ",(0,a.jsx)(n.code,{children:"disk -> system memory"})," step is prone to causing bottlenecks, as reading from disk is usually much slower than reading from memory. What would the difference in performance be if we instead loaded the whole dataset to the system memory before starting our computations?"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"cifar10 = [(batch_idx,(inputs,targets)) for batch_idx, (inputs, targets) in enumerate(train_loader)]\n\n%timeit train(net,n_iterations=3, data=cifar10)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The convolution neural network you used above was, relatively speaking, not that big. If you look at how much GPU memory is used when that model is loaded, you will see it is somewhere between 1 and 2 GB. There is more than enough space left to load the CIFAR10 dataset. Could we have sped-up the training algorithm even more simply by loading all images in GPU memory before calling ",(0,a.jsx)(n.code,{children:"train()"}),"?"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"cifar10.cuda()\n\n%timeit train(net,n_iterations=3, data=cifar10)\n#     358 ms \xb1 825 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now let's try this out with a much larger convolutional neural network - the ResNet152 model, which takes up about 20GB in memory and could thus justify keeping the dataset on disk instead of loading it on the GPU:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from torchvision.models import resnet152\n\ntorch.cuda.empty_cache()\n\nresnet = resnet152().cuda()\ncriterion = nn.CrossEntropyLoss().cuda() # Load the loss function on the GPU\noptimizer = optim.SGD(net.parameters(), lr=0.001)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"%timeit train(resnet,n_iterations=3,data=enumerate(train_loader))\n#     7.15 s \xb1 68.2 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"submitting-jobs",children:"Submitting Jobs"}),"\n",(0,a.jsxs)(n.p,{children:["So far we've been executing our code examples inside a Jupyter notebook. In real life, we encourage you to do so as a means of verifying that your code is able to use a GPU efficiently. In other words, Jupyter notebooks are a great option to ",(0,a.jsx)(n.strong,{children:"test your code"})," and keep an eye on resource usage as it runs."]}),"\n",(0,a.jsxs)(n.p,{children:["Using Jupyter notebooks, however, is ",(0,a.jsx)(n.strong,{children:"not recommended"})," on the Alliance's clusters if you do not need to run your code interactively. If your plan is to run a large workload that will run for a long duration without any intervention on your part, you should ",(0,a.jsx)(n.strong,{children:"submit a batch job"})," instead."]}),"\n",(0,a.jsxs)(n.p,{children:["Here is an example of a ",(0,a.jsx)(n.strong,{children:"job submission script"})," for a job that requires a GPU:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n\n#SBATCH ntasks=1\n#SBATCH gres=gpu:1\n#SBATCH cpus-per-task=4\n#SBATCH mem=8000M\n#SBATCH account=def-myself_gpu\n\n## YOUR JOB'S COMMANDS GO HERE ##\n"})}),"\n",(0,a.jsx)(n.p,{children:"And here is an example of a job that does not requires multiple CPUs, but no GPU:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n\n#SBATCH ntasks=1\n#SBATCH cpus-per-task=16\n#SBATCH mem=32000M\n#SBATCH account=def-myself_cpu\n\n## YOUR JOB'S COMMANDS GO HERE ##\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Important note:"})," the parameter ",(0,a.jsx)(n.code,{children:"--mem"})," controls the amount of ",(0,a.jsx)(n.strong,{children:"system memory"})," your job is requesting. The amount of GPU memory you will get is fixed, ad depends on the type of GPU allocated to your job:"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"GPU Types",src:s(91819).Z+"",width:"1198",height:"383"})}),"\n",(0,a.jsxs)(n.p,{children:['On Clusters where there\'s more than one type of GPU available, you can specify the one you want by adding the "Slurm type specifier" (third column on the image above) to the parameter ',(0,a.jsx)(n.code,{children:"--gres=gpu"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["For example, to specify that you want one V100 GPU with 32GB of memory on Cedar, the parameter should be set as ",(0,a.jsx)(n.code,{children:"--gres=gpu:v100l:1"})]}),"\n",(0,a.jsx)(n.h2,{id:"using-multiple-cpus",children:"Using Multiple CPUs"}),"\n",(0,a.jsxs)(n.p,{children:["In all code examples in this notebook, you experimented with using multiple CPUs for different purposes. On the first two cases, we compared execution performance on GPU versus an increasing number of CPUs. On the third, we used the ",(0,a.jsx)(n.code,{children:"num_workers"})," parameter of the ",(0,a.jsx)(n.code,{children:"DataLoader"}),' object to tell our program to use multiple CPUs and multiple processes to load data from disk. In all these cases, we had a variable in our code where we "hardcoded" the desired number of CPUs.']}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"That is not the recommended way to this when submitting jobs."})}),"\n",(0,a.jsxs)(n.p,{children:["Instead, you should use a special environment variable that the cluster scheduler creates for you when you submit a job: ",(0,a.jsx)(n.code,{children:"SLURM_CPUS_PER_TASK"}),". This variable will be set to the same value you pass to the parameter ",(0,a.jsx)(n.code,{children:"--cpus-per-task"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["On the first example, setting the variable ",(0,a.jsx)(n.code,{children:"OMP_NUM_THREADS"})," would look like this:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'N_CPUS = os.environ["SLURM_CPUS_PER_TASK"]\n\nos.environ["OMP_NUM_THREADS"] = N_CPUS\n'})}),"\n",(0,a.jsxs)(n.p,{children:["On the third example, we would limit the number of threads that ",(0,a.jsx)(n.code,{children:"torch"})," can spawn with:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'N_CPUS = os.environ["SLURM_CPUS_PER_TASK"]\ntorch.set_num_threads(N_CPUS)\n'})}),"\n",(0,a.jsx)(n.p,{children:"Why is this important?"}),"\n",(0,a.jsxs)(n.p,{children:["As it turns out, many widely used Python libraries, including ",(0,a.jsx)(n.code,{children:"torch"})," and ",(0,a.jsx)(n.code,{children:"jax"}),", ",(0,a.jsx)(n.code,{children:"tensorflow"})," will by default try to spawn a number of threads equal to the number of CPUs physically installed on the machine where the code is running. As we've seen above, that number will be somwehere between 32 and 64 depending on the cluster you choose."]}),"\n",(0,a.jsxs)(n.p,{children:["That means that even if you set ",(0,a.jsx)(n.code,{children:"--cpus-per-task=4"}),", for example, these libraries would still try to spawn 32 - 64 threads. This would result in a scenario called ",(0,a.jsx)(n.strong,{children:"core oversubscription"}),", or, in other words, too many threads per CPU. This typically causes code execution to ",(0,a.jsx)(n.strong,{children:"slow down"})," significantly, sometimes even freezing it completely. It is thus important to have exactly one thread per CPU."]}),"\n",(0,a.jsx)(n.p,{children:"This wraps up our discussion of the best practices when reasoning about using CPUs or a GPU. We hope this material will prove useful to you in designing your workloads in the future!"}),"\n",(0,a.jsx)(n.h2,{id:"see-also",children:"See also"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"https://github.com/ComputeCanada/magic_castle",children:"Magic Castle"})," de Compute Canada"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"https://github.com/calculquebec/",children:"GutHub"})," de Calcul Qu\xe9bec"]}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://calculquebec.github.io/cip201-serveurs-calcul/2-ressources.html",children:"Bien choisir les ressources"})}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"https://www.calculquebec.ca/services-aux-chercheurs/formation/",children:"Formations"})," de Calcul Qu\xe9bec"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"https://calculquebec.eventbrite.ca/",children:"Activit\xe9s"})," de Calcul Qu\xe9bec sur eventbrite"]}),"\n",(0,a.jsxs)(n.li,{children:["Portails utilisateur de ",(0,a.jsx)(n.a,{href:"https://portail.narval.calculquebec.ca/",children:"Narval"})," et ",(0,a.jsx)(n.a,{href:"https://portail.beluga.calculquebec.ca/",children:"Beluga"})]}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Bfloat16_floating-point_format",children:"bfloat16 floating-point format"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/",children:"TensorFloat-32 in the A100 GPU Accelerates AI Training, HPC up to 20x"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},47667:(e,n,s)=>{s.d(n,{Z:()=>a});const a=s.p+"assets/images/bounce_buffer-7e19713f9d381783430ab02a97e0c5a7.png"},85912:(e,n,s)=>{s.d(n,{Z:()=>a});const a=s.p+"assets/images/cpu-vs-gpu-bdwidth-5fc1d7419134b79c84e0823fab2afcf8.png"},98596:(e,n,s)=>{s.d(n,{Z:()=>a});const a=s.p+"assets/images/cpu-vs-gpu.001-0531136527a3b7b26b70f82672d3260f.png"},91819:(e,n,s)=>{s.d(n,{Z:()=>a});const a=s.p+"assets/images/gpu_types-e1b4672714ecb8276fb939c0aa7d6158.png"},84976:(e,n,s)=>{s.d(n,{Z:()=>a});const a=s.p+"assets/images/nvidia-smi-bb2d4e72a28842df6ff3b30f41a155b0.png"},38765:(e,n,s)=>{s.d(n,{Z:()=>a});const a=s.p+"assets/images/nvtop-2e1bea267464707ea292db96321596af.png"},15244:(e,n,s)=>{s.d(n,{Z:()=>a});const a=s.p+"assets/images/portal-7526d8307c3e5fc8015ff90824a0d69b.png"},91386:(e,n,s)=>{s.d(n,{Z:()=>a});const a=s.p+"assets/images/s01-c34cdee85863f5ea42be88f596d30e66.png"},26606:(e,n,s)=>{s.d(n,{Z:()=>a});const a=s.p+"assets/images/s02-6e17dae2382037c97bbeea620b17452c.png"},11151:(e,n,s)=>{s.d(n,{Z:()=>o,a:()=>r});var a=s(67294);const t={},i=a.createContext(t);function r(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);