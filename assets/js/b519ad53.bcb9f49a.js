"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[4974],{33554:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>d,contentTitle:()=>o,default:()=>m,frontMatter:()=>l,metadata:()=>a,toc:()=>c});var i=n(85893),r=n(11151),t=n(2828);const l={sidebar_label:"Examen de mi-session",sidebar_position:91},o="Examen de mi-session",a={id:"courses/university/ift-7022/exam-intra",title:"Examen de mi-session",description:"- Quand? le 17 oct. 2023 de 18h30 \xe0 21h45",source:"@site/docs/courses/university/ift-7022/exam-intra.mdx",sourceDirName:"courses/university/ift-7022",slug:"/courses/university/ift-7022/exam-intra",permalink:"/docs/courses/university/ift-7022/exam-intra",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/university/ift-7022/exam-intra.mdx",tags:[],version:"current",sidebarPosition:91,frontMatter:{sidebar_label:"Examen de mi-session",sidebar_position:91},sidebar:"tutorialSidebar",previous:{title:"Travail pratique 3",permalink:"/docs/courses/university/ift-7022/travail-3"},next:{title:"Examen final",permalink:"/docs/courses/university/ift-7022/exam-final"}},d={},c=[{value:"Directives",id:"directives",level:2},{value:"Questions",id:"questions",level:2}];function u(e){const s={a:"a",em:"em",h1:"h1",h2:"h2",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.h1,{id:"examen-de-mi-session",children:"Examen de mi-session"}),"\n","\n","\n",(0,i.jsxs)(t.Z,{children:[(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Quand? le 17 oct. 2023 de 18h30 \xe0 21h45"}),"\n",(0,i.jsx)(s.li,{children:"Dur\xe9e : 3 h 10 min"}),"\n"]}),(0,i.jsx)(s.h2,{id:"directives",children:"Directives"}),(0,i.jsx)(s.p,{children:"Cet examen \xe9valuera votre compr\xe9hension de la mati\xe8re des 6 premi\xe8res semaines du cours.\nL'examen sera effectu\xe9 \xe0 l'aide d'un questionnaire sur MonPortail durant la plage horaire du cours\n(mardi soir de 18h30 \xe0 21h20). Donc la pr\xe9sence des \xe9tudiants sur le campus universitaire n'est pas requise."}),(0,i.jsx)(s.p,{children:"Toute documentation est permise pour l'examen (par ex. livre du cours, notes de cours, capsules vid\xe9o).\nCependant veuillez noter que le temps est limit\xe9 et qu'il faut un bon niveau de compr\xe9hension de la mati\xe8re\npour compl\xe9ter l'examen durant la p\xe9riode allou\xe9e. Veuillez \xe9galement noter que les retours arri\xe8re sur\nles sections de l'examen sont permis et que la composition des examens est al\xe9atoire (N questions parmi\nles M que nous avons pr\xe9par\xe9es)."}),(0,i.jsx)(s.p,{children:"Le contenu de l'examen porte sur les th\xe8mes suivants (\xe0 confirmer apr\xe8s le cours du 10 octobre) :"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Expressions r\xe9guli\xe8res (ch. 2)","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"regular expressions 101"}),"\n",(0,i.jsx)(s.li,{children:"Untitled Pattern"}),"\n",(0,i.jsx)(s.li,{children:"Regular Expressions Cheat Sheet"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.li,{children:"Tok\xe9nisation de mots et segmentation de phrases (ch. 2)"}),"\n",(0,i.jsx)(s.li,{children:"Normalisation de mots - stemming, lemmatisation, analyse morphologique (ch. 2)"}),"\n",(0,i.jsxs)(s.li,{children:["Mod\xe8les de langue N-grammes (ch.3)","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Construction de mod\xe8les, estimation des probabilit\xe9s, lissage, perplexit\xe9, \xe9valuation"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["Classification de textes (ch. 4-5)","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Naive bayes, repr\xe9sentation de texte (BOW), type d\u2019attributs"}),"\n",(0,i.jsx)(s.li,{children:"R\xe9gression logistique, apprentissage par descente de gradient (ajustement des poids), r\xe9gression multinomiale"}),"\n",(0,i.jsx)(s.li,{children:"\xc9valuation, m\xe9triques, validation crois\xe9e"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["R\xe9seaux de neurones (ch.7)","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Feedforward (MLP), fonctions d\u2019activation, repr\xe9sentation interm\xe9diaire"}),"\n",(0,i.jsx)(s.li,{children:"Propagation avant, \xe9poque et minibatch"}),"\n",(0,i.jsx)(s.li,{children:"Mod\xe8le de langue neuronal et classification de texte"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["S\xe9mantique lexicale et plongements de mots (ch.6)","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Cooccurrences, Similarit\xe9 distributionnelle, word embeddings, vecteur dense, Word2Vec, analogies de mots, Similarit\xe9 s\xe9mantique entre les mots"}),"\n",(0,i.jsx)(s.li,{children:"Pointwise Mutual Information ou information mutuelle marginale"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\xc9tiquetage de s\xe9quences et analyse grammaticale (ch. 8)","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Classe de mots, POS tags, \xe9tiquetage"}),"\n",(0,i.jsx)(s.li,{children:"HMM, Viterbi, CRF-MEMM, choix d'attributs"}),"\n",(0,i.jsx)(s.li,{children:"Entit\xe9s nomm\xe9es, \xe9tiquetage BIO vs. IO, \xe9valuation"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["R\xe9seaux de neurones r\xe9currents (ch.9) - ",(0,i.jsx)(s.strong,{children:"NE SERA PAS \xc9VALU\xc9 \xc0 L'EXAMEN DE MI-SESSION"})]}),"\n",(0,i.jsxs)(s.li,{children:["Les notebooks - compr\xe9hension des r\xe9sultats pr\xe9sent\xe9s","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Week 1: Conversion de questions \xe0 la forme affirmative (Regex)"}),"\n",(0,i.jsxs)(s.li,{children:["Week 2:","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Exemples avec NLTK pour l'analyse de textes"}),"\n",(0,i.jsx)(s.li,{children:"Exemples avec Spacy pour l'analyse de textes"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.li,{children:"Week 3: Mod\xe8les N-grammes de mots avec NLTK"}),"\n",(0,i.jsx)(s.li,{children:"Week 4: Classification de textes avec Scikit-learn"}),"\n",(0,i.jsx)(s.li,{children:"Week 5: Analyse de cooccurrences et de collocations"}),"\n",(0,i.jsxs)(s.li,{children:["Week 6:","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"R\xe9seaux de neurones #1 : Classification avec un r\xe9seau \xe0 1 seule couche (bow_one_layer_nn.ipynb)"}),"\n",(0,i.jsx)(s.li,{children:"R\xe9seaux de neurones #2 : Classification de documents avec un r\xe9seau feedforward multicouches (bow_nlp.ipynb)"}),"\n",(0,i.jsx)(s.li,{children:"R\xe9seaux de neurones #3 : Classification de documents avec un r\xe9seau multicouches et des embeddings (mlp_embeddings.ipynb)"}),"\n",(0,i.jsx)(s.li,{children:"Gensim et les plongements de mots (word embeddings) pr\xe9entra\xeen\xe9s (word_embeddings.ipynb)"}),"\n",(0,i.jsx)(s.li,{children:"Plongements de mots pr\xe9entra\xeen\xe9s fastText (fasttext_embeddings.ipynb)"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.li,{children:"Week 7: Analyse grammaticale avec NLTK et Spacy (pos_tagging.ipynb)"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.li,{children:"Le premier travail"}),"\n",(0,i.jsxs)(s.li,{children:["Les exemples de calculs num\xe9riques.","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"probabilit\xe9s n-grammes"}),"\n",(0,i.jsx)(s.li,{children:"classification avec un mod\xe8le na\xeff bay\xe9sien"}),"\n",(0,i.jsx)(s.li,{children:"descente du gradien et ajustement de poids"}),"\n",(0,i.jsx)(s.li,{children:"Cooccurrences"}),"\n",(0,i.jsx)(s.li,{children:"similarit\xe9 textuelle"}),"\n",(0,i.jsxs)(s.li,{children:["Viterbi et ",(0,i.jsx)(s.a,{href:"https://docs.google.com/spreadsheets/d/16IFFca4_kpr7gYX58fWA8TbQMUl9LDWfErB9keU7F2c/edit#gid=0",children:"Google Sheet"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\xc0 v\xe9rifier","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Similarit\xe9 distributionnelle n'est plus \xe9tudi\xe9e dans le cours pourtant il est dans la liste des sujet \xe0 l'examen."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["Quatre types de question","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Vrai ou faux et choix multiples"}),"\n",(0,i.jsxs)(s.li,{children:["Questions \xe0 d\xe9veloppement (8-10)","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Habituellement court, max 1-2 phrases."}),"\n",(0,i.jsx)(s.li,{children:"Rarement un paragraphe."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["Mise en situation","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Quelle technique devriez-vous utiliser pour..."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["R\xe9solution de probl\xe8me (2-3)","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Des calculs","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Probabilit\xe9s N-grammes"}),"\n",(0,i.jsx)(s.li,{children:"Naives Bayes"}),"\n",(0,i.jsx)(s.li,{children:"R\xe9gression logistique"}),"\n",(0,i.jsx)(s.li,{children:"Viterbi"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),(0,i.jsx)(s.h2,{id:"questions",children:"Questions"}),(0,i.jsx)(s.p,{children:"Peux-tu r\xe9pondre \xe0 ces questions concernant NLP? Pour chaque question, tu dois r\xe9pondre par Vrai ou Faux."}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 1"})," : Pour certains algorithmes d'analyse grammaticale et de reconnaissance d'entit\xe9s nomm\xe9es, la forme d'un mot (",(0,i.jsx)(s.em,{children:"word shape"}),") est un\nattribut tr\xe8s utile  pour \xe9tiqueter des mots appartenant \xe0 des classes ferm\xe9es."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," : C'est utile pour \xe9tiqueter les mots de classes ouvertes (noms, adjectifs, adverbes, verbes). \xc7a n'apporte rien pour\nles mots de classes ferm\xe9es (pr\xe9positions, conjonctions, d\xe9terminants, etc.)."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 2"})," :\nPour l'analyse grammaticale, il est possible d'obtenir un \xe9tiquetage de type Universal dependencies autant avec NTLK que Spacy."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," : Cela est pr\xe9sent\xe9 dans un le notebook sur l'analyse grammaticale. NLTK --\x3e Penn treebank seulement."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 3"})," :\nPour entra\xeener un analyseur grammatical de type HMM, on a besoin d'un corpus qui contient les \xe9tats cach\xe9s qui correspondent aux  \xe9tiquettes de mots (les parts of speech). ",(0,i.jsx)(s.strong,{children:"Vrai"})]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 4"})," :\nLe plongement Word2Vec d'un mot est un vecteur num\xe9rique qui est une repr\xe9sentation dense des contextes dans lesquels on retrouve ce mot.  ",(0,i.jsx)(s.strong,{children:"Vrai"})]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 5"})," :\nLa fonction sigmo\xefde utilis\xe9e dans la r\xe9gression logistique pour la classification binaire offre l'avantage d'\xeatre d\xe9rivable, ce qui permet de calculer le gradient de la fonction de perte (entropie crois\xe9e) et de mettre \xe0 jour les param\xe8tres wi et b. ",(0,i.jsx)(s.strong,{children:"Vrai"})]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 6"})," :\nUne repr\xe9sentation par sac de mots ne permet pas de pr\xe9server l\u2019ordre des jetons d\u2019un texte. ",(0,i.jsx)(s.strong,{children:"Vrai"})]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 7"})," :\nL\u2019analyse grammaticale est une t\xe2che facile \xe0 r\xe9soudre parce seulement 15% des mots sont ambigus."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," - L'analyse grammaticale est complexe en raison de la nature ambigu\xeb et flexible de la langue humaine. Plus de mots peuvent \xeatre ambigus, et l'ambigu\xeft\xe9 n'est pas le seul d\xe9fi.\nLa plupart des occurences de mot correspondent au POS tag le plus fr\xe9quent de ce mot."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 8"}),' :\nDans les mod\xe8les de langue N-grammes, si on estime la probabilit\xe9 du mot inconnu "<UNK>", il n\'est pas n\xe9cessaire de faire de lissage.']}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"}),' - Le lissage est souvent n\xe9cessaire dans les mod\xe8les N-grammes pour g\xe9rer les mots inconnus et \xe9viter les probabilit\xe9s nulles, m\xeame lorsque le mot "<UNK>" est utilis\xe9.\nM\xeame si on donne  une probabilit\xe9 \xe0 un mot rare,  on ne l\'a probablement pas vu dans certains bigrammes ou trigrammes. Et sans lissage, la probabilit\xe9 du bigramme ou du trigramme  serait nulle.']}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 9"})," :\nLa fonction Softmax permet de convertir des valeurs num\xe9riques (positives ou n\xe9gatives) en des valeurs de probabilit\xe9 entre 0 et 1.  ",(0,i.jsx)(s.strong,{children:"Vrai"})]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 10"})," :\nUn vecteur de contexte distributionnel permet de repr\xe9senter un mot cible \xe0 l'aide de ses cooccurrences. De plus, ce vecteur nous aide \xe0 trouver d'autres mots qui peuvent \xeatre utilis\xe9s dans des contextes similaires \xe0 ceux du mot cible."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," : Presque une d\xe9finition compl\xe8te de ce que sont les vecteurs de contexte distributionnels."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 11"})," :\nUn CountVectorizer permet de cr\xe9er des attributs qui correspondent \xe0 des N-grammes de caract\xe8res. Il permet \xe9galement de g\xe9n\xe9rer des attributs \xe0 partir des N-grammes de mots."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Vrai"})," : Cette fonctionnalit\xe9 est pr\xe9sent\xe9e dans un notebook sur le site du cours. C'est \xe9galement le type d'attributs qu'on utilise pour l'identification de langue d'un document."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 12"})," :\nSi on construit un mod\xe8le de langue avec un r\xe9seau de neurones, on n'a pas besoin de faire de lissage parce que le pouvoir pr\xe9dictif du mod\xe8le neuronal est plus \xe9lev\xe9 que celui d'un mod\xe8le N-gramme."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"Vrai"})}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 13"})," :\nLe r\xe9sultat de la lemmatisation de intelligence artificielle  est intellig artifici."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"}),' - La lemmatisation de "intelligence artificielle" serait plus probablement "intelligence artificiel", selon le contexte et les r\xe8gles de lemmatisation.\nC\'est du stemming, pas de la lemmatisation.']}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 14"}),' :\nSi on fait la lemmatisation de "\xeatre ou ne pas \xeatre" , on obtient la m\xeame chose. ',(0,i.jsx)(s.strong,{children:"Vrai"})]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 15"})," :\nL'expression r\xe9guli\xe8re suivante est bonne et efficace pour extraire des d\xe9terminants dans un texte fran\xe7ais.\nregex_pattern = \"[le|la|les|l'|un|une|des|du|au|aux] \""]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," - L'expression r\xe9guli\xe8re correcte serait plut\xf4t quelque chose comme \"le|la|les|l'|un|une|des|du|au|aux\\b\". L'original ne capture pas les d\xe9terminants de mani\xe8re efficace en raison de l'utilisation incorrecte des crochets et de l'absence de d\xe9limitation des mots. C'est une \xe9num\xe9ration de mots, pas une expression r\xe9guli\xe8re."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 16"})," :\nUn one-hot vector est un vecteur ayant une valeur de 1 pour toutes les cellules correspondant aux mots d'un texte."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"}),' - Un "one-hot vector" est un vecteur o\xf9 une seule position est marqu\xe9e avec 1 et toutes les autres positions sont 0.\none-hot vector --\x3e repr\xe9sentation pour 1 seul mot (representation of a single word)']}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 17"})," :\nLa normalisation permet de r\xe9duire le nombre de types d\u2019un corpus et la taille du vocabulaire d\u2019une application. ",(0,i.jsx)(s.strong,{children:"Vrai"})]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 18"})," :\nOn peut toujours trouver le lex\xe8me d\u2019un mot dans un dictionnaire."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," - Lexeme = stemming = no dictionary."]}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 19"})," :\nLa forme de surface d'un mot peut \xeatre d\xe9riv\xe9e ou infl\xe9chie. ",(0,i.jsx)(s.strong,{children:"Vrai"})]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 20"})," :\nLes synonymes sont souvent des cooccurrences. \xcates-vous d\u2019accord ?"]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Faux"})," - Les synonymes ne sont pas n\xe9cessairement des cooccurrences. Les synonymes sont des mots ayant des significations similaires, tandis que les cooccurrences sont typiquement des mots qui apparaissent fr\xe9quemment ensemble. Les synonymes n'ont pas tendance \xe0 se retrouver dans les m\xeames phrases."]}),"\n"]}),(0,i.jsx)(s.p,{children:"Formulez une r\xe9ponse courte, claire et concise \xe0 la question suivante. Maximum 4 phrases."}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 21"})," : Dans quel cas devrait-on faire un encodage IOB des s\xe9quences d\u2019entra\xeenement au lieu d\u2019un encodage IO?"]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Lorsqu'on doit distinguer clairement les limites des entit\xe9s adjacentes ou multi-mots dans les s\xe9quences d'entra\xeenement."}),"\n",(0,i.jsx)(s.li,{children:"Lorsque des entit\xe9s nomm\xe9es de m\xeame type se suivent dans des textes."}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 22"})," : Associer les plongements de mots pr\xe9entra\xeen\xe9s au type de gestion de mots inconnus, soit Word2Vec, Spacy ou FastText."]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 22-A"})," : Je construis une repr\xe9sentation \xe0 partir des caract\xe8res du mot inconnu."]}),(0,i.jsx)(s.p,{children:'La m\xe9thode que vous d\xe9crivez, "construire une repr\xe9sentation \xe0 partir des caract\xe8res du mot inconnu", correspond \xe0 la fa\xe7on dont FastText g\xe8re les mots inconnus.'}),(0,i.jsx)(s.p,{children:"FastText se diff\xe9rencie par sa capacit\xe9 \xe0 cr\xe9er des vecteurs de mots pour des mots inconnus en prenant en compte la morphologie, c'est-\xe0-dire en analysant les sous-mots ou n-grammes de caract\xe8res. Ainsi, m\xeame si le mot entier n'a jamais \xe9t\xe9 vu durant l'entra\xeenement, sa repr\xe9sentation peut \xeatre construite \xe0 partir de ses sous-composants, ce qui est particuli\xe8rement utile pour les mots avec des fautes de frappe, des conjugaisons ou des formes fl\xe9chies qui n'ont pas \xe9t\xe9 observ\xe9es lors de l'entra\xeenement."}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 22-B"})," : Je ne construis pas de repr\xe9sentation pour les mots inconnus."]}),(0,i.jsx)(s.p,{children:'La m\xe9thode "je ne construis pas de repr\xe9sentation pour les mots inconnus" est associ\xe9e \xe0 "Word2Vec."'}),(0,i.jsx)(s.p,{children:"Word2Vec, l'un des mod\xe8les de plongement lexical les plus connus, ne g\xe9n\xe8re des vecteurs que pour les mots qu'il a vus pendant la phase d'entra\xeenement. Si un mot n'a pas \xe9t\xe9 vu lors de l'entra\xeenement, Word2Vec n'est pas en mesure de construire un vecteur pour ce mot \xe0 partir de z\xe9ro et, dans de tels cas, souvent un vecteur sp\xe9cial pour les mots inconnus (comme un vecteur de z\xe9ros ou un vecteur sp\xe9cial \"<UNK>\") doit \xeatre utilis\xe9. Cela contraste avec des approches comme FastText qui peuvent g\xe9n\xe9rer des vecteurs pour des mots inconnus bas\xe9s sur leurs sous-unit\xe9s."}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 22-C"})," : Je construis un vecteur rempli de z\xe9ros pour les mots inconnus."]}),(0,i.jsx)(s.p,{children:'La m\xe9thode "je construis un vecteur rempli de z\xe9ros pour les mots inconnus" peut \xeatre associ\xe9e \xe0 "Spacy."'}),(0,i.jsx)(s.p,{children:"Spacy, une biblioth\xe8que populaire pour le traitement du langage naturel, utilise des vecteurs pr\xe9entra\xeen\xe9s pour la repr\xe9sentation des mots. Lorsqu'il rencontre un mot inconnu (c'est-\xe0-dire un mot qui n'\xe9tait pas pr\xe9sent dans le vocabulaire lors de l'entra\xeenement des vecteurs de mots), Spacy attribue un vecteur de z\xe9ros \xe0 ce mot. C'est une diff\xe9rence importante par rapport \xe0 des mod\xe8les comme FastText, qui g\xe9n\xe8rent des vecteurs pour des mots inconnus en utilisant des n-grammes de caract\xe8res, permettant une certaine repr\xe9sentation m\xeame pour les mots non vus lors de l'entra\xeenement."}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 23"})," : Dans Word2Vec, maximiser la probabilit\xe9 \xe9quivaut \xe0 maximiser la similarit\xe9 des vecteurs de mot et de contexte. Quelle est la principale partie de l'architecture Word2Vec qui m\xe8ne \xe0 ce r\xe9sultat ."]}),(0,i.jsx)(s.p,{children:"Dans Word2Vec, c'est la couche de projection cach\xe9e qui, en optimisant les poids pendant l'entra\xeenement, permet de maximiser la similarit\xe9 des vecteurs de mots et de contexte."}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 24"})," : Dans le travail pratique #1, vous pouviez utiliser soit la perplexit\xe9 ou le logprob pour choisir l'ordre des mots compl\xe9tant un proverbe. Est-ce que les deux mesures donnent le m\xeame choix de proverbe ? Pourquoi ?."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Ils donnent le m\xeame r\xe9sultat. La perplexit\xe9 est normalis\xe9e en fonction du nombre de mot (N). Les r\xe9sultats pourraient \xeatre diff\xe9rents seulement si les diff\xe9rentes versions de proverbes n'avaient pas toutes la m\xeame longueur (c.-\xe0-d. si les proverbes compl\xe9t\xe9s n'avaient pas tous le m\xeame nombre de mots). Ce n'est pas le cas - tous les proverbes candidats ont la m\xeame longueur."}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 25"})," : Pour le POS tagging, pourquoi est-ce que les mod\xe8les CRF (MEMM) donnent souvent de meilleurs r\xe9sultats que les mod\xe8les HMM ?"]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"La capacit\xe9 des CRF (MEMM) \xe0 utiliser des caract\xe9ristiques complexes (ex. attributs comme les pr\xe9fixes, suffixes, majuscules), et \xe0 consid\xe9rer l'ensemble de la s\xe9quence d'entr\xe9e les rend g\xe9n\xe9ralement plus performants que les HMM"}),"\n",(0,i.jsx)(s.li,{children:"Ils permettent de prendre en compte plus d'attributs qui d\xe9crivent le contexte et la morphologie des mots (par ex. pr\xe9fixe, suffixe, word shape)."}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 26"})," : Si nous avons le choix entre deux corpus de contenus diff\xe9rents pour construire un mod\xe8le (par ex. un mod\xe8le N-grammes ou Naive Bayes), devrait-on toujours choisir le corpus le plus gros? Pourquoi ?"]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Non, un corpus plus grand n'est pas toujours meilleur. La qualit\xe9, la pertinence, et la repr\xe9sentativit\xe9 des donn\xe9es sont importantes. Un grand corpus non pertinent ou biais\xe9 peut d\xe9t\xe9riorer la performance du mod\xe8le."}),"\n",(0,i.jsx)(s.li,{children:"Si le contenu du petit corpus est plus pertinent \xe0 la t\xe2che \xe0 accomplir (par ex. des critiques de films pour construire un analyseur de sentiment), on devrait alors le choisir. Sinon on devrait choisir le plus gros."}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 27"})," : Dans Word2vec, \xe0 quoi correspond la taille des plongements de mots ?  Et \xe0 quoi correspondent les valeurs du plongement d'un mot ?"]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Taille :\xa0 Nombre de neurones dans la couche cach\xe9e."}),"\n",(0,i.jsx)(s.li,{children:"Valeurs : Les poids reliant un mot cible de la couche d'entr\xe9e \xe0 la couche cach\xe9e."}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 28"})," :"]}),(0,i.jsx)(s.p,{children:"Lesquelles des 3 paires de mots suivantes sont des cooccurrences ?"}),(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{children:"\u03bc"}),(0,i.jsx)(s.th,{children:"\u03c3"}),(0,i.jsx)(s.th,{children:"Mot 1"}),(0,i.jsx)(s.th,{children:"Mot 2"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"0.17"}),(0,i.jsx)(s.td,{children:"2.49"}),(0,i.jsx)(s.td,{children:"clustering"}),(0,i.jsx)(s.td,{children:"classification"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"2.03"}),(0,i.jsx)(s.td,{children:"\xa01.25"}),(0,i.jsx)(s.td,{children:"nations"}),(0,i.jsx)(s.td,{children:"Canada"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"0.97"}),(0,i.jsx)(s.td,{children:"0.37"}),(0,i.jsx)(s.td,{children:"Nova"}),(0,i.jsx)(s.td,{children:"Scotia"})]})]})]}),(0,i.jsx)(s.p,{children:"Une cooccurrence dans le contexte du traitement du langage naturel se r\xe9f\xe8re g\xe9n\xe9ralement \xe0 des mots qui apparaissent fr\xe9quemment ensemble dans un texte. Les chiffres \u03bc (moyenne) et \u03c3 (\xe9cart-type) semblent indiquer la proximit\xe9 statistique des mots dans un corpus, probablement leur fr\xe9quence de cooccurrence et la consistance de cette cooccurrence."}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"clustering | classification"}),": \u03bc est faible, mais \u03c3 est \xe9lev\xe9, indique une faible cooccurrence. Les mots sont th\xe9matiquement li\xe9s mais ne cooccurrent pas n\xe9cessairement directement."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"nations | Canada"}),': Un \u03bc \xe9lev\xe9 et un \u03c3 moyen sugg\xe8rent que ces termes apparaissent probablement souvent ensemble, avec une variabilit\xe9 mod\xe9r\xe9e. Ils sont des cooccurrences, car "Canada" est souvent discut\xe9 dans le contexte des "nations".']}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Nova | Scotia"}),': Avec un \u03bc mod\xe9r\xe9 et un \u03c3 faible, ces mots cooccurrent presque toujours ("Nova Scotia" est une province canadienne), indiquant une cooccurrence forte et consistante.']}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 29"})," : Peux-tu compl\xe9ter cette phrase?\nSi on utilisait un mod\xe8le HMM pour faire de la lemmatisation, les observations seraient _________ et les \xe9tats cach\xe9s seraient __________."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Observations = Les mots tels qu'ils apparaissent dans le texte"}),"\n",(0,i.jsx)(s.li,{children:"\xc9tats cach\xe9s = Les lemmes, soit la forme de base des mots"}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 30"})," : Pour la t\xe2che 1 du travail pratique #1, avez-vous utilis\xe9 1 seule expression r\xe9guli\xe8re ou plusieurs? Pourquoi? Soyez sp\xe9cifique."]}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"On s'attend pour ce probl\xe8me qu'on utilise plusieurs regex car il est difficile d'en construire une seule qui prend en compte tous les cas possibles et toutes les informations \xe0 extraire."}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 31"})," : Apprentissage des poids d'un mod\xe8le de r\xe9gression logistique"]}),(0,i.jsx)(s.p,{children:"On retrouve dans le chapitre de Jurafsky et Martin sur la r\xe9gression logistique un exemple de descente de gradient.  \xc0 la fin de cet exemple, les poids du mod\xe8le sont [.15, .1, .05]. - An example of gradient descent can be found in Jurafsky and Martin's chapter on logistic regression. At the end of this example, the model weights are [.15, .1, .05]."}),(0,i.jsx)(s.p,{children:"En utilisant l'observation suivante - Using the following observation :"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"x1 = Compte de mots positifs = Count of positive lexicon words = 1"}),"\n",(0,i.jsx)(s.li,{children:"x2 = Compte de mots n\xe9gatifs = Count of negative lexicon words = 3"}),"\n",(0,i.jsx)(s.li,{children:"y = 0  (critique n\xe9gative - negative review)"}),"\n"]}),(0,i.jsx)(s.p,{children:"La 2e \xe9tape de descente de gradient est calcul\xe9e ainsi."}),(0,i.jsx)(s.p,{children:"Soit w = [w1, w2] = [.15, .1], b=.05,\xa0x = [x1, x2] = [1, 3] et y = 0. On doit calculer\xa0\u2207L qui correspond \xe0 une matrice 3x1 o\xf9 les \xe9l\xe9ments sont respectivement"}),(0,i.jsx)(s.p,{children:"\u03c3(wx+b)-y)x1 =\xa0\u03c3((.15\xd71+.1\xd73+.05)-0) \xd7 1 =\xa00.6225"}),(0,i.jsx)(s.p,{children:"\u03c3(wx+b)-y)x2\xa0=\xa0\u03c3((.15\xd71+.1\xd73+.05)-0) \xd7 3\xa0=\xa01.8674"}),(0,i.jsx)(s.p,{children:"\u03c3(wx+b)-y)\xa0=\xa0\u03c3((.15\xd71+.1\xd73+.05)-0)\xa0=\xa0=\xa00.6225"}),(0,i.jsx)(s.p,{children:"\xc7a donne la matrice 3x1 [0.6225, 1.8674, 0.6225] transpos\xe9e."}),(0,i.jsx)(s.p,{children:"Ensuite, nous avons\xa0\u03b7 = .1 et on calcul les nouveaux param\xe8tres\xa0\u03b82\xa0ainsi :"}),(0,i.jsx)(s.p,{children:"w1 = w1 -\xa0\u03b7\xa0\xd7\xa00.6225 = .15 - 0.1\xa0\xd7 0.6225 = 0.0878"}),(0,i.jsx)(s.p,{children:"w2 =\xa0w2\xa0-\xa0\u03b7\xa0\xd7 1.8674\xa0= .1\xa0- 0.1\xa0\xd7 1.8674\xa0= -0.0867"}),(0,i.jsx)(s.p,{children:"b = b -\xa0\u03b7\xa0\xd7 0.6225 = .05 - 0.1\xa0\xd7 0.6225 = -0.0122"}),(0,i.jsx)(s.p,{children:"Que pensez-vous des changements de poids ? Les poids changent beaucoup entre l'\xe9tape 1 et l'\xe9tape 2, cela sugg\xe8re que nous sommes encore loin de la convergence. Faudrait faire encore plusieurs it\xe9rations avant de voir les poids se stabiliser."}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Le r\xe9ponse sugg\xe9r\xe9e est : Apprendre \xe0 partir d'un exemple n\xe9gatif pr\xe9dit comme positif diminue les poids."}),"\n"]}),(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 32"})," : Mod\xe8les N-grammes"]}),(0,i.jsx)(s.p,{children:"Supposons que l\u2019on veut construire des mod\xe8les N-grammes \xe0 partir d\u2019un corpus constitu\xe9 des 3 phrases normalis\xe9es suivantes - Let's suppose we want to build N-gram models from a corpus consisting of the following 3 normalized sentences:"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"<s> le qu\xe9bec est une province du canada </s>"}),"\n",(0,i.jsx)(s.li,{children:"<s> le qu\xe9bec a une population de huit millions de personnes </s>"}),"\n",(0,i.jsx)(s.li,{children:"<s> le qu\xe9bec est un peu plus petit que le mexique </s>"}),"\n"]}),(0,i.jsx)(s.p,{children:"Quelles sont les valeurs de probabilit\xe9 suivantes - What are the following probability values?"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"PMLE(petit) , PMLE(grand), PMLE(mexique | le), PMLE(a | le qu\xe9bec), PMLE(est | le mexique)"}),"\n",(0,i.jsx)(s.li,{children:"PLAPLACE(petit), PLAPLACE(grand), PLAPLACE (mexique | le) , PLAPLACE (a | le qu\xe9bec), PLAPLACE (est | le mexique)"}),"\n"]}),(0,i.jsx)(s.p,{children:"R\xe9ponse :"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"N = 33, incluant les marqueurs <s> et </s>"}),"\n",(0,i.jsx)(s.li,{children:"|V| = 20 mots diff\xe9rents, incluant les marqueurs  <s> et  </s>"}),"\n",(0,i.jsx)(s.li,{children:"PMLE(petit) = 1/33"}),"\n",(0,i.jsx)(s.li,{children:"PMLE(grand) = 0/33"}),"\n",(0,i.jsx)(s.li,{children:"PMLE(mexique | le) = 1/4"}),"\n",(0,i.jsx)(s.li,{children:"PMLE(a | le qu\xe9bec) = 1/3"}),"\n",(0,i.jsx)(s.li,{children:"PMLE(est | le mexique) = 0/1"}),"\n",(0,i.jsx)(s.li,{children:"PLAPLACE(petit) = (1+1)/(33+20) = 2/53"}),"\n",(0,i.jsx)(s.li,{children:"PLAPLACE(grand) = (0+1)/(33+20) = 1/53"}),"\n",(0,i.jsx)(s.li,{children:"PLAPLACE\xa0(mexique | le) = (1+1)/(4+33) = 2/37"}),"\n",(0,i.jsx)(s.li,{children:"PLAPLACE\xa0(a | le qu\xe9bec) = (1+1)/(3+33) = 2/36"}),"\n",(0,i.jsx)(s.li,{children:"PLAPLACE\xa0(est | le mexique) = (0+1)/(1+33) = 1/34"}),"\n"]}),(0,i.jsx)(s.p,{children:"Commentaire de l'enseignant : |V| = 21. Erreur au d\xe9nominateur des PLaplace bigrammes et trigrammes - +33 au lieu de +21."})]})]})}function m(e={}){const{wrapper:s}={...(0,r.a)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}},2828:(e,s,n)=>{n.d(s,{Z:()=>r});n(67294);var i=n(85893);const r=e=>{let{children:s}=e;return(0,i.jsx)("p",{children:"Sorry, this content is protected and not available."})}},11151:(e,s,n)=>{n.d(s,{Z:()=>o,a:()=>l});var i=n(67294);const r={},t=i.createContext(r);function l(e){const s=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:s},e.children)}}}]);