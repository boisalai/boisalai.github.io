"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[9905],{37720:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>i,contentTitle:()=>t,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var l=a(85893),s=a(11151);const o={},t="Ollama",r={id:"references/llms/ollama",title:"Ollama",description:"Ollama is a cutting-edge platform designed to run open-source",source:"@site/docs/references/llms/ollama.md",sourceDirName:"references/llms",slug:"/references/llms/ollama",permalink:"/docs/references/llms/ollama",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/references/llms/ollama.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"LlaVA Large Language-and-Vision Assistant",permalink:"/docs/references/llms/llava"},next:{title:"Phi-2",permalink:"/docs/references/llms/phi-2"}},i={},c=[{value:"Install",id:"install",level:2},{value:"Run a model",id:"run-a-model",level:2},{value:"Structured Outputs with Ollama\xb6",id:"structured-outputs-with-ollama",level:2},{value:"Docker",id:"docker",level:2},{value:"See also",id:"see-also",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.a)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.h1,{id:"ollama",children:"Ollama"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.a,{href:"https://ollama.ai/",children:"Ollama"})," is a cutting-edge platform designed to run open-source\nlarge language models locally on your machine."]}),"\n",(0,l.jsx)(n.h2,{id:"install",children:"Install"}),"\n",(0,l.jsxs)(n.p,{children:["Go to ",(0,l.jsx)(n.a,{href:"https://ollama.ai/",children:"https://ollama.ai/"})," and follow the instructions."]}),"\n",(0,l.jsx)(n.h2,{id:"run-a-model",children:"Run a model"}),"\n",(0,l.jsx)(n.p,{children:"Before anything, quit Ollama applicationand make sure that Ollama is not running. Then, run the following command:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"$ ollama run qwen:14b\n>>> Bonjour\nBonjour! Comment puis-je vous aider aujourd'hui?\n"})}),"\n",(0,l.jsxs)(n.p,{children:["If you go to ",(0,l.jsx)(n.a,{href:"http://127.0.0.1:11434/",children:"http://127.0.0.1:11434/"}),', you should see "Ollama is running" in your browser.']}),"\n",(0,l.jsxs)(n.p,{children:["To exit the LLM, type ",(0,l.jsx)(n.code,{children:"/bye"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["See ",(0,l.jsx)(n.a,{href:"https://ollama.ai/library/qwen",children:"https://ollama.ai/library/qwen"})," for more information."]}),"\n",(0,l.jsx)(n.h2,{id:"structured-outputs-with-ollama",children:"Structured Outputs with Ollama\xb6"}),"\n",(0,l.jsx)(n.p,{children:"Ollama can be used to generate structured outputs. For example, you can use Ollama to generate a list of items, a table, or a list of bullet points."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nimport instructor\n\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    fact: List[str] = Field(..., description="A list of facts about the character")\n\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url="http://localhost:11434/v1",\n        api_key="ollama",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nresp = client.chat.completions.create(\n    model="llama2",\n    messages=[\n        {\n            "role": "user",\n            "content": "Tell me about the Harry Potter",\n        }\n    ],\n    response_model=Character,\n)\nprint(resp.model_dump_json(indent=2))\n"""\n{\n  "name": "Harry James Potter",\n  "age": 37,\n  "fact": [\n    "He is the chosen one.",\n    "He has a lightning-shaped scar on his forehead.",\n    "He is the son of James and Lily Potter.",\n    "He attended Hogwarts School of Witchcraft and Wizardry.",\n    "He is a skilled wizard and sorcerer.",\n    "He fought against Lord Voldemort and his followers.",\n    "He has a pet owl named Snowy."\n  ]\n}\n"""\n'})}),"\n",(0,l.jsxs)(n.p,{children:["See ",(0,l.jsx)(n.a,{href:"https://jxnl.github.io/instructor/blog/2024/02/08/ollama/",children:"Structured Outputs with Ollama"}),"."]}),"\n",(0,l.jsx)(n.h2,{id:"docker",children:"Docker"}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsx)(n.p,{children:"2024-01-31"}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Deploy Ollama using official docker image."}),"\n",(0,l.jsxs)(n.p,{children:["To run a LLM locally, run the following commands. We assume you have Docker installed if you don\u2019t check ",(0,l.jsx)(n.a,{href:"https://docs.docker.com/get-docker/",children:"this page"}),"."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"docker pull ollama/ollama\ndocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Check if the image is running with ",(0,l.jsx)(n.code,{children:"docker ps"}),". You should see something like this:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-txt",children:'\u279c  ~ docker ps\nCONTAINER ID   IMAGE           COMMAND               CREATED         STATUS         PORTS                      NAMES\n04acc4469846   ollama/ollama   "/bin/ollama serve"   2 minutes ago   Up 2 minutes   0.0.0.0:11434->11434/tcp   ollama\n\u279c  ~ \n'})}),"\n",(0,l.jsxs)(n.p,{children:["Also, go to ",(0,l.jsx)(n.a,{href:"http://localhost:11434/",children:"http://localhost:11434/"}),' and you should see "Allama is running" in your browser.']}),"\n",(0,l.jsx)(n.p,{children:"Run a model inside docker container:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"docker exec -it ollama ollama run llama2\n"})}),"\n",(0,l.jsx)(n.p,{children:"You should see something like this:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-txt",children:"pulling manifest \npulling 8934d96d3f08... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 3.8 GB                         \npulling 8c17c2ebb0ea... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 7.0 KB                         \npulling 7c23fb36d801... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.8 KB                         \npulling 2e0493f67d0c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   59 B                         \npulling fa304d675061... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   91 B                         \npulling 42ba7f8a01dd... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  557 B                         \nverifying sha256 digest \nwriting manifest \nremoving any unused layers \nsuccess \n>>> Qui es-tu?\n\nI am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner.\nI am trained on a massive dataset of text from the internet and can generate human-like responses to a wide range of topics\nand questions. Is there something specific you would like to talk about or ask?\n\n>>> Send a message (/? for help)\n"})}),"\n",(0,l.jsx)(n.p,{children:"For more:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://ollama.ai/blog/ollama-is-now-available-as-an-official-docker-image",children:"Ollama is now available as an official Docker image"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://hub.docker.com/r/ollama/ollama",children:"https://hub.docker.com/r/ollama/ollama"})}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"see-also",children:"See also"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://ollama.com/blog/openai-compatibility",children:"OpenAI compatibility"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(d,{...e})}):d(e)}},11151:(e,n,a)=>{a.d(n,{Z:()=>r,a:()=>t});var l=a(67294);const s={},o=l.createContext(s);function t(e){const n=l.useContext(o);return l.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),l.createElement(o.Provider,{value:n},e.children)}}}]);