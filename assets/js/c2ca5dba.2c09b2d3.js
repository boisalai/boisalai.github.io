"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[1085],{65181:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>_,frontMatter:()=>o,metadata:()=>i,toc:()=>l});var s=t(85893),a=t(11151);const o={sidebar_label:"Travail pratique 2",sidebar_position:82,sidebar_class_name:"hidden"},r="Travail pratique 2",i={id:"courses/ift-7022/travail-2",title:"Travail pratique 2",description:"\xc9ch\xe9ance : \xc0 remettre le 24 nov. 2023 \xe0 23h59. Je suis l'\xe9quipe 43.",source:"@site/docs/courses/ift-7022/travail-2.md",sourceDirName:"courses/ift-7022",slug:"/courses/ift-7022/travail-2",permalink:"/docs/courses/ift-7022/travail-2",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/ift-7022/travail-2.md",tags:[],version:"current",sidebarPosition:82,frontMatter:{sidebar_label:"Travail pratique 2",sidebar_position:82,sidebar_class_name:"hidden"},sidebar:"tutorialSidebar",previous:{title:"Travail pratique 1",permalink:"/docs/courses/ift-7022/travail-1"},next:{title:"Travail pratique 3",permalink:"/docs/courses/ift-7022/travail-3"}},c={},l=[];function d(e){const n={code:"code",h1:"h1",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"travail-pratique-2",children:"Travail pratique 2"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\xc9ch\xe9ance"})," : \xc0 remettre le 24 nov. 2023 \xe0 23h59. Je suis l'\xe9quipe 43."]}),"\n",(0,s.jsx)(n.p,{children:"Nous avons trois fichiers contenant des descriptions d'accidents."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["un fichier d\u2019entra\xeenement : ",(0,s.jsx)(n.code,{children:"data/incidents_train.json"})]}),"\n",(0,s.jsxs)(n.li,{children:["un fichier de validation : ",(0,s.jsx)(n.code,{children:"data/incidents_dev.json"})]}),"\n",(0,s.jsxs)(n.li,{children:["un fichier de test : ",(0,s.jsx)(n.code,{children:"data/incidents_test.json"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Chaque fichier contient deux colonnes :"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["colonne ",(0,s.jsx)(n.code,{children:"text"})," qui est une cha\xeene de caract\xe8re correspondant \xe0 la description d'un accident."]}),"\n",(0,s.jsxs)(n.li,{children:["colonne ",(0,s.jsx)(n.code,{children:"label"})," qui est un entier de 0 \xe0 8 correspondant \xe0 l'\xe9tiquette du type d'accident."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Nous devons entra\xeener un mod\xe8le de r\xe9seau de neurones de type feedforward multicouche (MLP) avec plongements de mots pour d\xe9terminer le type d\u2019un incident \xe0 partir de sa description."}),"\n",(0,s.jsx)(n.p,{children:"Nous devons respecter les consignes suivantes :"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Utilisation de Spacy pour la tokenisation et les plongements de mots."}),"\n",(0,s.jsx)(n.li,{children:"Aucune normalisation des donn\xe9es."}),"\n",(0,s.jsx)(n.li,{children:"Agr\xe9gation des plongements de mots : Comparer les approches max, average et min pooling."}),"\n",(0,s.jsx)(n.li,{children:"Structure du r\xe9seau : une seule couche cach\xe9e mais nous devons choisir la taille."}),"\n",(0,s.jsx)(n.li,{children:"Explication de la d\xe9marche utilis\xe9e."}),"\n",(0,s.jsxs)(n.li,{children:["Le code doit \xeatre structur\xe9 selon les \xe9tapes suivantes :","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Cr\xe9ation du jeu de donn\xe9es (dataset)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Gestion de plongements de mots (embeddings)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:"Cr\xe9ation de mod\xe8le(s)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.ol,{start:"4",children:["\n",(0,s.jsx)(n.li,{children:"Fonctions utilitaires"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.ol,{start:"5",children:["\n",(0,s.jsx)(n.li,{children:"Entra\xeenement de mod\xe8le(s)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.ol,{start:"6",children:["\n",(0,s.jsx)(n.li,{children:"\xc9valuation et analyse de r\xe9sultats"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Peux-tu cr\xe9er ce code?"}),"\n",(0,s.jsx)(n.p,{children:"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"}),"\n",(0,s.jsx)(n.p,{children:"J'ai ce code:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \xc9tape 1. Cr\xe9ation du jeu de donn\xe9es (dataset)\n\nimport json\n\n# Chargement des fichiers\ndef load_data(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        data_list = json.load(file)\n\n    # Utilisation de la compr\xe9hension de liste pour extraire les textes et les labels\n    texts = [item['text'] for item in data_list]\n    labels = [int(item['label']) for item in data_list]\n\n    return texts, labels\n\n# Charger les donn\xe9es en utilisant la fonction modifi\xe9e\ntrain_texts, train_targets = load_data('data/incidents_train.json')\nvalid_texts, valid_targets = load_data('data/incidents_dev.json')\ntest_texts, test_targets = load_data('data/incidents_test.json')\n\n# \xc9tape 2. Gestion de plongements de mots (embeddings)\xb6\n\nimport spacy \n\nnlp = spacy.load('en_core_web_md')\n\nimport numpy as np\n\ndef average_embedding(sentence, nlp_model=nlp):\n    tokenised_sentence = nlp_model(sentence)  # tokenized\n    nb_column = len(tokenised_sentence)\n    nb_rows =  nlp_model.vocab.vectors_length\n    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                  \n    for index, token in enumerate(tokenised_sentence):\n        sentence_embedding_matrix[:, index] = token.vector\n    return np.average(sentence_embedding_matrix, axis=1)\n\ndef maxpool_embedding(sentence, nlp_model=nlp): \n    tokenised_sentence = nlp_model(sentence)\n    nb_column = len(tokenised_sentence)\n    nb_rows =  nlp_model.vocab.vectors_length \n    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                    \n    for index, token in enumerate(tokenised_sentence):\n        sentence_embedding_matrix[:, index] = token.vector\n    return np.max(sentence_embedding_matrix, axis=1)\n\ndef minpool_embedding(sentence, nlp_model=nlp): \n    tokenised_sentence = nlp_model(sentence)\n    nb_column = len(tokenised_sentence)\n    nb_rows =  nlp_model.vocab.vectors_length \n    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                    \n    for index, token in enumerate(tokenised_sentence):\n        sentence_embedding_matrix[:, index] = token.vector\n    return np.min(sentence_embedding_matrix, axis=1)\n\ndef spacy_embedding(sentence, nlp_model=nlp):\n    tokenised_sentence = nlp_model(sentence)\n    return tokenised_sentence.vector\n\n# \xc9tape 3. Cr\xe9ation de mod\xe8le(s)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MultiLayerPerceptron(nn.Module):\n    def __init__(self, input_size, hidden_layer_size, output_size) :\n        super(MultiLayerPerceptron, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n        self.fc2 = nn.Linear(hidden_layer_size, output_size)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# \xc9tape 4. Fonctions utilitaires\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import FloatTensor, LongTensor\nfrom typing import List\n\nclass SpacyDataset(Dataset):\n    def __init__(self, dataset: List[str] , target: np.array, sentence_aggregation_function):\n        self.dataset = dataset\n        self.doc_embeddings = [None for _ in range(len(dataset))]\n        self.sentence_aggregation_function = sentence_aggregation_function \n        self.target = target\n    \n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n        if self.doc_embeddings[index] is None:\n            self.doc_embeddings[index] = self.sentence_aggregation_function(self.dataset[index])\n        return FloatTensor(self.doc_embeddings[index]), LongTensor([self.target[index]]).squeeze(0)\n\nfrom typing import Callable \n\ndef create_data_loader(aggregation_function: Callable):\n    # On finalise la construction des 3 jeux de donn\xe9es et leurs dataloaders\n    train_dataset = SpacyDataset(train_texts, train_targets, aggregation_function)\n    valid_dataset = SpacyDataset(valid_texts, valid_targets, aggregation_function)\n    test_dataset = SpacyDataset(test_texts, test_targets, aggregation_function) # \xe0 changer\n    \n    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n\n    return train_dataloader, valid_dataloader, test_dataloader\n\n# Un dictionnaire pour choisir le type d'agr\xe9gation\naggregation = {\n    \"maxpool\" : maxpool_embedding,\n    \"average\" : average_embedding,\n    \"minpool\" : minpool_embedding\n}\n\naggregation_function = aggregation[\"average\"] \ntrain_dataloader, valid_dataloader, test_dataloader = create_data_loader(average_embedding)\n\n# \xc9tape 5. Entra\xeenement de mod\xe8le(s)\n\nembedding_size = nlp.meta['vectors']['width'] # La dimension des vecteurs d'embeddings de Spacy\nnb_classes = len(set(train_targets))\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.optim.lr_scheduler import StepLR\n\n# Fixez le seed pour la reproductibilit\xe9.\ntorch.manual_seed(42)\n\n# Param\xe8tres\nhidden_size = 100\nnum_epochs = 30\nlearning_rate = 0.1 \nmomentum = 0.9\n\n# Cr\xe9ez le mod\xe8le\nmodel = MultiLayerPerceptron(embedding_size, hidden_size, nb_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n\n# D\xe9finir le scheduler (ici, nous r\xe9duisons le taux d'apprentissage de 10% toutes les 10 \xe9poques)\nscheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n\n# D\xe9placez le mod\xe8le vers l'appareil appropri\xe9\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Fonction pour la phase d'entra\xeenement\ndef train_model(model, criterion, optimizer, train_loader, valid_loader, num_epochs):\n    \n    # Pour stocker les pertes et les pr\xe9cisions pour chaque \xe9poque\n    history = {\n        \"train_loss\": [],\n        \"valid_loss\": [],\n        \"train_acc\": [],\n        \"valid_acc\": []\n    }\n    \n    for epoch in range(num_epochs):\n        model.train()  # D\xe9finissez le mod\xe8le en mode d'entra\xeenement\n\n        # Suivi des mesures\n        running_loss = 0.0\n        correct_predictions = 0\n        total_predictions = 0\n\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            optimizer.zero_grad()  # Effacer les gradients pr\xe9c\xe9dents\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Avance directe\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            # R\xe9tropropagation et optimisation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Mettre \xe0 jour les statistiques\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct_predictions += predicted.eq(targets).sum().item()\n            total_predictions += targets.size(0)\n\n        # Appeler le scheduler \xe0 la fin de chaque \xe9poque\n        scheduler.step()\n\n        # Apr\xe8s la boucle d'entra\xeenement, calculer la perte et la pr\xe9cision moyennes\n        epoch_loss = running_loss / len(train_loader)\n        epoch_acc = (correct_predictions / total_predictions) * 100.0\n\n        # Ajouter la perte et la pr\xe9cision \xe0 l'historique\n        history['train_loss'].append(epoch_loss)\n        history['train_acc'].append(epoch_acc)\n\n        # Valider le mod\xe8le et obtenir la perte et la pr\xe9cision\n        valid_loss, valid_acc = validate_model(model, criterion, valid_loader)\n\n        # Ajouter la perte et la pr\xe9cision de validation \xe0 l'historique\n        history['valid_loss'].append(valid_loss)\n        history['valid_acc'].append(valid_acc)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, \" \n                + f\"Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%, \" \n                + f\"Val_Loss: {valid_loss:.4f}, Val_Accuracy: {valid_acc:.2f}%\")\n\n    return history\n\n\n# Fonction pour la phase de validation\ndef validate_model(model, criterion, valid_loader):\n    model.eval()  # D\xe9finissez le mod\xe8le en mode d'\xe9valuation\n\n    valid_loss = 0.0\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad():  # D\xe9sactivez la gradation pour l'\xe9valuation, \xe9conomisez de la m\xe9moire et des calculs\n        for inputs, targets in valid_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Avance directe\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            # Mettre \xe0 jour les statistiques\n            valid_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct_predictions += predicted.eq(targets).sum().item()\n            total_predictions += targets.size(0)\n\n    # Calculer la perte et l'exactitude.\n    epoch_loss = valid_loss / len(valid_loader)\n    epoch_acc = (correct_predictions / total_predictions) * 100.0\n    return epoch_loss, epoch_acc\n\naggregation_function = aggregation['maxpool']  \ntrain_dataloader, valid_dataloader, test_dataloader = create_data_loader(aggregation_function)\n\n# Lancer la formation et la validation\nhistory = train_model(model, criterion, optimizer, train_dataloader, valid_dataloader, num_epochs)\n\n# Sauvegardez le mod\xe8le apr\xe8s l'entra\xeenement\ntorch.save({'model': model, 'history': history}, 'model/model_checkpoint.pth') \n"})}),"\n",(0,s.jsx)(n.p,{children:"J'obtiens une exactitude qui reste toujours constante."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-txt",children:"Epoch 1/30, Loss: 97.7849, Accuracy: 37.01%, Val_Loss: 1.7463, Val_Accuracy: 39.55%\nEpoch 2/30, Loss: 1.7395, Accuracy: 38.71%, Val_Loss: 1.7425, Val_Accuracy: 39.55%\nEpoch 3/30, Loss: 1.7393, Accuracy: 38.71%, Val_Loss: 1.7409, Val_Accuracy: 39.55%\nEpoch 4/30, Loss: 1.7422, Accuracy: 38.71%, Val_Loss: 1.7446, Val_Accuracy: 39.55%\nEpoch 5/30, Loss: 1.7387, Accuracy: 38.71%, Val_Loss: 1.7170, Val_Accuracy: 39.55%\nEpoch 6/30, Loss: 1.7367, Accuracy: 38.71%, Val_Loss: 1.7313, Val_Accuracy: 39.55%\nEpoch 7/30, Loss: 1.7369, Accuracy: 38.71%, Val_Loss: 1.7203, Val_Accuracy: 39.55%\nEpoch 8/30, Loss: 1.7412, Accuracy: 38.71%, Val_Loss: 1.7021, Val_Accuracy: 39.55%\nEpoch 9/30, Loss: 1.7495, Accuracy: 37.90%, Val_Loss: 1.7294, Val_Accuracy: 39.55%\nEpoch 10/30, Loss: 1.7406, Accuracy: 38.71%, Val_Loss: 1.7475, Val_Accuracy: 39.55%\nEpoch 11/30, Loss: 1.7460, Accuracy: 38.71%, Val_Loss: 1.7162, Val_Accuracy: 39.55%\nEpoch 12/30, Loss: 1.7354, Accuracy: 38.71%, Val_Loss: 1.7157, Val_Accuracy: 39.55%\nEpoch 13/30, Loss: 1.7433, Accuracy: 38.71%, Val_Loss: 1.6990, Val_Accuracy: 39.55%\nEpoch 14/30, Loss: 1.7418, Accuracy: 38.71%, Val_Loss: 1.7800, Val_Accuracy: 39.55%\nEpoch 15/30, Loss: 1.7409, Accuracy: 38.71%, Val_Loss: 1.7223, Val_Accuracy: 39.55%\nEpoch 16/30, Loss: 1.7407, Accuracy: 38.71%, Val_Loss: 1.7235, Val_Accuracy: 39.55%\nEpoch 17/30, Loss: 1.7351, Accuracy: 38.71%, Val_Loss: 1.7356, Val_Accuracy: 39.55%\nEpoch 18/30, Loss: 1.7379, Accuracy: 38.71%, Val_Loss: 1.7161, Val_Accuracy: 39.55%\nEpoch 19/30, Loss: 1.7433, Accuracy: 38.71%, Val_Loss: 1.7237, Val_Accuracy: 39.55%\nEpoch 20/30, Loss: 1.7401, Accuracy: 38.71%, Val_Loss: 1.7202, Val_Accuracy: 39.55%\nEpoch 21/30, Loss: 1.7397, Accuracy: 38.71%, Val_Loss: 1.7337, Val_Accuracy: 39.55%\nEpoch 22/30, Loss: 1.7364, Accuracy: 38.71%, Val_Loss: 1.7321, Val_Accuracy: 39.55%\nEpoch 23/30, Loss: 1.7380, Accuracy: 38.71%, Val_Loss: 1.7309, Val_Accuracy: 39.55%\nEpoch 24/30, Loss: 1.7409, Accuracy: 38.71%, Val_Loss: 1.7237, Val_Accuracy: 39.55%\nEpoch 25/30, Loss: 1.7340, Accuracy: 38.71%, Val_Loss: 1.7202, Val_Accuracy: 39.55%\nEpoch 26/30, Loss: 1.7393, Accuracy: 38.71%, Val_Loss: 1.7026, Val_Accuracy: 39.55%\nEpoch 27/30, Loss: 1.7371, Accuracy: 38.71%, Val_Loss: 1.7187, Val_Accuracy: 39.55%\nEpoch 28/30, Loss: 1.7389, Accuracy: 38.71%, Val_Loss: 1.7117, Val_Accuracy: 39.55%\nEpoch 29/30, Loss: 1.7364, Accuracy: 38.71%, Val_Loss: 1.7263, Val_Accuracy: 39.55%\nEpoch 30/30, Loss: 1.7358, Accuracy: 38.71%, Val_Loss: 1.7383, Val_Accuracy: 39.55%\n"})})]})}function _(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>i,a:()=>r});var s=t(67294);const a={},o=s.createContext(a);function r(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);