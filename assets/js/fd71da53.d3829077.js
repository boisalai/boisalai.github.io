"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[1272],{43041:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>h,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>t,toc:()=>o});var s=i(85893),l=i(11151);const a={sidebar_label:"Large Language Models (LLMs)",sidebar_position:4,tags:["LLM","GPT","Llama"]},r="Large Language Models (LLMs)",t={id:"references/llms/index",title:"Large Language Models (LLMs)",description:'"The limits of my language mean the limits of my world." &#x2014; Ludwig Wittgenstein',source:"@site/docs/references/llms/index.md",sourceDirName:"references/llms",slug:"/references/llms/",permalink:"/docs/references/llms/",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/references/llms/index.md",tags:[{label:"LLM",permalink:"/docs/tags/llm"},{label:"GPT",permalink:"/docs/tags/gpt"},{label:"Llama",permalink:"/docs/tags/llama"}],version:"current",sidebarPosition:4,frontMatter:{sidebar_label:"Large Language Models (LLMs)",sidebar_position:4,tags:["LLM","GPT","Llama"]},sidebar:"tutorialSidebar",previous:{title:"MLX: Array framework for Apple silicon",permalink:"/docs/references/ml/mlx/"},next:{title:"Llama",permalink:"/docs/references/llms/llama"}},h={},o=[{value:"Architectures",id:"architectures",level:2},{value:"Models",id:"models",level:2},{value:"BERT",id:"bert",level:3},{value:"Databricks Dolly",id:"databricks-dolly",level:3},{value:"EleutherAI Pythia",id:"eleutherai-pythia",level:3},{value:"Facebook Llama",id:"facebook-llama",level:3},{value:"Mistral-7B",id:"mistral-7b",level:3},{value:"Mosaic MPT",id:"mosaic-mpt",level:3},{value:"OpenAI GPT",id:"openai-gpt",level:3},{value:"Poe",id:"poe",level:3},{value:"Stanford Alpaca",id:"stanford-alpaca",level:3},{value:"TII Falcon",id:"tii-falcon",level:3},{value:"StableLM",id:"stablelm",level:3},{value:"Vicuna",id:"vicuna",level:3},{value:"WizardLM",id:"wizardlm",level:3},{value:"TinyLlama",id:"tinyllama",level:3},{value:"ChatBot",id:"chatbot",level:2},{value:"Code LLMs",id:"code-llms",level:2},{value:"Leaderboard",id:"leaderboard",level:2},{value:"Common NLP tasks",id:"common-nlp-tasks",level:2},{value:"Choose the right LLM",id:"choose-the-right-llm",level:2},{value:"Training LLMs",id:"training-llms",level:2},{value:"Fine-tuning",id:"fine-tuning",level:3},{value:"Embeddings",id:"embeddings",level:2},{value:"Techniques",id:"techniques",level:2},{value:"Tools",id:"tools",level:2},{value:"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain",id:"\ufe0f-langchain",level:3},{value:"LlamaIndex",id:"llamaindex",level:3},{value:"Others",id:"others",level:3},{value:"Evaluation",id:"evaluation",level:2},{value:"Papers",id:"papers",level:2},{value:"Instructions",id:"instructions",level:2},{value:"Using Llama 2 on Mac M1/M2",id:"using-llama-2-on-mac-m1m2",level:3},{value:"Courses",id:"courses",level:2},{value:"LLM Blogs",id:"llm-blogs",level:2},{value:"See also",id:"see-also",level:2}];function c(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"large-language-models-llms",children:"Large Language Models (LLMs)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.em,{children:'"The limits of my language mean the limits of my world."'})," \u2014 Ludwig Wittgenstein"]}),"\n",(0,s.jsx)(n.h2,{id:"architectures",children:"Architectures"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1706.03762",children:"Transformer"})," (2017)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2312.00752",children:"Mamba: Linear-Time Sequence Modeling with Selective State Spaces"})," and ",(0,s.jsx)(n.a,{href:"https://huggingface.co/papers/2312.00752",children:"here"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/state-spaces/mamba",children:"Mamba"}),"."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.youtube.com/watch?v=9dSkvxS2EB0",children:"Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Paper Explained)"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"models",children:"Models"}),"\n",(0,s.jsx)(n.p,{children:"Large language models (LLMs) refer to Transformer language models that contain hundreds of billions (or\nmore) of parameters, which are trained on massive text data."}),"\n",(0,s.jsxs)(n.p,{children:["See also ",(0,s.jsx)(n.a,{href:"https://crfm.stanford.edu/ecosystem-graphs/index.html",children:"Table of LLMs"}),"\nand this ",(0,s.jsx)(n.a,{href:"https://www.promptingguide.ai/models/collection",children:"LLM Collection"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"bert",children:"BERT"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/togethercomputer/m2-bert-80M-2k-retrieval",children:"togethercomputer/m2-bert-80M-2k-retrieval"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/togethercomputer/m2-bert-80M-8k-retrieval",children:"togethercomputer/m2-bert-80M-8k-retrieval"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/togethercomputer/m2-bert-80M-32k-retrieval",children:"togethercomputer/m2-bert-80M-32k-retrieval"})}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"databricks-dolly",children:"Databricks Dolly"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html",children:"Dolly v1"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm",children:"Dolly v2"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/datasets/databricks/databricks-dolly-15k",children:"databricks/databricks-dolly-15k"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"eleutherai-pythia",children:"EleutherAI Pythia"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/EleutherAI/pythia-12b",children:"EleutherAI/pythia-12b"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"facebook-llama",children:"Facebook Llama"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2302.13971",children:"LLaMA: Open and Efficient Foundation Language Models"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://ai.meta.com/llama/",children:"Meta AI Introducing Llama 2"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/papers/2307.09288",children:"Llama 2 Research Paper"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"Llama 2 on Hugging Face"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/facebookresearch/llama-recipes/tree/main",children:"Meta Examples and recipes for Llama model"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://labs.perplexity.ai/?utm_content=first_codellama&s=u&utm_source=twitter&utm_campaign=labs",children:"LLaMa Chat"})," on Perplexity.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.mosaicml.com/blog/llama2-inference",children:"Introducing Llama2-70B-Chat with MosaicML Inference"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Code Llama","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/codellama",children:"Code Llama"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/blog/codellama",children:"Llama 2 learns to code"}),"."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2308.12950.pdf",children:"Code Llama: Open Foundation Models for Code"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/facebookresearch/codellama",children:"Llama repository"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Reddit","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.reddit.com/r/LocalLLaMA/",children:"r/LocalLLaMA/"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.reddit.com/r/LocalLLaMA/wiki/models/",children:"r/LocalLLaMA/wiki/models/"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://ai.meta.com/resources/models-and-libraries/llama-downloads/",children:"Request access"})," to the next version of Llama."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"mistral-7b",children:"Mistral-7B"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca",children:"Open-Orca/Mistral-7B-OpenOrca"})}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://medium.com/@andysingal/mistral-7b-instruct-conversational-genius-redefined-542a841c8635",children:"Mistral-7B-Instruct"})," is designed to excel in two primary domains: English language tasks and coding tasks.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://medium.com/@mne/run-mistral-7b-model-on-macbook-m1-pro-with-16gb-ram-using-llama-cpp-44134694b773",children:"Run Mistral 7B Model on MacBook M1 Pro with 16GB RAM using llama.cpp"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.2",children:"mlx-community/Mistral-7B-Instruct-v0.2"})," converted to MLX format."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://towardsdatascience.com/llms-for-everyone-running-langchain-and-a-mistralai-7b-model-in-google-colab-246ca94d7c4d",children:"LLMs for Everyone: Running LangChain and a MistralAI 7B Model in Google Colab"})," from Dmitrii Eliuseev - Dec 5, 2023."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"mosaic-mpt",children:"Mosaic MPT"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.mosaicml.com/",children:"Mosaic MPT"})}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"openai-gpt",children:"OpenAI GPT"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://platform.openai.com/overview",children:"OpenAI platform"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/models/gpt-4",children:"GPT-4"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/models/gpt-3-5",children:"GPT-3.5"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"gpt-3.5-turbo"})," has been optimized for chat using the ",(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/chat",children:"Chat completions API"}),"."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://community.openai.com/tag/chat9gpt",children:"Forum"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/prompt-engineering",children:"Prompt engineering guide"})}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"poe",children:"Poe"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://poe.com/Solar-0-70b",children:"Solar"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/upstage/SOLAR-0-70b-16bit",children:"upstage/SOLAR-0-70b-16bit"})}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"stanford-alpaca",children:"Stanford Alpaca"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://crfm.stanford.edu/2023/03/13/alpaca.html",children:"Stanford Alpaca"})}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"tii-falcon",children:"TII Falcon"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/tiiuae/falcon-40b",children:"Falcon-40B"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/blog/falcon-180b",children:"Spread Your Wings: Falcon 180B is here"}),".","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Falcon 180b can be commercially used but under very restrictive conditions, excluding any "hosting use".'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"stablelm",children:"StableLM"}),"\n",(0,s.jsx)(n.h3,{id:"vicuna",children:"Vicuna"}),"\n",(0,s.jsx)(n.h3,{id:"wizardlm",children:"WizardLM"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/nlpxucan/WizardLM",children:"WizardLM repository"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0",children:"WizardLM/WizardCoder-Python-34B-V1.0"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"tinyllama",children:"TinyLlama"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://ollama.ai/library/tinyllama",children:"TinyLlama"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/papers/2401.02385",children:"Paper"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/jzhang38/TinyLlama",children:"Repo"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"chatbot",children:"ChatBot"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://platform.openai.com/playground",children:"OpenAI Playground"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://chat.openai.com/",children:"ChatGPT"})," from OpenAI."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.chatpdf.com/c/xr1xDbkr9BUTR6EnOEUWo",children:"ChatPDF"})}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.bing.com/",children:"Bing"})," from Microsoft."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.perplexity.ai/",children:"Perplexity"}),", a new chatbot based on OpenAI's ChatGPT."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://bard.google.com/",children:"Google Bard"})," from Google, currently not available in Canada."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://claude.ai/",children:"Claude.ai"})," Claude.ai is only available in the US and UK."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.phind.com/",children:"phind"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/KillianLucas/open-interpreter/",children:"Open Interpreter"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://poe.com/Solar-0-70b",children:"Poe"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"code-llms",children:"Code LLMs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.cursor.so/",children:"cursor.so"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/features/copilot",children:"GitHub Copilot"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/bigcode-project/starcoder",children:"StarCoder"})," and ",(0,s.jsx)(n.a,{href:"https://huggingface.co/blog/starcoder",children:"here"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://alphacode.deepmind.com/",children:"DeepMind AlphaCode"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://aws.amazon.com/codewhisperer/",children:"Amazon CodeWhisperer"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder",children:"WizardCoder"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/facebookresearch/codellama",children:"Code Llama"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/collections/lmstudio-ai/metaais-codellama-coding-assistant-llm-64fb1d4ab60e2c9ddd07f8e6",children:"MetaAI's CodeLlama - Coding Assistant LLM"}),": Fast, small, and capable coding model you can run locally,"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/Deci/DeciCoder-6B",children:"DeciCoder-6B"}),", ",(0,s.jsx)(n.a,{href:"https://colab.research.google.com/drive/1QRbuser0rfUiFmQbesQJLXVtBYZOlKpB",children:"here"})," and ",(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/Deci/DeciCoder-6B-Demo",children:"here"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"leaderboard",children:"Leaderboard"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a",children:"The Big Benchmarks Collection"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard",children:"\ud83e\udd17 Open LLM Leaderboard"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/optimum/llm-perf-leaderboard",children:"\ud83e\udd17 Open LLM-Perf Leaderboard \ud83c\udfcb\ufe0f"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard",children:"\u2b50 Big Code Models Leaderboard"}),"."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://crfm.stanford.edu/fmti/",children:"The Foundation Model Transparency Index"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"common-nlp-tasks",children:"Common NLP tasks"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Summarization"}),"\n",(0,s.jsx)(n.li,{children:"Sentiment analysis"}),"\n",(0,s.jsx)(n.li,{children:"Translation"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/tasks/zero-shot-classification",children:"Zero-shot classification"}),": A model is trained on a set of labeled examples but is then able to classify new examples from previously unseen classes."]}),"\n",(0,s.jsx)(n.li,{children:"Few-shot learning"}),"\n",(0,s.jsx)(n.li,{children:"Conversation / chat"}),"\n",(0,s.jsx)(n.li,{children:"Question-answering"}),"\n",(0,s.jsx)(n.li,{children:"Text classification"}),"\n",(0,s.jsx)(n.li,{children:"Text generation"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"choose-the-right-llm",children:"Choose the right LLM"}),"\n",(0,s.jsx)(n.p,{children:"There is no \u201cperfect\u201d model. Trade-offs are required."}),"\n",(0,s.jsx)(n.p,{children:"Decision criteria"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Privacy"}),"\n",(0,s.jsx)(n.li,{children:"Quality"}),"\n",(0,s.jsx)(n.li,{children:"Cost"}),"\n",(0,s.jsx)(n.li,{children:"Latency"}),"\n",(0,s.jsx)(n.li,{children:"Customizability"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"training-llms",children:"Training LLMs"}),"\n",(0,s.jsx)(n.p,{children:"There's essentially three (3) approaches to training LLMs: pre-training, fine-tuning, and LoRA."}),"\n",(0,s.jsx)(n.h3,{id:"fine-tuning",children:"Fine-tuning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://rentry.org/llm-training",children:"The Novice's LLM Training Guide"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.youtube.com/watch?v=kmkcNVvEz-k",children:"How to Fine-Tune Mistral 7B on Your Own Data"})," with QLoRA.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb",children:"Fine-tuning Mistral on your own data"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.youtube.com/watch?v=ahnGLM-RC1Y",children:"A Survey of Techniques for Maximizing LLM Performance"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?",children:"How to Fine-Tune LLMs in 2024 with Hugging Face"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"embeddings",children:"Embeddings"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://aman.ai/primers/ai/word-vectors/",children:"Word Vectors/Embeddings"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"techniques",children:"Techniques"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Tokenization: Transforming text into word-pieces."}),"\n",(0,s.jsx)(n.li,{children:"Word Embeddings: Represent words with vectors."}),"\n",(0,s.jsxs)(n.li,{children:["Parameter-efficient fine-tuning (PEFT).","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/blog/peft",children:"\ud83e\udd17 PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Additive: Keep the foundation model weights frozen and update only the new layer weights.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Soft Prompt tuning: Concatenates trainable parameters with the input embeddings."}),"\n",(0,s.jsx)(n.li,{children:"Prefix tuning: Adding tunable layer to each transformer block, rather than just the input layer."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Selective.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2106.10199",children:"BitFit"}),": Only updates bias parameters."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://aclanthology.org/2021.acl-long.378/",children:"Diff Pruning"}),': Create task-specific "diff" vectors and only updates them']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Re-parametrization: Decompose weight matrix updates into smaller-rank matrices.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2106.09685",children:"Low-Rank Adaptation (LoRA)"}),", aim to refine a relatively small subset of\nparameters, thereby minimizing resource utilization and accelerating the training cycle."]}),"\n",(0,s.jsx)(n.li,{children:"LoRA with ZeRO-3"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2205.05638",children:"(IA)"}),"."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2307.05695",children:"ReLoRA: High-Rank Training Through Low-Rank Updates"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/finetune_adapter.md",children:"Finetune with Adapters"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Faster calculations","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2205.14135",children:"Flash Attention!"}),": Calculating attention in a flash, provides drastic speedups over standard attention through clever hardware optimization."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Improving Model Footprint","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Quantization","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/timdettmers/bitsandbytes",children:"bitsandbytes"})," (4-bit quantization)."]}),"\n",(0,s.jsx)(n.li,{children:"Quantized Low-Rank Adaptation (QLoRA)"}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/optimum/concept_guides/quantization#going-further-how-do-machines-represent-numbers",children:"Quantization"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/ggerganov/ggml",children:"GGML"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GGUF"})," is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML,\nwhich is no longer supported by llama.cpp."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://towardsdatascience.com/democratizing-llms-4-bit-quantization-for-optimal-llm-inference-be30cf4e0e34",children:"Democratizing LLMs: 4-bit Quantization for Optimal LLM Inference"})}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2210.17323",children:"GPTQ"})," is a post-training quantziation method to compress LLMs.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPTQ compresses GPT models by reducing the number of bits needed to store each weight in the model, from 32 bits down to just 3-4 bits."}),"\n",(0,s.jsx)(n.li,{children:"GPTQ analyzes each layer of the model separately and approximating the weights in a way that preserves the overall accuracy."}),"\n",(0,s.jsxs)(n.li,{children:["See ",(0,s.jsx)(n.a,{href:"https://www.philschmid.de/gptq-llama",children:"Optimize open LLMs using GPTQ and Hugging Face Optimum"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/blog/gptq-integration",children:"AutoGPTQ"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Multi-LLM Inferencing","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Mixture-of-Experts (MoE) and ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2101.03961",children:"Switch Transformer"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["LLM Cascades and ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2305.05176",children:"FrugalGPT"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Multi-modal Language Models (MLLMs)","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Chain-of-Thought MLLMs"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2201.11903.pdf",children:"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html",children:"Language Models Perform Reasoning via Chain of Thought"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Chain of Density prompt"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://rentry.org/llm-training",children:"The Novice's LLM Training Guide"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"Reinforcement learning with human feedback (RLHF)"}),"\n",(0,s.jsxs)(n.li,{children:["Retrieval Augmented Generation (RAG)","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6",children:"Advanced RAG Techniques: an Illustrated Overview"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2312.10997v1",children:"Retrieval-Augmented Generation for Large Language Models: A Survey"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/Tongji-KGLLM/RAG-Survey",children:"RAG-Survey"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://aman.ai/primers/ai/RAG/",children:"NLP \u2022 Retrieval Augmented Generation"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.admonition,{title:"Low Rank Adaptation (LoRA)",type:"info",children:[(0,s.jsx)(n.p,{children:"Want to train a specialized LLM on your own data?\nThe easiest way to do this is with low rank adaptation (LoRA), but many variants of LoRA exist. Here\u2019s an overview\nof all (or at least most) of the techniques that are out there\u2026"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"LoRA"})," models the update derived for a model\u2019s weights during finetuning with a low rank decomposition, implemented\nin practice as a pair of linear projections. LoRA leaves the pretrained layers of the LLM fixed and injects a\ntrainable rank decomposition matrix into each layer of the model."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"QLoRA"})," is (arguably) the most popular LoRA variant and uses model quantization techniques to reduce memory usage\nduring finetuning while maintaining (roughly) equal levels of performance. QLoRA uses 4-bit quantization on the\npretrained model weights and trains LoRA modules on top of this. In practice, QLoRA saves memory at the cost of\nslightly-reduced training speed."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"QA-LoRA"})," is an extension of LoRA/QLoRA that further reduces the computational burden of training and deploying LLMs.\nIt does this by combining parameter-efficient finetuning with quantization (i.e., group-wise quantization applied\nduring training/inference)."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"LoftQ"})," studies a similar idea to QA-LoRA\u2014applying quantization and LoRA finetuning on a pretrained model simultaneously."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"LongLoRA"})," attempts to cheaply adapt LLMs to longer context lengths using a parameter-efficient (LoRA-based) finetuning\nscheme. In particular, we start with a pretrained model and finetune it to have a longer context length. This finetuning is made efficient by:"]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Using sparse local attention instead of dense global attention (optional at inference time)."}),"\n",(0,s.jsx)(n.li,{children:"Using LoRA (authors find that this works well for context extension)."}),"\n"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"S-LoRA"})," aims to solve the problem of deploying multiple LoRA modules that are used to adapt the same pretrained model\nto a variety of different tasks. Put simply, S-LoRA does the following to serve thousands of LoRA modules on a single GPU (or across GPUs):"]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Stores all LoRA modules in main memory."}),"\n",(0,s.jsx)(n.li,{children:"Puts modules being used to run the current query into GPU memory."}),"\n",(0,s.jsx)(n.li,{children:"Uses unified paging to allocate GPU memory and avoid fragmentation."}),"\n",(0,s.jsx)(n.li,{children:"Proposes a new tensor parallelism strategy to batch LoRA computations."}),"\n"]}),(0,s.jsx)(n.p,{children:"Many other LoRA variants exist as well\u2026"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LQ-LoRA"}),": uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MultiLoRA"}),": extension of LoRA that better handles complex multi-task learning scenarios."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LoRA-FA"}),": freezes half of the low-rank decomposition matrix (i.e., the A matrix within the product AB) to further reduce memory overhead."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tied-LoRA"}),": leverages weight tying to further improve the parameter efficiency of LoRA."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GLoRA"}),": extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer."]}),"\n"]}),(0,s.jsxs)(n.p,{children:["See also : ",(0,s.jsx)(n.a,{href:"https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft",children:"Easily Train a Specialized LLM: PEFT, LoRA, QLoRA, LLaMA-Adapter, and More"}),"."]}),(0,s.jsxs)(n.p,{children:["Source : ",(0,s.jsx)(n.a,{href:"https://x.com/cwolferesearch/status/1736795049579491751?s=20",children:"here"}),", 2023-12-18."]})]}),"\n",(0,s.jsx)(n.h2,{id:"tools",children:"Tools"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/",children:"\ud83e\udd17 Hugging Face"}),": The GitHub of Large Language Models","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/datasets",children:"Datasets"}),", pipelines, tokenizers, and ",(0,s.jsx)(n.a,{href:"https://huggingface.co/models",children:"models"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/huggingface/transformers",children:"\ud83e\udd17 Hugging Face Transformers"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/transformers_agents",children:"Transformers Agent"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.nltk.org/",children:"NLTK"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://spacy.io/",children:"SpaCy"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://radimrehurek.com/gensim/",children:"Gensim"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://pypi.org/project/openai/",children:"OpenAI"}),".","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["OpenAI ",(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference",children:"API Reference"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/hwchase17/langchain",children:"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Microsoft ",(0,s.jsx)(n.a,{href:"https://github.com/microsoft/DeepSpeed",children:"DeepSpeed"}),": Optimization library."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp",children:"llama.cpp"}),": Port of Facebook's LLaMA model in C/C++."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://ollama.ai/",children:"Ollama"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/guides/langsmith/",children:"LangSmith"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/explodinggradients/ragas",children:"Ragas"}),": Evaluation framework for Retrieval Augmented Generation (RAG) pipelines."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/bigscience-workshop/petals",children:"Petals"})," a system for inference and fine-tuning of large models\ncollaboratively by joining the resources of multiple parties [",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2209.01188.pdf",children:"Paper"}),"]."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.mendable.ai/",children:"Mendable.ai"})}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"\ufe0f-langchain",children:"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/get_started/introduction.html",children:"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain"})," is a framework\nfor developing applications powered by language models."]}),"\n",(0,s.jsx)(n.p,{children:"Released in late 2022. Useful for multi-stage reasoning, LLM-based workflows"}),"\n",(0,s.jsx)(n.p,{children:"The core idea of the library is that we can \u201cchain\u201d together different components to create more advanced use cases around LLMs. Chains may consist of multiple components from several modules:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt templates"}),": Prompt templates are templates for different types of prompts. Like \u201cchatbot\u201d style templates, ELI5 question-answering, etc"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLMs"}),": Large language models like GPT-3, BLOOM, etc"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Agents"}),": Agents use LLMs to decide what actions should be taken. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory"}),": Short-term memory, long-term memory."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"See also:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://api.python.langchain.com/en/latest/api_reference.html",children:"API Reference"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/langchain-ai/langchain",children:"Github"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://blog.langchain.dev/",children:"Blog"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.pinecone.io/learn/series/langchain/",children:"LangChain AI Handbook"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/logspace-ai/langflow",children:"\u26d3\ufe0f Langflow"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"llamaindex",children:"LlamaIndex"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://gpt-index.readthedocs.io/",children:"LlamaIndex"})," is a toolkit for building LLM-powered applications over custom data. It consists of two stages: the indexing stage, where the knowledge base is prepared using data connectors and indexes, and the querying stage, where relevant context is retrieved from the knowledge base to assist the LLM in responding to a question. LlamaIndex provides building blocks such as retrievers, node postprocessors, and response synthesizers, as well as pipelines like query engines, chat engines, and agents."]}),"\n",(0,s.jsx)(n.h3,{id:"others",children:"Others"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/microsoft/LLMLingua",children:"LLMLingua"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"evaluation",children:"Evaluation"}),"\n",(0,s.jsx)(n.p,{children:"A good language will model will have high accuracy and low perplexity.\nAccuracy = next word is right or wrong. Perplexity = how confident was that choice."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/BLEU",children:"BLEU"})," (bilingual evaluation understudy) is an algorithm for evaluating the quality of\ntext which has been machine-translated from one natural language to another."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://aclanthology.org/W04-1013.pdf",children:"ROUGE"})," for summarization.\nSee also ",(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/evaluate-metric/bleu",children:"here"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"sacreBLEU, TER, ChrF, ChrF++, BERTScore, METEOR, and Semantic Similarity"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/openai/evals",children:"Evals"})," is a framework for evaluating LLMs and LLM systems,\nand an open-source registry of benchmarks."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard",children:"\ud83e\udd17 Open LLM Leaderboard"}),".","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/blog/evaluating-mmlu-leaderboard",children:"What's going on with the Open LLM Leaderboard?"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/bigcode/multilingual-code-evals",children:"\ud83e\udd17 Multilingual Code Models Evaluation"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.mosaicml.com/llm-evaluation",children:"Mosaic LLM Evaluation Leaderboard"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/microsoft/promptbench",children:"A unified evaluation framework for large language models"}),"."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://chat.lmsys.org/?arena",children:"Chatbot Arena: Benchmarking LLMs in the Wild"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["See also ",(0,s.jsx)(n.a,{href:"https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI",children:"Local LLM Comparison & Colab Links"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"papers",children:"Papers"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Attention is all you need"}),", NIPS 2017. [",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1706.03762",children:"Paper"}),"]."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"A Survey of Large Language Models"}),", arXiv 2023. [",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2303.18223",children:"Paper"}),", ",(0,s.jsx)(n.a,{href:"https://github.com/RUCAIBox/LLMSurvey",children:"GitHub"}),"]."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"instructions",children:"Instructions"}),"\n",(0,s.jsx)(n.h3,{id:"using-llama-2-on-mac-m1m2",children:"Using Llama 2 on Mac M1/M2"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Created 2023-08-26. Last updated 2023-08-26."}),"\n"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Download the original version of",(0,s.jsx)(n.a,{href:"https://github.com/facebookresearch/llama",children:"Llama"})," and extract it to a ",(0,s.jsx)(n.code,{children:"llama-main"})," folder."]}),"\n",(0,s.jsxs)(n.li,{children:["Download the cpu version from ",(0,s.jsx)(n.a,{href:"https://github.com/krychu/llama",children:"https://github.com/krychu/llama"}),", extract it and replace files in the ",(0,s.jsx)(n.code,{children:"llama-main"})," folder."]}),"\n",(0,s.jsxs)(n.li,{children:["Go to the ",(0,s.jsx)(n.code,{children:"llama-main"})," folder."]}),"\n",(0,s.jsxs)(n.li,{children:["Follow the instructions in the ",(0,s.jsx)(n.a,{href:"https://github.com/facebookresearch/llama",children:"README"})," to run the ",(0,s.jsx)(n.code,{children:"download.sh"})," script.\n",(0,s.jsx)(n.a,{href:"https://ai.meta.com/resources/models-and-libraries/llama-downloads/",children:"Request a new download link"}),".\nThen run the ",(0,s.jsx)(n.code,{children:"download.sh"})," script, passing the URL provided when prompted to start the download."]}),"\n",(0,s.jsxs)(n.li,{children:["Create a virtual environment with ",(0,s.jsx)(n.code,{children:"python3 -m venv env"})," and activate it with ",(0,s.jsx)(n.code,{children:"source env/bin/activate"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Install the cpu version of pytorch with ",(0,s.jsx)(n.code,{children:"python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Install dependencies of llama with ",(0,s.jsx)(n.code,{children:"python3 -m pip install -e ."}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Run ",(0,s.jsx)(n.code,{children:"torchrun"})," like below."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"$ git clone https://github.com/facebookresearch/llama\n$ mv llama llama-main\n$ git clone https://github.com/krychu/llama\n$ cp -r llama/ llama-main/\n$ cd llama-main\n$ chmod u+x download.sh\n$ ./download.sh\nEnter the URL from email: https://download.llamameta.net/*?Policy=eyJTdGF0Z...\n$ python3 -m venv env\n$ source env/bin/activate\n$ python3 -m pip install torch torchvision torchaudio \\\n  --index-url https://download.pytorch.org/whl/cpu\n$ python3 -m pip install -e .\n$ torchrun --nproc_per_node 1 \\\n  example_text_completion.py \\\n  --ckpt_dir llama-2-7b/ \\\n  --tokenizer_path tokenizer.model \\\n  --max_seq_len 128 --max_batch_size 1 #(instead of 4)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["I get a failure with this message: ",(0,s.jsx)(n.code,{children:"RuntimeError: Distributed package doesn't have NCCL built in"})]}),"\n",(0,s.jsxs)(n.p,{children:["Instead, people are using ",(0,s.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp",children:"https://github.com/ggerganov/llama.cpp"}),", which is a Port of Facebook's LLaMA model in C/C++."]}),"\n",(0,s.jsxs)(n.p,{children:["Found this link ",(0,s.jsx)(n.a,{href:"https://gist.github.com/cedrickchee/e8d4cb0c4b1df6cc47ce8b18457ebde0",children:"https://gist.github.com/cedrickchee/e8d4cb0c4b1df6cc47ce8b18457ebde0"}),". Will try this."]}),"\n",(0,s.jsx)(n.p,{children:"Reference:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/facebookresearch/llama",children:"https://github.com/facebookresearch/llama"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/facebookresearch/llama/issues/433#issuecomment-1650002750",children:"https://github.com/facebookresearch/llama/issues/433#issuecomment-1650002750"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/krychu/llama",children:"https://github.com/krychu/llama"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/aggiee/llama-v2-mps",children:"https://github.com/aggiee/llama-v2-mps"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://ryandam.net/blog/2023/8/2/using-llama2/index.html",children:"https://ryandam.net/blog/2023/8/2/using-llama2/index.html"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"courses",children:"Courses"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/",children:"The Full Stack LLM Bootcamp"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Standford ",(0,s.jsx)(n.a,{href:"https://stanford-cs324.github.io/winter2022/",children:"CS324 - Large Language Models"}),"."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.cohere.com/docs/llmu",children:"cohere"})}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://learnprompting.org/",children:"Learn Prompting"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.wandb.courses/courses/building-llm-powered-apps",children:"Building LLM-Powered Apps"})," from WandB."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://learn.activeloop.ai/courses/langchain",children:"LangChain & Vector Databases in Production"})," from ActiveLoop."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://rentry.org/llm-training",children:"The Novice's LLM Training Guide"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/mlabonne/llm-course?tab=readme-ov-file",children:"llm-course"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"llm-blogs",children:"LLM Blogs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://eugeneyan.com/",children:"Eugene Yan"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huyenchip.com/",children:"Chip Huyen"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://medium.com/@iamleonie",children:"Leonie Monigatti"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://lilianweng.github.io/",children:"Lilian Weng"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://logankilpatrick.medium.com/",children:"Logan Kilpatrick"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://magazine.sebastianraschka.com/",children:"Sebastian Raschka"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"see-also",children:"See also"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/blog/2023-in-llms",children:"2023, year of open LLMs"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,l.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},11151:(e,n,i)=>{i.d(n,{Z:()=>t,a:()=>r});var s=i(67294);const l={},a=s.createContext(l);function r(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);