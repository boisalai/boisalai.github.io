"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9040],{3905:(e,a,t)=>{t.d(a,{Zo:()=>s,kt:()=>d});var l=t(7294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function n(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);a&&(l=l.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,l)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?n(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):n(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,l,r=function(e,a){if(null==e)return{};var t,l,r={},n=Object.keys(e);for(l=0;l<n.length;l++)t=n[l],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(l=0;l<n.length;l++)t=n[l],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var p=l.createContext({}),m=function(e){var a=l.useContext(p),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},s=function(e){var a=m(e.components);return l.createElement(p.Provider,{value:a},e.children)},h="mdxType",u={inlineCode:"code",wrapper:function(e){var a=e.children;return l.createElement(l.Fragment,{},a)}},c=l.forwardRef((function(e,a){var t=e.components,r=e.mdxType,n=e.originalType,p=e.parentName,s=o(e,["components","mdxType","originalType","parentName"]),h=m(t),c=r,d=h["".concat(p,".").concat(c)]||h[c]||u[c]||n;return t?l.createElement(d,i(i({ref:a},s),{},{components:t})):l.createElement(d,i({ref:a},s))}));function d(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var n=t.length,i=new Array(n);i[0]=c;var o={};for(var p in a)hasOwnProperty.call(a,p)&&(o[p]=a[p]);o.originalType=e,o[h]="string"==typeof e?e:r,i[1]=o;for(var m=2;m<n;m++)i[m]=t[m];return l.createElement.apply(null,i)}return l.createElement.apply(null,t)}c.displayName="MDXCreateElement"},4947:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>p,contentTitle:()=>i,default:()=>u,frontMatter:()=>n,metadata:()=>o,toc:()=>m});var l=t(7462),r=(t(7294),t(3905));const n={sidebar_label:"Large Language Models (LLMs)",sidebar_position:4,tags:["LLM","GPT","Llama"]},i="Large Language Models (LLMs)",o={unversionedId:"references/llm",id:"references/llm",title:"Large Language Models (LLMs)",description:'"The limits of my language mean the limits of my world." &#x2014; Ludwig Wittgenstein',source:"@site/docs/references/llm.md",sourceDirName:"references",slug:"/references/llm",permalink:"/docs/references/llm",draft:!1,tags:[{label:"LLM",permalink:"/docs/tags/llm"},{label:"GPT",permalink:"/docs/tags/gpt"},{label:"Llama",permalink:"/docs/tags/llama"}],version:"current",sidebarPosition:4,frontMatter:{sidebar_label:"Large Language Models (LLMs)",sidebar_position:4,tags:["LLM","GPT","Llama"]},sidebar:"tutorialSidebar",previous:{title:"SAS",permalink:"/docs/references/sas"},next:{title:"Code Snippets",permalink:"/docs/category/code-snippets"}},p={},m=[{value:"Models",id:"models",level:2},{value:"Open Source",id:"open-source",level:3},{value:"Stanford AAlpaca",id:"stanford-aalpaca",level:4},{value:"Databricks Dolly",id:"databricks-dolly",level:4},{value:"EleutherAI Pythia",id:"eleutherai-pythia",level:4},{value:"Facebook Llama",id:"facebook-llama",level:4},{value:"TII Falcon",id:"tii-falcon",level:4},{value:"Mosaic MPT",id:"mosaic-mpt",level:4},{value:"StableLM",id:"stablelm",level:4},{value:"Vicuna",id:"vicuna",level:4},{value:"WizardLM",id:"wizardlm",level:4},{value:"Proprietary",id:"proprietary",level:3},{value:"GPT",id:"gpt",level:4},{value:"ChatBot",id:"chatbot",level:2},{value:"Code LLMs",id:"code-llms",level:2},{value:"Leaderboard",id:"leaderboard",level:2},{value:"Common NLP tasks",id:"common-nlp-tasks",level:2},{value:"Choose the right LLM",id:"choose-the-right-llm",level:2},{value:"Training LLMs",id:"training-llms",level:2},{value:"Techniques",id:"techniques",level:2},{value:"Tools",id:"tools",level:2},{value:"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain",id:"\ufe0f-langchain",level:3},{value:"Vector Stores",id:"vector-stores",level:3},{value:"Evaluation",id:"evaluation",level:2},{value:"Papers",id:"papers",level:2},{value:"Instructions",id:"instructions",level:2},{value:"Using Llama 2 on Mac M1/M2",id:"using-llama-2-on-mac-m1m2",level:3},{value:"Courses",id:"courses",level:2}],s={toc:m},h="wrapper";function u(e){let{components:a,...t}=e;return(0,r.kt)(h,(0,l.Z)({},s,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"large-language-models-llms"},"Large Language Models (LLMs)"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},'"The limits of my language mean the limits of my world."')," ","\u2014"," Ludwig Wittgenstein"),(0,r.kt)("h2",{id:"models"},"Models"),(0,r.kt)("p",null,"Large language models (LLMs) refer to Transformer language models that contain hundreds of billions (or\nmore) of parameters, which are trained on massive text data."),(0,r.kt)("p",null,"See also ",(0,r.kt)("a",{parentName:"p",href:"https://crfm.stanford.edu/ecosystem-graphs/index.html"},"Table of LLMs"),"."),(0,r.kt)("h3",{id:"open-source"},"Open Source"),(0,r.kt)("h4",{id:"stanford-aalpaca"},"Stanford AAlpaca"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://crfm.stanford.edu/2023/03/13/alpaca.html"},"https://crfm.stanford.edu/2023/03/13/alpaca.html"))),(0,r.kt)("h4",{id:"databricks-dolly"},"Databricks Dolly"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html"},"Dolly v1"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm"},"Dolly v2"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/datasets/databricks/databricks-dolly-15k"},"databricks/databricks-dolly-15k"),".")),(0,r.kt)("h4",{id:"eleutherai-pythia"},"EleutherAI Pythia"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/EleutherAI/pythia-12b"},"EleutherAI/pythia-12b"),".")),(0,r.kt)("h4",{id:"facebook-llama"},"Facebook Llama"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2302.13971"},"LLaMA: Open and Efficient Foundation Language Models"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://ai.meta.com/llama/"},"Meta AI Introducing Llama 2"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/papers/2307.09288"},"Llama 2 Research Paper"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/meta-llama"},"Llama 2 on Hugging Face"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/llama-recipes/tree/main"},"Meta Examples and recipes for Llama model"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://labs.perplexity.ai/?utm_content=first_codellama&s=u&utm_source=twitter&utm_campaign=labs"},"LLaMa Chat")," on Perplexity.",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.mosaicml.com/blog/llama2-inference"},"Introducing Llama2-70B-Chat with MosaicML Inference"),"."))),(0,r.kt)("li",{parentName:"ul"},"Code Llama",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/codellama"},"Code Llama"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/blog/codellama"},"Llama 2 learns to code"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/2308.12950.pdf"},"Code Llama: Open Foundation Models for Code")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/codellama"},"Llama repository")))),(0,r.kt)("li",{parentName:"ul"},"Reddit",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.reddit.com/r/LocalLLaMA/"},"r/LocalLLaMA/")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.reddit.com/r/LocalLLaMA/wiki/models/"},"r/LocalLLaMA/wiki/models/")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://ai.meta.com/resources/models-and-libraries/llama-downloads/"},"Request access")," to the next version of Llama.")),(0,r.kt)("h4",{id:"tii-falcon"},"TII Falcon"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/tiiuae/falcon-40b"},"Falcon-40B"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/blog/falcon-180b"},"Spread Your Wings: Falcon 180B is here"),'.\n** Falcon 180b can be commercially used but under very restrictive conditions, excluding any "hosting use".')),(0,r.kt)("h4",{id:"mosaic-mpt"},"Mosaic MPT"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.mosaicml.com/"},"https://www.mosaicml.com/"))),(0,r.kt)("h4",{id:"stablelm"},"StableLM"),(0,r.kt)("h4",{id:"vicuna"},"Vicuna"),(0,r.kt)("h4",{id:"wizardlm"},"WizardLM"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/nlpxucan/WizardLM"},"WizardLM repository"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0"},"WizardLM/WizardCoder-Python-34B-V1.0"),".")),(0,r.kt)("h3",{id:"proprietary"},"Proprietary"),(0,r.kt)("h4",{id:"gpt"},"GPT"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://platform.openai.com/overview"},"OpenAI platform"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://platform.openai.com/docs/models/gpt-4"},"GPT-4"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://platform.openai.com/docs/models/gpt-3-5"},"GPT-3.5"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"gpt-3.5-turbo")," has been optimized for chat using the ",(0,r.kt)("a",{parentName:"li",href:"https://platform.openai.com/docs/api-reference/chat"},"Chat completions API"),". ")),(0,r.kt)("h2",{id:"chatbot"},"ChatBot"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://chat.openai.com/"},"ChatGPT")," from OpenAI."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.bing.com/"},"Bing")," from Microsoft."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.perplexity.ai/"},"Perplexity"),", a new chatbot based on OpenAI's ChatGPT."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://bard.google.com/"},"Google Bard")," from Google, currently not available in Canada."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://claude.ai/"},"Claude.ai")," Claude.ai is only available in the US and UK. "),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.phind.com/"},"phind"),".")),(0,r.kt)("h2",{id:"code-llms"},"Code LLMs"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.cursor.so/"},"cursor.so"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/features/copilot"},"GitHub Copilot"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/bigcode-project/starcoder"},"StarCoder")," and ",(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/blog/starcoder"},"here"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://alphacode.deepmind.com/"},"DeepMind AlphaCode"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://aws.amazon.com/codewhisperer/"},"Amazon CodeWhisperer"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder"},"WizardCoder"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/codellama"},"Code Llama"),".")),(0,r.kt)("h2",{id:"leaderboard"},"Leaderboard"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"},"\ud83e\udd17 Open LLM Leaderboard"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/spaces/optimum/llm-perf-leaderboard"},"\ud83e\udd17 Open LLM-Perf Leaderboard \ud83c\udfcb\ufe0f"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard"},"\u2b50 Big Code Models Leaderboard"),".")),(0,r.kt)("h2",{id:"common-nlp-tasks"},"Common NLP tasks"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Summarization"),(0,r.kt)("li",{parentName:"ul"},"Sentiment analysis"),(0,r.kt)("li",{parentName:"ul"},"Translation"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/tasks/zero-shot-classification"},"Zero-shot classification"),": A model is trained on a set of labeled examples but is then able to classify new examples from previously unseen classes."),(0,r.kt)("li",{parentName:"ul"},"Few-shot learning"),(0,r.kt)("li",{parentName:"ul"},"Conversation / chat"),(0,r.kt)("li",{parentName:"ul"},"Question-answering"),(0,r.kt)("li",{parentName:"ul"},"Text classification"),(0,r.kt)("li",{parentName:"ul"},"Text generation")),(0,r.kt)("h2",{id:"choose-the-right-llm"},"Choose the right LLM"),(0,r.kt)("p",null,"There is no \u201cperfect\u201d model. Trade-offs are required. "),(0,r.kt)("p",null,"Decision criteria"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Privacy"),(0,r.kt)("li",{parentName:"ul"},"Quality "),(0,r.kt)("li",{parentName:"ul"},"Cost"),(0,r.kt)("li",{parentName:"ul"},"Latency"),(0,r.kt)("li",{parentName:"ul"},"Customizability")),(0,r.kt)("h2",{id:"training-llms"},"Training LLMs"),(0,r.kt)("p",null,"There's essentially three (3) approaches to training LLMs: pre-training, fine-tuning, and LoRA."),(0,r.kt)("h2",{id:"techniques"},"Techniques"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Tokenization: Transforming text into word-pieces."),(0,r.kt)("li",{parentName:"ul"},"Word Embeddings: Represent words with vectors."),(0,r.kt)("li",{parentName:"ul"},"Parameter-efficient fine-tuning (PEFT).",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/blog/peft"},"\ud83e\udd17 PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware"),"."),(0,r.kt)("li",{parentName:"ul"},"Additive: Keep the foundation model weights frozen and update only the new layer weights. ",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Soft Prompt tuning: Concatenates trainable parameters with the input embeddings."),(0,r.kt)("li",{parentName:"ul"},"Prefix tuning: Adding tunable layer to each transformer block, rather than just the input layer."))),(0,r.kt)("li",{parentName:"ul"},"Selective.",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2106.10199"},"BitFit"),": Only updates bias parameters."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://aclanthology.org/2021.acl-long.378/"},"Diff Pruning"),': Create task-specific "diff" vectors and only updates them'))),(0,r.kt)("li",{parentName:"ul"},"Re-parametrization: Decompose weight matrix updates into smaller-rank matrices.",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2106.09685"},"Low-Rank Adaptation (LoRA)"),", aim to refine a relatively small subset of\nparameters, thereby minimizing resource utilization and accelerating the training cycle."),(0,r.kt)("li",{parentName:"ul"},"LoRA with ZeRO-3"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2205.05638"},"(IA)",(0,r.kt)("sup",null,"3")),"."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/finetune_adapter.md"},"Finetune with Adapters"),"."))),(0,r.kt)("li",{parentName:"ul"},"Faster calculations",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2205.14135"},"Flash Attention!"),": Calculating attention in a flash."))),(0,r.kt)("li",{parentName:"ul"},"Improving Model Footprint",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Quantization ",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/timdettmers/bitsandbytes"},"bitsandbytes")," (4-bit quantization)."),(0,r.kt)("li",{parentName:"ul"},"Quantized Low-Rank Adaptation (QLoRA)"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2210.17323"},"GPTQ")," is a post-training quantziation method to compress LLMs.",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"GPTQ compresses GPT models by reducing the number of bits needed to store each weight in the model, from 32 bits down to just 3-4 bits. "),(0,r.kt)("li",{parentName:"ul"},"GPTQ analyzes each layer of the model separately and approximating the weights in a way that preserves the overall accuracy."),(0,r.kt)("li",{parentName:"ul"},"See ",(0,r.kt)("a",{parentName:"li",href:"https://www.philschmid.de/gptq-llama"},"Optimize open LLMs using GPTQ and Hugging Face Optimum"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/blog/gptq-integration"},"AutoGPTQ"),"."))))),(0,r.kt)("li",{parentName:"ul"},"Multi-LLM Inferencing",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Mixture-of-Experts (MoE) and ",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2101.03961"},"Switch Transformer"),"."),(0,r.kt)("li",{parentName:"ul"},"LLM Cascades and ",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2305.05176"},"FrugalGPT"),"."))),(0,r.kt)("li",{parentName:"ul"},"Multi-modal Language Models (MLLMs)",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Chain-of-Thought MLLMs"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/2201.11903.pdf"},"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html"},"Language Models Perform Reasoning via Chain of Thought"),"."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://rentry.org/llm-training"},"The Novice's LLM Training Guide"),".  "),(0,r.kt)("li",{parentName:"ul"},"Reinforcement learning with human feedback (RLHF)"),(0,r.kt)("li",{parentName:"ul"},"Retrieval Augmented Generation (RAG)")),(0,r.kt)("h2",{id:"tools"},"Tools"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/"},"\ud83e\udd17 Hugging Face"),": The GitHub of Large Language Models",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/datasets"},"Datasets"),", pipelines, tokenizers, and ",(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/models"},"models"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/huggingface/transformers"},"\ud83e\udd17 Hugging Face Transformers"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/docs/transformers/transformers_agents"},"Transformers Agent"),"."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.nltk.org/"},"NLTK"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://spacy.io/"},"SpaCy"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://radimrehurek.com/gensim/"},"Gensim"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://pypi.org/project/openai/"},"OpenAI"),".",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"OpenAI ",(0,r.kt)("a",{parentName:"li",href:"https://platform.openai.com/docs/api-reference"},"API Reference"),"."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/hwchase17/langchain"},"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain"),"."),(0,r.kt)("li",{parentName:"ul"},"Microsoft ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/microsoft/DeepSpeed"},"DeepSpeed"),": Optimization library. "),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/ggerganov/llama.cpp"},"llama.cpp"),": Port of Facebook's LLaMA model in C/C++."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://ollama.ai/"},"Ollama"),"."),(0,r.kt)("li",{parentName:"ul"},"Vector stores (databases, libraries, plugins).",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.trychroma.com/"},"Chroma"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.pinecone.io/"},"Pinecone"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/pgvector/pgvector"},"PGVector"),"."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://python.langchain.com/docs/guides/langsmith/"},"LangSmith"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/explodinggradients/ragas"},"Ragas"),": Evaluation framework for Retrieval Augmented Generation (RAG) pipelines. "),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/bigscience-workshop/petals"},"Petals")," a system for inference and fine-tuning of large models\ncollaboratively by joining the resources of multiple parties [",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/2209.01188.pdf"},"Paper"),"].")),(0,r.kt)("h3",{id:"\ufe0f-langchain"},"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://python.langchain.com/docs/get_started/introduction.html"},"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain")," is a framework\nfor developing applications powered by language models."),(0,r.kt)("p",null,"Released in late 2022. Useful for multi-stage reasoning, LLM-based workflows"),(0,r.kt)("p",null,"The core idea of the library is that we can \u201cchain\u201d together different components to create more advanced use cases around LLMs. Chains may consist of multiple components from several modules:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Prompt templates"),": Prompt templates are templates for different types of prompts. Like \u201cchatbot\u201d style templates, ELI5 question-answering, etc"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"LLMs"),": Large language models like GPT-3, BLOOM, etc"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Agents"),": Agents use LLMs to decide what actions should be taken. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Memory"),": Short-term memory, long-term memory.")),(0,r.kt)("p",null,"See also:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://api.python.langchain.com/en/latest/api_reference.html"},"API Reference"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/langchain-ai/langchain"},"Github"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://blog.langchain.dev/"},"Blog"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.pinecone.io/learn/series/langchain/"},"LangChain AI Handbook"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/logspace-ai/langflow"},"\u26d3\ufe0f Langflow"),".")),(0,r.kt)("h3",{id:"vector-stores"},"Vector Stores"),(0,r.kt)("p",null,"Using a vector store requires setting up an indexing pipeline to load data from sources (a website, a file, etc.),\ntransform the data into documents, embed those documents, and insert the embeddings and documents into the vector store."),(0,r.kt)("h2",{id:"evaluation"},"Evaluation"),(0,r.kt)("p",null,"A good language will model will have high accuracy and low perplexity.\nAccuracy = next word is right or wrong. Perplexity = how confident was that choice. "),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/BLEU"},"BLEU")," (bilingual evaluation understudy) is an algorithm for evaluating the quality of\ntext which has been machine-translated from one natural language to another."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://aclanthology.org/W04-1013.pdf"},"ROUGE")," for summarization.\nSee also ",(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/spaces/evaluate-metric/bleu"},"here"),"."),(0,r.kt)("li",{parentName:"ul"},"sacreBLEU, TER, ChrF, ChrF++, BERTScore, METEOR, and Semantic Similarity"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/openai/evals"},"Evals")," is a framework for evaluating LLMs and LLM systems,\nand an open-source registry of benchmarks."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"},"\ud83e\udd17 Open LLM Leaderboard"),".",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/blog/evaluating-mmlu-leaderboard"},"What's going on with the Open LLM Leaderboard?"),"."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/spaces/bigcode/multilingual-code-evals"},"\ud83e\udd17 Multilingual Code Models Evaluation"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.mosaicml.com/llm-evaluation"},"Mosaic LLM Evaluation Leaderboard"),".")),(0,r.kt)("h2",{id:"papers"},"Papers"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Attention is all you need"),", NIPS 2017. [",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1706.03762"},"Paper"),"]."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"A Survey of Large Language Models"),", arXiv 2023. [",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2303.18223"},"Paper"),", ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/RUCAIBox/LLMSurvey"},"GitHub"),"].")),(0,r.kt)("h2",{id:"instructions"},"Instructions"),(0,r.kt)("h3",{id:"using-llama-2-on-mac-m1m2"},"Using Llama 2 on Mac M1/M2"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Created 2023-08-26. Last updated 2023-08-26.")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Download the original version of Llama from ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/llama"},"https://github.com/facebookresearch/llama")," and extract it to a ",(0,r.kt)("inlineCode",{parentName:"li"},"llama-main")," folder."),(0,r.kt)("li",{parentName:"ol"},"Download the cpu version from ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/krychu/llama"},"https://github.com/krychu/llama"),", extract it and replace files in the ",(0,r.kt)("inlineCode",{parentName:"li"},"llama-main")," folder."),(0,r.kt)("li",{parentName:"ol"},"Go to the ",(0,r.kt)("inlineCode",{parentName:"li"},"llama-main")," folder."),(0,r.kt)("li",{parentName:"ol"},"Follow the instructions in the ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/llama"},"README")," to run the ",(0,r.kt)("inlineCode",{parentName:"li"},"download.sh")," script.\n",(0,r.kt)("a",{parentName:"li",href:"https://ai.meta.com/resources/models-and-libraries/llama-downloads/"},"Request a new download link"),".\nThen run the ",(0,r.kt)("inlineCode",{parentName:"li"},"download.sh")," script, passing the URL provided when prompted to start the download. "),(0,r.kt)("li",{parentName:"ol"},"Create a virtual environment with ",(0,r.kt)("inlineCode",{parentName:"li"},"python3 -m venv env")," and activate it with ",(0,r.kt)("inlineCode",{parentName:"li"},"source env/bin/activate"),"."),(0,r.kt)("li",{parentName:"ol"},"Install the cpu version of pytorch with ",(0,r.kt)("inlineCode",{parentName:"li"},"python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"),"."),(0,r.kt)("li",{parentName:"ol"},"Install dependencies of llama with ",(0,r.kt)("inlineCode",{parentName:"li"},"python3 -m pip install -e ."),"."),(0,r.kt)("li",{parentName:"ol"},"Run ",(0,r.kt)("inlineCode",{parentName:"li"},"torchrun")," like below.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ git clone https://github.com/facebookresearch/llama\n$ mv llama llama-main\n$ git clone https://github.com/krychu/llama\n$ cp -r llama/ llama-main/\n$ cd llama-main \n$ chmod u+x download.sh\n$ ./download.sh\nEnter the URL from email: https://download.llamameta.net/*?Policy=eyJTdGF0Z...\n$ python3 -m venv env\n$ source env/bin/activate\n$ python3 -m pip install torch torchvision torchaudio \\\n  --index-url https://download.pytorch.org/whl/cpu\n$ python3 -m pip install -e .\n$ torchrun --nproc_per_node 1 \\\n  example_text_completion.py \\\n  --ckpt_dir llama-2-7b/ \\\n  --tokenizer_path tokenizer.model \\\n  --max_seq_len 128 --max_batch_size 1 #(instead of 4)\n")),(0,r.kt)("p",null,"I get a failure with this message: ",(0,r.kt)("inlineCode",{parentName:"p"},"RuntimeError: Distributed package doesn't have NCCL built in")),(0,r.kt)("p",null,"Instead, people are using ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ggerganov/llama.cpp"},"https://github.com/ggerganov/llama.cpp"),", which is a Port of Facebook's LLaMA model in C/C++."),(0,r.kt)("p",null,"Found this link ",(0,r.kt)("a",{parentName:"p",href:"https://gist.github.com/cedrickchee/e8d4cb0c4b1df6cc47ce8b18457ebde0"},"https://gist.github.com/cedrickchee/e8d4cb0c4b1df6cc47ce8b18457ebde0"),". Will try this."),(0,r.kt)("p",null,"Reference:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/llama"},"https://github.com/facebookresearch/llama")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/llama/issues/433#issuecomment-1650002750"},"https://github.com/facebookresearch/llama/issues/433#issuecomment-1650002750")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/krychu/llama"},"https://github.com/krychu/llama")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/aggiee/llama-v2-mps"},"https://github.com/aggiee/llama-v2-mps")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://ryandam.net/blog/2023/8/2/using-llama2/index.html"},"https://ryandam.net/blog/2023/8/2/using-llama2/index.html"))),(0,r.kt)("h2",{id:"courses"},"Courses"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Standford ",(0,r.kt)("a",{parentName:"li",href:"https://stanford-cs324.github.io/winter2022/"},"CS324 - Large Language Models"),".")))}u.isMDXComponent=!0}}]);