"use strict";(self.webpackChunkmy_doc=self.webpackChunkmy_doc||[]).push([[4918],{24182:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>l});var o=t(85893),a=t(11151);const s={},r="LlamaIndex",i={id:"references/llms/llama-index",title:"LlamaIndex",description:"Loading data and creating an index",source:"@site/docs/references/llms/llama-index.md",sourceDirName:"references/llms",slug:"/references/llms/llama-index",permalink:"/docs/references/llms/llama-index",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/references/llms/llama-index.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain",permalink:"/docs/references/llms/langchain/"},next:{title:"Phi-2",permalink:"/docs/references/llms/phi-2"}},d={},l=[{value:"Loading data and creating an index",id:"loading-data-and-creating-an-index",level:2},{value:"Running a query",id:"running-a-query",level:2},{value:"Saving and loading the context",id:"saving-and-loading-the-context",level:2},{value:"Chatbot",id:"chatbot",level:2},{value:"Discover LlamaIndex",id:"discover-llamaindex",level:2},{value:"Basic examples",id:"basic-examples",level:3},{value:"QA and Refine Template",id:"qa-and-refine-template",level:3},{value:"Chat Example",id:"chat-example",level:3},{value:"Documents/Nodes",id:"documentsnodes",level:3},{value:"Evaluation Baseline",id:"evaluation-baseline",level:3},{value:"Loading our Docs",id:"loading-our-docs",level:4},{value:"Create the indicies",id:"create-the-indicies",level:3},{value:"Create Query Engine Tools",id:"create-query-engine-tools",level:3},{value:"Create Unified Query Engine",id:"create-unified-query-engine",level:3},{value:"Test the Query Engine!",id:"test-the-query-engine",level:3},{value:"Generate the Dataset",id:"generate-the-dataset",level:3},{value:"Evaluate with the Dataset",id:"evaluate-with-the-dataset",level:3},{value:"Embeddings",id:"embeddings",level:3},{value:"Custom Embeddings",id:"custom-embeddings",level:3},{value:"JSON Query Engine",id:"json-query-engine",level:3},{value:"Answering complex queries",id:"answering-complex-queries",level:3},{value:"Joint Text to SQL and Semantic Search",id:"joint-text-to-sql-and-semantic-search",level:3},{value:"Document Management",id:"document-management",level:3},{value:"LlamaIndex Webinars",id:"llamaindex-webinars",level:2},{value:"See also",id:"see-also",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"llamaindex",children:"LlamaIndex"}),"\n",(0,o.jsx)(n.h2,{id:"loading-data-and-creating-an-index",children:"Loading data and creating an index"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_index import TreeIndex, SimpleDirectoryReader\n\nDATA_PATH = "/Users/alain/downloads/sct"\n\nresume = SimpleDirectoryReader(DATA_PATH).load_data()\xe6\nnew_index = TreeIndex.from_documents(resume)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"running-a-query",children:"Running a query"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'query_engine = new_index.as_query_engine()\nresponse = query_engine.query("Quel projet au C\xe9gep John Abbott?")\nprint(response)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"saving-and-loading-the-context",children:"Saving and loading the context"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"new_index.storage_context.persist()\n"})}),"\n",(0,o.jsx)(n.p,{children:"Once that is done, we can quickly load the storage context and create an index."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_index import StorageContext, load_index_from_storage\n\nstorage_context = StorageContext.from_defaults(persist_dir="./storage")\nindex = load_index_from_storage(storage_context)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query("What is Abid\'s job title?")\nprint(response)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"chatbot",children:"Chatbot"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'query_engine = index.as_chat_engine()\nresponse = query_engine.chat("What is the job title of Abid in 2021?")\nprint(response)\nresponse = query_engine.chat("What else did he do during that time?")\nprint(response)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"discover-llamaindex",children:"Discover LlamaIndex"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"2024-01-01."}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["The examples in this section come from ",(0,o.jsx)(n.a,{href:"https://github.com/run-llama/llama_docs_bot/tree/main",children:"llama_docs_bot"}),".\nSee also ",(0,o.jsx)(n.a,{href:"https://www.youtube.com/playlist?list=PLTZkGHtR085ZjK1srrSZIrkeEzQiMjO9W",children:"Discover LlamaIndex"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"basic-examples",children:"Basic examples"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_index.llms import OpenAI\n\nllm = OpenAI(temperature=0, model="gpt-3.5-turbo", max_tokens=256)\nresponse = llm.complete("Tell me a joke!")\n\nprint(response.text)\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_index.llms import OpenAI, ChatMessage\n\nllm = OpenAI(temperature=0, model="gpt-3.5-turbo", max_tokens=256)\nmessages = [\n    ChatMessage(role="system", content="Talk like a pirate in responmses."),\n    ChatMessage(role="user", content="Tell me a joke!")\n]\n\nresponse = llm.chat(messages)\nprint(response)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"qa-and-refine-template",children:"QA and Refine Template"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'with open("llama_docs_bot/docs/getting_started/starter_example.md", "r") as f:\n    text = f.read()\n\nfrom llama_index.llms import OpenAI\nllm = OpenAI(model="gpt-3.5-turbo", temperature=0)\n\nfrom llama_index import Prompt\n\ntext_qa_template = Prompt(\n    "Context information is below.\\n"\n    "---------------------\\n"\n    "{context_str}\\n"\n    "---------------------\\n"\n    "Given the context information and not prior knowledge, "\n    "answer the question: {query_str}\\n"\n)\n\nrefine_template = Prompt(\n    "We have the opportunity to refine the original answer "\n    "(only if needed) with some more context below.\\n"\n    "------------\\n"\n    "{context_msg}\\n"\n    "------------\\n"\n    "Given the new context, refine the original answer to better "\n    "answer the question: {query_str}. "\n    "If the context isn\'t useful, output the original answer again.\\n"\n    "Original Answer: {existing_answer}"\n)\n\nquestion = "I need to follow which steps to create an index?"\nprompt = text_qa_template.format(context_str=text, query_str=question)\nresponse = llm.complete(prompt)\nprint(response.text)\n\nprint("-----")\nquestion = "How do I create an index? Write your answer using only code."\nprompt = text_qa_template.format(context_str=text, query_str=question)\nresponse_gen = llm.stream_complete(prompt)\nfor response in response_gen:\n    print(response.delta, end="")\n\nquestion = "How do I create an index? Write your answer using only code."\nexisting_answer = """To create an index using LlamaIndex, you need to follow these steps:\n\n1. Download the LlamaIndex repository by cloning it from GitHub.\n2. Navigate to the `examples/paul_graham_essay` folder in the cloned repository.\n3. Create a new Python file and import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader`.\n4. Load the documents from the `data` folder using `SimpleDirectoryReader(\'data\').load_data()`.\n5. Build the index using `VectorStoreIndex.from_documents(documents)`.\n6. To persist the index to disk, use `index.storage_context.persist()`.\n7. To reload the index from disk, use the `StorageContext` and `load_index_from_storage` functions.\n\nNote: This answer assumes that you have already installed LlamaIndex and have the necessary dependencies."""\nprompt = refine_template.format(context_msg=text, query_str=question, existing_answer=existing_answer)\nresponse = llm.complete(prompt)\nprint(response.text)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"chat-example",children:"Chat Example"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_index.llms import ChatMessage\n\nchat_history = [\n    ChatMessage(role="system", content="You are a helpful QA chatbot that can answer questions about llama-index."),\n    ChatMessage(role="user", content="How do I create an index?"),\n]\n\nresponse = llm.chat(chat_history)\nprint(response.message)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"documentsnodes",children:"Documents/Nodes"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_index import Document, SimpleDirectoryReader\n\n# create document manually\ndocument = Document(text="text")\n\n# load from a directory\ndocuments = SimpleDirectoryReader("./data").load_data()\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_index import Document\n\n# Manually customize a document + metadata\ndocument = Document(\n    title="Hello World",\n    content="This is a super-customized document",\n    metadata={"category": "finance"},\n    excluded_llm_metadata_keys=["category"],\n    metadata_separator="::", \n    metadata_template="{key}{separator}{value}",\n    text_template="Metadata: {metadata_str}\\n-----\\nContent:{content}",\n)\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import sys\nsys.path.append("./llama_docs_bot")\n\nfrom llama_docs_bot.markdown_docs_reader import MarkdownDocsReader\nfrom llama_index import SimpleDirectoryReader\n\ndef load_markdown_docs(filepath):\n    """Load markdown docs from a directory, excluding all other file types."""\n    loader = SimpleDirectoryReader(\n        input_dir=filepath, \n        exclude=["*.rst", "*.ipynb", "*.py", "*.bat", "*.txt", "*.png", "*.jpg", "*.jpeg", "*.csv", "*.html", "*.js", "*.css", "*.pdf", "*.json"],\n        file_extractor={".md": MarkdownDocsReader()},\n        recursive=True\n    )\n\n    return loader.load_data()\n\n\n# load our documents from each folder.\n# we keep them seperate for now, in order to create seperate indexes later\ngetting_started_docs = load_markdown_docs("llama_docs_bot/docs/getting_started")\ncommunity_docs = load_markdown_docs("llama_docs_bot/docs/community")\ndata_docs = load_markdown_docs("llama_docs_bot/docs/core_modules/data_modules")\nagent_docs = load_markdown_docs("llama_docs_bot/docs/core_modules/agent_modules")\nmodel_docs = load_markdown_docs("llama_docs_bot/docs/core_modules/model_modules")\nquery_docs = load_markdown_docs("llama_docs_bot/docs/core_modules/query_modules")\nsupporting_docs = load_markdown_docs("llama_docs_bot/docs/core_modules/supporting_modules")\ntutorials_docs = load_markdown_docs("llama_docs_bot/docs/end_to_end_tutorials")\ncontributing_docs = load_markdown_docs("llama_docs_bot/docs/development")\n\n# Make our printing look nice\nfrom llama_index.schema import MetadataMode\n\nprint(agent_docs[5].get_content(metadata_mode=MetadataMode.ALL))\nprint(agent_docs[0].metadata)\n\ntext_template = "Content Metadata:\\n{metadata_str}\\n\\nContent:\\n{content}"\n\nmetadata_template = "{key}: {value},"\nmetadata_seperator= " "\n\nfor doc in agent_docs:\n    doc.text_template = text_template\n    doc.metadata_template = metadata_template\n    doc.metadata_seperator = metadata_seperator\n\nprint(agent_docs[0].get_content(metadata_mode=MetadataMode.ALL))\n\n# Hide the File Name from the LLM\nagent_docs[0].excluded_llm_metadata_keys = ["File Name"]\nprint(agent_docs[0].get_content(metadata_mode=MetadataMode.LLM))\n\n# Hide the File Name from the embedding model\nagent_docs[0].excluded_embed_metadata_keys = ["File Name"]\nprint(agent_docs[0].get_content(metadata_mode=MetadataMode.EMBED))\n'})}),"\n",(0,o.jsx)(n.h3,{id:"evaluation-baseline",children:"Evaluation Baseline"}),"\n",(0,o.jsx)(n.h4,{id:"loading-our-docs",children:"Loading our Docs"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import sys\nsys.path.append("./llama_docs_bot")\n\nfrom llama_docs_bot.markdown_docs_reader import MarkdownDocsReader\nfrom llama_index import SimpleDirectoryReader\n\n\ndef load_markdown_docs(filepath):\n    """Load markdown docs from a directory, excluding all other file types."""\n    loader = SimpleDirectoryReader(\n        input_dir=filepath, \n        required_exts=[".md"],\n        file_extractor={".md": MarkdownDocsReader()},\n        recursive=True\n    )\n\n    documents = loader.load_data()\n\n    # exclude some metadata from the LLM\n    for doc in documents:\n        doc.excluded_llm_metadata_keys = ["File Name", "Content Type", "Header Path"]\n\n    return documents\n\n\n# load our documents from each folder.\n# we keep them seperate for now, in order to create seperate indexes later\ngetting_started_docs = load_markdown_docs("llama_docs_bot/docs/getting_started")\ncommunity_docs = load_markdown_docs("llama_docs_bot/docs/community")\ndata_docs = load_markdown_docs("llama_docs_bot/docs/core_modules/data_modules")\nagent_docs = load_markdown_docs("llama_docs_bot/docs/core_modules/agent_modules")\nmodel_docs = load_markdown_docs("llama_docs_bot/docs/core_modules/model_modules")\nquery_docs = load_markdown_docs("llama_docs_bot/docs/core_modules/query_modules")\nsupporting_docs = load_markdown_docs("llama_docs_bot/docs/core_modules/supporting_modules")\ntutorials_docs = load_markdown_docs("llama_docs_bot/docs/end_to_end_tutorials")\ncontributing_docs = load_markdown_docs("llama_docs_bot/docs/development")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"create-the-indicies",children:"Create the indicies"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_index import ServiceContext, set_global_service_context\nfrom llama_index.llms import OpenAI\n\n# create a global service context\nservice_context = ServiceContext.from_defaults(llm=OpenAI(model="gpt-3.5-turbo", temperature=0))\nset_global_service_context(service_context)\n\nfrom llama_index import VectorStoreIndex, StorageContext, load_index_from_storage\n\n# create a vector store index for each folder\ntry:\n    getting_started_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./getting_started_index"))\n    community_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./community_index"))\n    data_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./data_index"))\n    agent_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./agent_index"))\n    model_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./model_index"))\n    query_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./query_index"))\n    supporting_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./supporting_index"))\n    tutorials_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./tutorials_index"))\n    contributing_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./contributing_index"))\nexcept:\n    getting_started_index = VectorStoreIndex.from_documents(getting_started_docs)\n    getting_started_index.storage_context.persist(persist_dir="./getting_started_index")\n\n    community_index = VectorStoreIndex.from_documents(community_docs)\n    community_index.storage_context.persist(persist_dir="./community_index")\n\n    data_index = VectorStoreIndex.from_documents(data_docs)\n    data_index.storage_context.persist(persist_dir="./data_index")\n\n    agent_index = VectorStoreIndex.from_documents(agent_docs)\n    agent_index.storage_context.persist(persist_dir="./agent_index")\n\n    model_index = VectorStoreIndex.from_documents(model_docs)\n    model_index.storage_context.persist(persist_dir="./model_index")\n\n    query_index = VectorStoreIndex.from_documents(query_docs)\n    query_index.storage_context.persist(persist_dir="./query_index")    \n\n    supporting_index = VectorStoreIndex.from_documents(supporting_docs)\n    supporting_index.storage_context.persist(persist_dir="./supporting_index")\n\n    tutorials_index = VectorStoreIndex.from_documents(tutorials_docs)\n    tutorials_index.storage_context.persist(persist_dir="./tutorials_index")\n\n    contributing_index = VectorStoreIndex.from_documents(contributing_docs)\n    contributing_index.storage_context.persist(persist_dir="./contributing_index")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"create-query-engine-tools",children:"Create Query Engine Tools"}),"\n",(0,o.jsx)(n.p,{children:"Since we have so many indicies, we can create a query engine tool for each and then use them in a single query engine!"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_index.tools import QueryEngineTool\n\n# create a query engine tool for each folder\ngetting_started_tool = QueryEngineTool.from_defaults(\n    query_engine=getting_started_index.as_query_engine(), \n    name="Getting Started", \n    description="Useful for answering questions about installing and running llama index, as well as basic explanations of how llama index works."\n)\n\ncommunity_tool = QueryEngineTool.from_defaults(\n    query_engine=community_index.as_query_engine(),\n    name="Community",\n    description="Useful for answering questions about integrations and other apps built by the community."\n)\n\ndata_tool = QueryEngineTool.from_defaults(\n    query_engine=data_index.as_query_engine(),\n    name="Data Modules",\n    description="Useful for answering questions about data loaders, documents, nodes, and index structures."\n)\n\nagent_tool = QueryEngineTool.from_defaults(\n    query_engine=agent_index.as_query_engine(),\n    name="Agent Modules",\n    description="Useful for answering questions about data agents, agent configurations, and tools."\n)\n\nmodel_tool = QueryEngineTool.from_defaults(\n    query_engine=model_index.as_query_engine(),\n    name="Model Modules",\n    description="Useful for answering questions about using and configuring LLMs, embedding modles, and prompts."\n)\n\nquery_tool = QueryEngineTool.from_defaults(\n    query_engine=query_index.as_query_engine(),\n    name="Query Modules",\n    description="Useful for answering questions about query engines, query configurations, and using various parts of the query engine pipeline."\n)\n\nsupporting_tool = QueryEngineTool.from_defaults(\n    query_engine=supporting_index.as_query_engine(),\n    name="Supporting Modules",\n    description="Useful for answering questions about supporting modules, such as callbacks, service context, and avaluation."\n)\n\n\ntutorials_tool = QueryEngineTool.from_defaults(\n    query_engine=tutorials_index.as_query_engine(),\n    name="Tutorials",\n    description="Useful for answering questions about end-to-end tutorials and giving examples of specific use-cases."\n)\n\ncontributing_tool = QueryEngineTool.from_defaults(\n    query_engine=contributing_index.as_query_engine(),\n    name="Contributing",\n    description="Useful for answering questions about contributing to llama index, including how to contribute to the codebase and how to build documentation."\n)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"create-unified-query-engine",children:"Create Unified Query Engine"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# needed for notebooks\nimport nest_asyncio\nnest_asyncio.apply()\n\nfrom llama_index.query_engine import SubQuestionQueryEngine\nfrom llama_index.response_synthesizers import get_response_synthesizer\n\n# Create Unified Query Engine\nquery_engine = SubQuestionQueryEngine.from_defaults(\n    query_engine_tools=[\n        getting_started_tool,\n        community_tool,\n        data_tool,\n        agent_tool,\n        model_tool,\n        query_tool,\n        supporting_tool,\n        tutorials_tool,\n        contributing_tool\n    ],\n    # enable this for streaming\n    # response_synthesizer=get_response_synthesizer(streaming=True),\n    verbose=False\n)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"test-the-query-engine",children:"Test the Query Engine!"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'response = query_engine.query("How do I install llama index?")\nprint(str(response))\n'})}),"\n",(0,o.jsx)(n.h3,{id:"generate-the-dataset",children:"Generate the Dataset"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_index import Document\n\n# Generate the Datasets\ndocuments = SimpleDirectoryReader("../docs", recursive=True, required_exts=[".md"]).load_data()\n\nall_text = ""\n\nfor doc in documents:\n    all_text += doc.text\n\ngiant_document = Document(text=all_text)\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import os\nimport random\nrandom.seed(42)\n\nfrom llama_index import ServiceContext\nfrom llama_index.prompts import Prompt\nfrom llama_index.llms import OpenAI\nfrom llama_index.evaluation import DatasetGenerator\n\ngpt4_service_context = ServiceContext.from_defaults(llm=OpenAI(llm="gpt-4", temperature=0))\n\nquestion_dataset = []\nif os.path.exists("question_dataset.txt"):\n    with open("question_dataset.txt", "r") as f:\n        for line in f:\n            question_dataset.append(line.strip())\nelse:\n    # generate questions\n    data_generator = DatasetGenerator.from_documents(\n        [giant_document],\n        text_question_template=Prompt(\n            "A sample from the LlamaIndex documentation is below.\\n"\n            "---------------------\\n"\n            "{context_str}\\n"\n            "---------------------\\n"\n            "Using the documentation sample, carefully follow the instructions below:\\n"\n            "{query_str}"\n        ),\n        question_gen_query=(\n            "You are an evaluator for a search pipeline. Your task is to write a single question "\n            "using the provided documentation sample above to test the search pipeline. The question should "\n            "reference specific names, functions, and terms. Restrict the question to the "\n            "context information provided.\\n"\n            "Question: "\n        ),\n        # set this to be low, so we can generate more questions\n        service_context=gpt4_service_context\n    )\n    generated_questions = data_generator.generate_questions_from_nodes()\n\n    # randomly pick 40 questions from each dataset\n    generated_questions = random.sample(generated_questions, 40)\n    question_dataset.extend(generated_questions)\n\n    print(f"Generated {len(question_dataset)} questions.")\n\n    # save the questions!\n    with open("question_dataset.txt", "w") as f:\n        for question in question_dataset:\n            f.write(f"{question.strip()}\\n")\n\nprint(random.sample(question_dataset, 5))\n'})}),"\n",(0,o.jsx)(n.h3,{id:"evaluate-with-the-dataset",children:"Evaluate with the Dataset"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import time\nimport asyncio\nimport nest_asyncio\nnest_asyncio.apply()\n\nfrom llama_index import Response\n\ndef evaluate_query_engine(evaluator, query_engine, questions):\n    async def run_query(query_engine, q):\n        try:\n            return await query_engine.aquery(q)\n        except:\n            return Response(response="Error, query failed.")\n\n    total_correct = 0\n    all_results = []\n    for batch_size in range(0, len(questions), 5):\n        batch_qs = questions[batch_size:batch_size+5]\n\n        tasks = [run_query(query_engine, q) for q in batch_qs]\n        responses = asyncio.run(asyncio.gather(*tasks))\n        print(f"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}")\n\n        for response in responses:\n            eval_result = 1 if "YES" in evaluator.evaluate(response) else 0\n            total_correct += eval_result\n            all_results.append(eval_result)\n        \n        # helps avoid rate limits\n        time.sleep(1)\n\n    return total_correct, all_results\n\nfrom llama_index.evaluation import ResponseEvaluator\n\n# gpt-4 evaluator!\nevaluator = ResponseEvaluator(service_context=gpt4_service_context)\n\ntotal_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n\nprint(f"Hallucination? Scored {total_correct} out of {len(question_dataset)} questions correctly.")\n# Hallucination? Scored 29 out of 40 questions correctly.\n'})}),"\n",(0,o.jsxs)(n.p,{children:["See ",(0,o.jsx)(n.a,{href:"https://github.com/run-llama/llama_docs_bot/blob/main/3_eval_baseline/3_eval_basline.ipynb",children:"3_eval_baseline/3_eval_basline.ipynb"}),".\nSee also ",(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=LQy8iHOJE2A&list=PLTZkGHtR085ZjK1srrSZIrkeEzQiMjO9W&index=3",children:"Part 3, Evaluation"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"embeddings",children:"Embeddings"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_index.embeddings import OpenAIEmbedding\n\ntext = "Bonjour!"\n\nembed_model = OpenAIEmbedding()\nembedding = embed_model.get_text_embedding(text)\nprint(len(embedding))\nprint(embedding)\n# 1536\n# [-0.00838178675621748, -0.013095753267407417, -0.012251272797584534, ...\n'})}),"\n",(0,o.jsx)(n.h3,{id:"custom-embeddings",children:"Custom Embeddings"}),"\n",(0,o.jsxs)(n.p,{children:["While we can integrate with any embeddings offered by Langchain, you can also implement the ",(0,o.jsx)(n.code,{children:"BaseEmbedding"})," class and run your own custom embedding model!"]}),"\n",(0,o.jsxs)(n.p,{children:["For this, we will use the ",(0,o.jsx)(n.code,{children:"InstructorEmbedding"})," pip package, in order to run ",(0,o.jsx)(n.code,{children:"hkunlp/instructor-large"})," model\nfound ",(0,o.jsx)(n.a,{href:"https://huggingface.co/hkunlp/instructor-large",children:"here"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"!pipenv install InstructorEmbedding torch transformers sentence_transformers\n!pipenv run pip install 'deepspeed<=0.9.3'\n\nfrom InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR('hkunlp/instructor-large')\nsentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\ninstruction = \"Represent the Science title:\"\nembeddings = model.encode([[instruction, sentence]])\nprint(embeddings)\n# load INSTRUCTOR_Transformer\n# max_seq_length  512\n# [[-6.15552627e-02  1.04199704e-02  5.88438474e-03  1.93768851e-02\n#    5.71417809e-02  2.57655438e-02 -4.01949983e-05 -2.80044544e-02\n# ...\n"})}),"\n",(0,o.jsx)(n.p,{children:"Looks good! But we can see the output is batched (i.e. a list of lists), so we need to undo the batching in our implementation!"}),"\n",(0,o.jsx)(n.p,{children:"There are only 4 methods we need to implement below."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from typing import Any, List\nfrom InstructorEmbedding import INSTRUCTOR\nfrom llama_index.embeddings.base import BaseEmbedding\n\nclass InstructorEmbeddings(BaseEmbedding):\n    def __init__(\n        self, \n        instructor_model_name: str = "hkunlp/instructor-large",\n        instruction: str = "Represent the Computer Science text for retrieval:",\n        **kwargs: Any,\n    ) -> None:\n        self._model = INSTRUCTOR(instructor_model_name)\n        self._instruction = instruction\n        super().__init__(**kwargs)\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        embeddings = model.encode([[self._instruction, query]])\n        return embeddings[0].tolist()\n    \n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        embeddings = model.encode([[self._instruction, text]])\n        return embeddings[0].tolist() \n    \n    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n        embeddings = model.encode([[self._instruction, text] for text in texts])\n        return embeddings.tolist()\n\n# set the batch size to 1 to avoid memory issues\n# if you have a large GPU, you can increase this\ninstructor_embeddings = InstructorEmbeddings(embed_batch_size=1)\n# load INSTRUCTOR_Transformer\n# max_seq_length  512\n\nembed = instructor_embeddings.get_text_embedding("How do I create a vector index?")\nprint(len(embed))\nprint(embed[:10])\n# 768\n# [0.003987083211541176, 0.01212295051664114, 0.0026905445847660303, ...\n'})}),"\n",(0,o.jsx)(n.h3,{id:"json-query-engine",children:"JSON Query Engine"}),"\n",(0,o.jsxs)(n.p,{children:["See ",(0,o.jsx)(n.a,{href:"https://github.com/run-llama/llama_docs_bot/blob/main/5_retrieval/5_retrieval.ipynb",children:"5_retrieval/5_retrieval.ipynb"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"answering-complex-queries",children:"Answering complex queries"}),"\n",(0,o.jsxs)(n.p,{children:["In this demo, we explore answering complex queries by decomposing them into simpler sub-queries.\nSee ",(0,o.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/examples/usecases/10k_sub_question.html",children:"10K Analysis"})]}),"\n",(0,o.jsx)(n.h3,{id:"joint-text-to-sql-and-semantic-search",children:"Joint Text to SQL and Semantic Search"}),"\n",(0,o.jsx)(n.p,{children:"In this tutorial, we show you how to use our SQLAutoVectorQueryEngine."}),"\n",(0,o.jsx)(n.p,{children:"This query engine allows you to combine insights from your structured tables with your unstructured\ndata. It first decides whether to query your structured tables for insights. Once it does, it can\nthen infer a corresponding query to the vector store in order to fetch corresponding documents."}),"\n",(0,o.jsxs)(n.p,{children:["See ",(0,o.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/examples/query_engine/SQLAutoVectorQueryEngine.html",children:"SQL Auto Vector Query Engine"}),".\nSee also ",(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=ZIvcVJGtCrY&list=PLTZkGHtR085ZjK1srrSZIrkeEzQiMjO9W&index=7",children:"Youtube"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"document-management",children:"Document Management"}),"\n",(0,o.jsx)(n.p,{children:"This notebook walks through the process of managing documents that come from ever-updating data sources."}),"\n",(0,o.jsx)(n.p,{children:"In this example, we have a directory where the #issues-and-help channel on the LlamaIndex discord is dumped\nperiodically. We want to ensure our index always has the latest data, without duplicating any messages."}),"\n",(0,o.jsxs)(n.p,{children:["See ",(0,o.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/latest/examples/discover_llamaindex/document_management/Discord_Thread_Management.html",children:"Discord Thread Management"}),".\nSee also ",(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=j6dJcODLd_c&list=PLTZkGHtR085ZjK1srrSZIrkeEzQiMjO9W&index=8",children:"Youtube"})]}),"\n",(0,o.jsx)(n.h2,{id:"llamaindex-webinars",children:"LlamaIndex Webinars"}),"\n",(0,o.jsxs)(n.p,{children:["See ",(0,o.jsx)(n.a,{href:"https://youtube.com/playlist?list=PLTZkGHtR085YK5CdOy8vAWQ7c-mUFzTBC",children:"LlamaIndex Webinars"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"see-also",children:"See also"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://blog.llamaindex.ai/introducing-query-pipelines-025dc2bb0537",children:"Introducing Query Pipelines"})}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>i,a:()=>r});var o=t(67294);const a={},s=o.createContext(a);function r(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);