<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-references/llms/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">Large Language Models (LLMs) | Alain Boisvert</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://boisalai.github.io/docs/references/llms"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Large Language Models (LLMs) | Alain Boisvert"><meta data-rh="true" name="description" content="&quot;The limits of my language mean the limits of my world.&quot; &amp;#x2014; Ludwig Wittgenstein"><meta data-rh="true" property="og:description" content="&quot;The limits of my language mean the limits of my world.&quot; &amp;#x2014; Ludwig Wittgenstein"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://boisalai.github.io/docs/references/llms"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/references/llms" hreflang="en"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/references/llms" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Alain Boisvert RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Alain Boisvert Atom Feed">



<link rel="stylesheet" href="/fonts/font-awesome/fontawesome.css">
<link rel="stylesheet" href="/fonts/font-awesome/solid.css">
<link rel="stylesheet" href="/fonts/font-awesome/regular.css">
<link rel="stylesheet" href="/fonts/font-awesome/brands.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.e0a8b603.css">
<script src="/assets/js/runtime~main.2e6d2f4d.js" defer="defer"></script>
<script src="/assets/js/main.ae67d273.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Alain Boisvert</b></a><a class="navbar__item navbar__link" target="" href="/docs/courses/university/glo-7030">GLO-7030</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/boisalai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-github"> </a><a href="https://www.linkedin.com/in/alain-boisvert-98b058156/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-linkedin-in"> </a><a href="mailto:ay.boisvert@gmail.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-solid fa-envelope"> </a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Alain Boisvert</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/cv">Curriculum vit√¶</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/learning">Learning path</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/certificates">Certificates</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/courses/university/gif-7005">Courses</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/models/revdisp">Interactive models</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/references/links">References</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/references/links">Curated Links</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/references/tools/command-line">Tools</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/references/editors/jupyter">Editors</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/references/languages/sas">Languages</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/references/ml">Machine Learning</a><button aria-label="Expand sidebar category &#x27;Machine Learning&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible menu__list-item-collapsible--active"><a class="menu__link menu__link--sublist menu__link--active" aria-current="page" aria-expanded="true" tabindex="0" href="/docs/references/llms">Large Language Models</a><button aria-label="Collapse sidebar category &#x27;Large Language Models&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/references/llms/llama">Llama</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/references/llms/openai">OpenAI</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/references/llms/google-gimini">Gemini</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/references/llms/gpt">GPT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/references/llms/langchain">ü¶úÔ∏èüîó LangChain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/references/llms/llama-index">LlamaIndex</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/references/llms/phi-2">Phi-2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/references/llms/prompt">Prompt</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/references/databases">Databases</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">References</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Large Language Models</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Large Language Models (LLMs)</h1>
<p><em>&quot;The limits of my language mean the limits of my world.&quot;</em> ‚Äî Ludwig Wittgenstein</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="architectures">Architectures<a href="#architectures" class="hash-link" aria-label="Direct link to Architectures" title="Direct link to Architectures">‚Äã</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">Transformer</a> (2017).</li>
<li><a href="https://arxiv.org/abs/2312.00752" target="_blank" rel="noopener noreferrer">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a> and <a href="https://huggingface.co/papers/2312.00752" target="_blank" rel="noopener noreferrer">here</a>
<ul>
<li><a href="https://github.com/state-spaces/mamba" target="_blank" rel="noopener noreferrer">Mamba</a>.</li>
<li><a href="https://www.youtube.com/watch?v=9dSkvxS2EB0" target="_blank" rel="noopener noreferrer">Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Paper Explained)</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="models">Models<a href="#models" class="hash-link" aria-label="Direct link to Models" title="Direct link to Models">‚Äã</a></h2>
<p>Large language models (LLMs) refer to Transformer language models that contain hundreds of billions (or
more) of parameters, which are trained on massive text data.</p>
<p>See also <a href="https://crfm.stanford.edu/ecosystem-graphs/index.html" target="_blank" rel="noopener noreferrer">Table of LLMs</a>
and this <a href="https://www.promptingguide.ai/models/collection" target="_blank" rel="noopener noreferrer">LLM Collection</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bert">BERT<a href="#bert" class="hash-link" aria-label="Direct link to BERT" title="Direct link to BERT">‚Äã</a></h3>
<ul>
<li><a href="https://huggingface.co/togethercomputer/m2-bert-80M-2k-retrieval" target="_blank" rel="noopener noreferrer">togethercomputer/m2-bert-80M-2k-retrieval</a></li>
<li><a href="https://huggingface.co/togethercomputer/m2-bert-80M-8k-retrieval" target="_blank" rel="noopener noreferrer">togethercomputer/m2-bert-80M-8k-retrieval</a></li>
<li><a href="https://huggingface.co/togethercomputer/m2-bert-80M-32k-retrieval" target="_blank" rel="noopener noreferrer">togethercomputer/m2-bert-80M-32k-retrieval</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="databricks-dolly">Databricks Dolly<a href="#databricks-dolly" class="hash-link" aria-label="Direct link to Databricks Dolly" title="Direct link to Databricks Dolly">‚Äã</a></h3>
<ul>
<li><a href="https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html" target="_blank" rel="noopener noreferrer">Dolly v1</a>.</li>
<li><a href="https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm" target="_blank" rel="noopener noreferrer">Dolly v2</a>.</li>
<li><a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k" target="_blank" rel="noopener noreferrer">databricks/databricks-dolly-15k</a>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="eleutherai-pythia">EleutherAI Pythia<a href="#eleutherai-pythia" class="hash-link" aria-label="Direct link to EleutherAI Pythia" title="Direct link to EleutherAI Pythia">‚Äã</a></h3>
<ul>
<li><a href="https://huggingface.co/EleutherAI/pythia-12b" target="_blank" rel="noopener noreferrer">EleutherAI/pythia-12b</a>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="facebook-llama">Facebook Llama<a href="#facebook-llama" class="hash-link" aria-label="Direct link to Facebook Llama" title="Direct link to Facebook Llama">‚Äã</a></h3>
<ul>
<li><a href="https://arxiv.org/abs/2302.13971" target="_blank" rel="noopener noreferrer">LLaMA: Open and Efficient Foundation Language Models</a>.</li>
<li><a href="https://ai.meta.com/llama/" target="_blank" rel="noopener noreferrer">Meta AI Introducing Llama 2</a>.</li>
<li><a href="https://huggingface.co/papers/2307.09288" target="_blank" rel="noopener noreferrer">Llama 2 Research Paper</a>.</li>
<li><a href="https://huggingface.co/meta-llama" target="_blank" rel="noopener noreferrer">Llama 2 on Hugging Face</a>.</li>
<li><a href="https://github.com/facebookresearch/llama-recipes/tree/main" target="_blank" rel="noopener noreferrer">Meta Examples and recipes for Llama model</a>.</li>
<li><a href="https://labs.perplexity.ai/?utm_content=first_codellama&amp;s=u&amp;utm_source=twitter&amp;utm_campaign=labs" target="_blank" rel="noopener noreferrer">LLaMa Chat</a> on Perplexity.<!-- -->
<ul>
<li><a href="https://www.mosaicml.com/blog/llama2-inference" target="_blank" rel="noopener noreferrer">Introducing Llama2-70B-Chat with MosaicML Inference</a>.</li>
</ul>
</li>
<li>Code Llama<!-- -->
<ul>
<li><a href="https://huggingface.co/codellama" target="_blank" rel="noopener noreferrer">Code Llama</a>.</li>
<li><a href="https://huggingface.co/blog/codellama" target="_blank" rel="noopener noreferrer">Llama 2 learns to code</a>.</li>
<li><a href="https://arxiv.org/pdf/2308.12950.pdf" target="_blank" rel="noopener noreferrer">Code Llama: Open Foundation Models for Code</a></li>
<li><a href="https://github.com/facebookresearch/codellama" target="_blank" rel="noopener noreferrer">Llama repository</a></li>
</ul>
</li>
<li>Reddit<!-- -->
<ul>
<li><a href="https://www.reddit.com/r/LocalLLaMA/" target="_blank" rel="noopener noreferrer">r/LocalLLaMA/</a></li>
<li><a href="https://www.reddit.com/r/LocalLLaMA/wiki/models/" target="_blank" rel="noopener noreferrer">r/LocalLLaMA/wiki/models/</a></li>
</ul>
</li>
<li><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" rel="noopener noreferrer">Request access</a> to the next version of Llama.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mistral-7b">Mistral-7B<a href="#mistral-7b" class="hash-link" aria-label="Direct link to Mistral-7B" title="Direct link to Mistral-7B">‚Äã</a></h3>
<ul>
<li><a href="https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca" target="_blank" rel="noopener noreferrer">Open-Orca/Mistral-7B-OpenOrca</a></li>
<li><a href="https://medium.com/@andysingal/mistral-7b-instruct-conversational-genius-redefined-542a841c8635" target="_blank" rel="noopener noreferrer">Mistral-7B-Instruct</a> is designed to excel in two primary domains: English language tasks and coding tasks.<!-- -->
<ul>
<li><a href="https://medium.com/@mne/run-mistral-7b-model-on-macbook-m1-pro-with-16gb-ram-using-llama-cpp-44134694b773" target="_blank" rel="noopener noreferrer">Run Mistral 7B Model on MacBook M1 Pro with 16GB RAM using llama.cpp</a></li>
</ul>
</li>
<li><a href="https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.2" target="_blank" rel="noopener noreferrer">mlx-community/Mistral-7B-Instruct-v0.2</a> converted to MLX format.</li>
<li><a href="https://towardsdatascience.com/llms-for-everyone-running-langchain-and-a-mistralai-7b-model-in-google-colab-246ca94d7c4d" target="_blank" rel="noopener noreferrer">LLMs for Everyone: Running LangChain and a MistralAI 7B Model in Google Colab</a> from Dmitrii Eliuseev - Dec 5, 2023.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mosaic-mpt">Mosaic MPT<a href="#mosaic-mpt" class="hash-link" aria-label="Direct link to Mosaic MPT" title="Direct link to Mosaic MPT">‚Äã</a></h3>
<ul>
<li><a href="https://www.mosaicml.com/" target="_blank" rel="noopener noreferrer">Mosaic MPT</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="openai-gpt">OpenAI GPT<a href="#openai-gpt" class="hash-link" aria-label="Direct link to OpenAI GPT" title="Direct link to OpenAI GPT">‚Äã</a></h3>
<ul>
<li><a href="https://platform.openai.com/overview" target="_blank" rel="noopener noreferrer">OpenAI platform</a>.</li>
<li><a href="https://platform.openai.com/docs/models/gpt-4" target="_blank" rel="noopener noreferrer">GPT-4</a>.</li>
<li><a href="https://platform.openai.com/docs/models/gpt-3-5" target="_blank" rel="noopener noreferrer">GPT-3.5</a>.</li>
<li><code>gpt-3.5-turbo</code> has been optimized for chat using the <a href="https://platform.openai.com/docs/api-reference/chat" target="_blank" rel="noopener noreferrer">Chat completions API</a>.</li>
<li><a href="https://community.openai.com/tag/chat9gpt" target="_blank" rel="noopener noreferrer">Forum</a></li>
<li><a href="https://platform.openai.com/docs/guides/prompt-engineering" target="_blank" rel="noopener noreferrer">Prompt engineering guide</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="poe">Poe<a href="#poe" class="hash-link" aria-label="Direct link to Poe" title="Direct link to Poe">‚Äã</a></h3>
<ul>
<li><a href="https://poe.com/Solar-0-70b" target="_blank" rel="noopener noreferrer">Solar</a></li>
<li><a href="https://huggingface.co/upstage/SOLAR-0-70b-16bit" target="_blank" rel="noopener noreferrer">upstage/SOLAR-0-70b-16bit</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="stanford-alpaca">Stanford Alpaca<a href="#stanford-alpaca" class="hash-link" aria-label="Direct link to Stanford Alpaca" title="Direct link to Stanford Alpaca">‚Äã</a></h3>
<ul>
<li><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank" rel="noopener noreferrer">Stanford Alpaca</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="tii-falcon">TII Falcon<a href="#tii-falcon" class="hash-link" aria-label="Direct link to TII Falcon" title="Direct link to TII Falcon">‚Äã</a></h3>
<ul>
<li><a href="https://huggingface.co/tiiuae/falcon-40b" target="_blank" rel="noopener noreferrer">Falcon-40B</a>.</li>
<li><a href="https://huggingface.co/blog/falcon-180b" target="_blank" rel="noopener noreferrer">Spread Your Wings: Falcon 180B is here</a>.<!-- -->
<ul>
<li>Falcon 180b can be commercially used but under very restrictive conditions, excluding any &quot;hosting use&quot;.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="stablelm">StableLM<a href="#stablelm" class="hash-link" aria-label="Direct link to StableLM" title="Direct link to StableLM">‚Äã</a></h3>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vicuna">Vicuna<a href="#vicuna" class="hash-link" aria-label="Direct link to Vicuna" title="Direct link to Vicuna">‚Äã</a></h3>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="wizardlm">WizardLM<a href="#wizardlm" class="hash-link" aria-label="Direct link to WizardLM" title="Direct link to WizardLM">‚Äã</a></h3>
<ul>
<li><a href="https://github.com/nlpxucan/WizardLM" target="_blank" rel="noopener noreferrer">WizardLM repository</a>.</li>
<li><a href="https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0" target="_blank" rel="noopener noreferrer">WizardLM/WizardCoder-Python-34B-V1.0</a>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="tinyllama">TinyLlama<a href="#tinyllama" class="hash-link" aria-label="Direct link to TinyLlama" title="Direct link to TinyLlama">‚Äã</a></h3>
<ul>
<li><a href="https://ollama.ai/library/tinyllama" target="_blank" rel="noopener noreferrer">TinyLlama</a></li>
<li><a href="https://huggingface.co/papers/2401.02385" target="_blank" rel="noopener noreferrer">Paper</a></li>
<li><a href="https://github.com/jzhang38/TinyLlama" target="_blank" rel="noopener noreferrer">Repo</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="chatbot">ChatBot<a href="#chatbot" class="hash-link" aria-label="Direct link to ChatBot" title="Direct link to ChatBot">‚Äã</a></h2>
<ul>
<li><a href="https://platform.openai.com/playground" target="_blank" rel="noopener noreferrer">OpenAI Playground</a>.</li>
<li><a href="https://chat.openai.com/" target="_blank" rel="noopener noreferrer">ChatGPT</a> from OpenAI.</li>
<li><a href="https://www.chatpdf.com/c/xr1xDbkr9BUTR6EnOEUWo" target="_blank" rel="noopener noreferrer">ChatPDF</a></li>
<li><a href="https://www.bing.com/" target="_blank" rel="noopener noreferrer">Bing</a> from Microsoft.</li>
<li><a href="https://www.perplexity.ai/" target="_blank" rel="noopener noreferrer">Perplexity</a>, a new chatbot based on OpenAI&#x27;s ChatGPT.</li>
<li><a href="https://bard.google.com/" target="_blank" rel="noopener noreferrer">Google Bard</a> from Google, currently not available in Canada.</li>
<li><a href="https://claude.ai/" target="_blank" rel="noopener noreferrer">Claude.ai</a> Claude.ai is only available in the US and UK.</li>
<li><a href="https://www.phind.com/" target="_blank" rel="noopener noreferrer">phind</a>.</li>
<li><a href="https://github.com/KillianLucas/open-interpreter/" target="_blank" rel="noopener noreferrer">Open Interpreter</a>.</li>
<li><a href="https://poe.com/Solar-0-70b" target="_blank" rel="noopener noreferrer">Poe</a>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="code-llms">Code LLMs<a href="#code-llms" class="hash-link" aria-label="Direct link to Code LLMs" title="Direct link to Code LLMs">‚Äã</a></h2>
<ul>
<li><a href="https://www.cursor.so/" target="_blank" rel="noopener noreferrer">cursor.so</a>.</li>
<li><a href="https://github.com/features/copilot" target="_blank" rel="noopener noreferrer">GitHub Copilot</a>.</li>
<li><a href="https://github.com/bigcode-project/starcoder" target="_blank" rel="noopener noreferrer">StarCoder</a> and <a href="https://huggingface.co/blog/starcoder" target="_blank" rel="noopener noreferrer">here</a>.</li>
<li><a href="https://alphacode.deepmind.com/" target="_blank" rel="noopener noreferrer">DeepMind AlphaCode</a>.</li>
<li><a href="https://aws.amazon.com/codewhisperer/" target="_blank" rel="noopener noreferrer">Amazon CodeWhisperer</a>.</li>
<li><a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder" target="_blank" rel="noopener noreferrer">WizardCoder</a>.</li>
<li><a href="https://github.com/facebookresearch/codellama" target="_blank" rel="noopener noreferrer">Code Llama</a>.</li>
<li><a href="https://huggingface.co/collections/lmstudio-ai/metaais-codellama-coding-assistant-llm-64fb1d4ab60e2c9ddd07f8e6" target="_blank" rel="noopener noreferrer">MetaAI&#x27;s CodeLlama - Coding Assistant LLM</a>: Fast, small, and capable coding model you can run locally,</li>
<li><a href="https://huggingface.co/Deci/DeciCoder-6B" target="_blank" rel="noopener noreferrer">DeciCoder-6B</a>, <a href="https://colab.research.google.com/drive/1QRbuser0rfUiFmQbesQJLXVtBYZOlKpB" target="_blank" rel="noopener noreferrer">here</a> and <a href="https://huggingface.co/spaces/Deci/DeciCoder-6B-Demo" target="_blank" rel="noopener noreferrer">here</a>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="leaderboard">Leaderboard<a href="#leaderboard" class="hash-link" aria-label="Direct link to Leaderboard" title="Direct link to Leaderboard">‚Äã</a></h2>
<ul>
<li><a href="https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a" target="_blank" rel="noopener noreferrer">The Big Benchmarks Collection</a>.</li>
<li><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" target="_blank" rel="noopener noreferrer">ü§ó Open LLM Leaderboard</a>.</li>
<li><a href="https://huggingface.co/spaces/optimum/llm-perf-leaderboard" target="_blank" rel="noopener noreferrer">ü§ó Open LLM-Perf Leaderboard üèãÔ∏è</a>.</li>
<li><a href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard" target="_blank" rel="noopener noreferrer">‚≠ê Big Code Models Leaderboard</a>.</li>
<li><a href="https://crfm.stanford.edu/fmti/" target="_blank" rel="noopener noreferrer">The Foundation Model Transparency Index</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="common-nlp-tasks">Common NLP tasks<a href="#common-nlp-tasks" class="hash-link" aria-label="Direct link to Common NLP tasks" title="Direct link to Common NLP tasks">‚Äã</a></h2>
<ul>
<li>Summarization</li>
<li>Sentiment analysis</li>
<li>Translation</li>
<li><a href="https://huggingface.co/tasks/zero-shot-classification" target="_blank" rel="noopener noreferrer">Zero-shot classification</a>: A model is trained on a set of labeled examples but is then able to classify new examples from previously unseen classes.</li>
<li>Few-shot learning</li>
<li>Conversation / chat</li>
<li>Question-answering</li>
<li>Text classification</li>
<li>Text generation</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="choose-the-right-llm">Choose the right LLM<a href="#choose-the-right-llm" class="hash-link" aria-label="Direct link to Choose the right LLM" title="Direct link to Choose the right LLM">‚Äã</a></h2>
<p>There is no ‚Äúperfect‚Äù model. Trade-offs are required.</p>
<p>Decision criteria</p>
<ul>
<li>Privacy</li>
<li>Quality</li>
<li>Cost</li>
<li>Latency</li>
<li>Customizability</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="training-llms">Training LLMs<a href="#training-llms" class="hash-link" aria-label="Direct link to Training LLMs" title="Direct link to Training LLMs">‚Äã</a></h2>
<p>There&#x27;s essentially three (3) approaches to training LLMs: pre-training, fine-tuning, and LoRA.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="fine-tuning">Fine-tuning<a href="#fine-tuning" class="hash-link" aria-label="Direct link to Fine-tuning" title="Direct link to Fine-tuning">‚Äã</a></h3>
<ul>
<li><a href="https://rentry.org/llm-training" target="_blank" rel="noopener noreferrer">The Novice&#x27;s LLM Training Guide</a>.</li>
<li><a href="https://www.youtube.com/watch?v=kmkcNVvEz-k" target="_blank" rel="noopener noreferrer">How to Fine-Tune Mistral 7B on Your Own Data</a> with QLoRA.<!-- -->
<ul>
<li><a href="https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb" target="_blank" rel="noopener noreferrer">Fine-tuning Mistral on your own data</a></li>
</ul>
</li>
<li><a href="https://www.youtube.com/watch?v=ahnGLM-RC1Y" target="_blank" rel="noopener noreferrer">A Survey of Techniques for Maximizing LLM Performance</a></li>
<li><a href="https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?" target="_blank" rel="noopener noreferrer">How to Fine-Tune LLMs in 2024 with Hugging Face</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="embeddings">Embeddings<a href="#embeddings" class="hash-link" aria-label="Direct link to Embeddings" title="Direct link to Embeddings">‚Äã</a></h2>
<ul>
<li><a href="https://aman.ai/primers/ai/word-vectors/" target="_blank" rel="noopener noreferrer">Word Vectors/Embeddings</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="techniques">Techniques<a href="#techniques" class="hash-link" aria-label="Direct link to Techniques" title="Direct link to Techniques">‚Äã</a></h2>
<ul>
<li>Tokenization: Transforming text into word-pieces.</li>
<li>Word Embeddings: Represent words with vectors.</li>
<li>Parameter-efficient fine-tuning (PEFT).<!-- -->
<ul>
<li><a href="https://huggingface.co/blog/peft" target="_blank" rel="noopener noreferrer">ü§ó PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware</a>.</li>
<li>Additive: Keep the foundation model weights frozen and update only the new layer weights.<!-- -->
<ul>
<li>Soft Prompt tuning: Concatenates trainable parameters with the input embeddings.</li>
<li>Prefix tuning: Adding tunable layer to each transformer block, rather than just the input layer.</li>
</ul>
</li>
<li>Selective.<!-- -->
<ul>
<li><a href="https://arxiv.org/abs/2106.10199" target="_blank" rel="noopener noreferrer">BitFit</a>: Only updates bias parameters.</li>
<li><a href="https://aclanthology.org/2021.acl-long.378/" target="_blank" rel="noopener noreferrer">Diff Pruning</a>: Create task-specific &quot;diff&quot; vectors and only updates them</li>
</ul>
</li>
<li>Re-parametrization: Decompose weight matrix updates into smaller-rank matrices.<!-- -->
<ul>
<li><a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="noopener noreferrer">Low-Rank Adaptation (LoRA)</a>, aim to refine a relatively small subset of
parameters, thereby minimizing resource utilization and accelerating the training cycle.</li>
<li>LoRA with ZeRO-3</li>
<li><a href="https://arxiv.org/abs/2205.05638" target="_blank" rel="noopener noreferrer">(IA)</a>.</li>
<li><a href="https://arxiv.org/abs/2307.05695" target="_blank" rel="noopener noreferrer">ReLoRA: High-Rank Training Through Low-Rank Updates</a></li>
</ul>
</li>
<li><a href="https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/finetune_adapter.md" target="_blank" rel="noopener noreferrer">Finetune with Adapters</a>.</li>
</ul>
</li>
<li>Faster calculations<!-- -->
<ul>
<li><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener noreferrer">Flash Attention!</a>: Calculating attention in a flash, provides drastic speedups over standard attention through clever hardware optimization.</li>
</ul>
</li>
<li>Improving Model Footprint<!-- -->
<ul>
<li>Quantization<!-- -->
<ul>
<li><a href="https://github.com/timdettmers/bitsandbytes" target="_blank" rel="noopener noreferrer">bitsandbytes</a> (4-bit quantization).</li>
<li>Quantized Low-Rank Adaptation (QLoRA)</li>
<li><a href="https://huggingface.co/docs/optimum/concept_guides/quantization#going-further-how-do-machines-represent-numbers" target="_blank" rel="noopener noreferrer">Quantization</a></li>
</ul>
</li>
<li><a href="https://github.com/ggerganov/ggml" target="_blank" rel="noopener noreferrer">GGML</a>.</li>
<li><strong>GGUF</strong> is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML,
which is no longer supported by llama.cpp.</li>
<li><a href="https://towardsdatascience.com/democratizing-llms-4-bit-quantization-for-optimal-llm-inference-be30cf4e0e34" target="_blank" rel="noopener noreferrer">Democratizing LLMs: 4-bit Quantization for Optimal LLM Inference</a></li>
<li><a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="noopener noreferrer">GPTQ</a> is a post-training quantziation method to compress LLMs.<!-- -->
<ul>
<li>GPTQ compresses GPT models by reducing the number of bits needed to store each weight in the model, from 32 bits down to just 3-4 bits.</li>
<li>GPTQ analyzes each layer of the model separately and approximating the weights in a way that preserves the overall accuracy.</li>
<li>See <a href="https://www.philschmid.de/gptq-llama" target="_blank" rel="noopener noreferrer">Optimize open LLMs using GPTQ and Hugging Face Optimum</a>.</li>
<li><a href="https://huggingface.co/blog/gptq-integration" target="_blank" rel="noopener noreferrer">AutoGPTQ</a>.</li>
</ul>
</li>
</ul>
</li>
<li>Multi-LLM Inferencing<!-- -->
<ul>
<li>Mixture-of-Experts (MoE) and <a href="https://arxiv.org/abs/2101.03961" target="_blank" rel="noopener noreferrer">Switch Transformer</a>.</li>
<li>LLM Cascades and <a href="https://arxiv.org/abs/2305.05176" target="_blank" rel="noopener noreferrer">FrugalGPT</a>.</li>
</ul>
</li>
<li>Multi-modal Language Models (MLLMs)<!-- -->
<ul>
<li>Chain-of-Thought MLLMs</li>
<li><a href="https://arxiv.org/pdf/2201.11903.pdf" target="_blank" rel="noopener noreferrer">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>.</li>
<li><a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html" target="_blank" rel="noopener noreferrer">Language Models Perform Reasoning via Chain of Thought</a>.</li>
</ul>
</li>
<li>Chain of Density prompt</li>
<li><a href="https://rentry.org/llm-training" target="_blank" rel="noopener noreferrer">The Novice&#x27;s LLM Training Guide</a>.</li>
<li>Reinforcement learning with human feedback (RLHF)</li>
<li>Retrieval Augmented Generation (RAG)<!-- -->
<ul>
<li><a href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6" target="_blank" rel="noopener noreferrer">Advanced RAG Techniques: an Illustrated Overview</a></li>
<li><a href="https://arxiv.org/abs/2312.10997v1" target="_blank" rel="noopener noreferrer">Retrieval-Augmented Generation for Large Language Models: A Survey</a></li>
<li><a href="https://github.com/Tongji-KGLLM/RAG-Survey" target="_blank" rel="noopener noreferrer">RAG-Survey</a></li>
<li><a href="https://aman.ai/primers/ai/RAG/" target="_blank" rel="noopener noreferrer">NLP ‚Ä¢ Retrieval Augmented Generation</a></li>
</ul>
</li>
</ul>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Low Rank Adaptation (LoRA)</div><div class="admonitionContent_BuS1"><p>Want to train a specialized LLM on your own data?
The easiest way to do this is with low rank adaptation (LoRA), but many variants of LoRA exist. Here‚Äôs an overview
of all (or at least most) of the techniques that are out there‚Ä¶</p><p><strong>LoRA</strong> models the update derived for a model‚Äôs weights during finetuning with a low rank decomposition, implemented
in practice as a pair of linear projections. LoRA leaves the pretrained layers of the LLM fixed and injects a
trainable rank decomposition matrix into each layer of the model.</p><p><strong>QLoRA</strong> is (arguably) the most popular LoRA variant and uses model quantization techniques to reduce memory usage
during finetuning while maintaining (roughly) equal levels of performance. QLoRA uses 4-bit quantization on the
pretrained model weights and trains LoRA modules on top of this. In practice, QLoRA saves memory at the cost of
slightly-reduced training speed.</p><p><strong>QA-LoRA</strong> is an extension of LoRA/QLoRA that further reduces the computational burden of training and deploying LLMs.
It does this by combining parameter-efficient finetuning with quantization (i.e., group-wise quantization applied
during training/inference).</p><p><strong>LoftQ</strong> studies a similar idea to QA-LoRA‚Äîapplying quantization and LoRA finetuning on a pretrained model simultaneously.</p><p><strong>LongLoRA</strong> attempts to cheaply adapt LLMs to longer context lengths using a parameter-efficient (LoRA-based) finetuning
scheme. In particular, we start with a pretrained model and finetune it to have a longer context length. This finetuning is made efficient by:</p><ul>
<li>Using sparse local attention instead of dense global attention (optional at inference time).</li>
<li>Using LoRA (authors find that this works well for context extension).</li>
</ul><p><strong>S-LoRA</strong> aims to solve the problem of deploying multiple LoRA modules that are used to adapt the same pretrained model
to a variety of different tasks. Put simply, S-LoRA does the following to serve thousands of LoRA modules on a single GPU (or across GPUs):</p><ul>
<li>Stores all LoRA modules in main memory.</li>
<li>Puts modules being used to run the current query into GPU memory.</li>
<li>Uses unified paging to allocate GPU memory and avoid fragmentation.</li>
<li>Proposes a new tensor parallelism strategy to batch LoRA computations.</li>
</ul><p>Many other LoRA variants exist as well‚Ä¶</p><ul>
<li><strong>LQ-LoRA</strong>: uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget.</li>
<li><strong>MultiLoRA</strong>: extension of LoRA that better handles complex multi-task learning scenarios.</li>
<li><strong>LoRA-FA</strong>: freezes half of the low-rank decomposition matrix (i.e., the A matrix within the product AB) to further reduce memory overhead.</li>
<li><strong>Tied-LoRA</strong>: leverages weight tying to further improve the parameter efficiency of LoRA.</li>
<li><strong>GLoRA</strong>: extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer.</li>
</ul><p>See also : <a href="https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft" target="_blank" rel="noopener noreferrer">Easily Train a Specialized LLM: PEFT, LoRA, QLoRA, LLaMA-Adapter, and More</a>.</p><p>Source : <a href="https://x.com/cwolferesearch/status/1736795049579491751?s=20" target="_blank" rel="noopener noreferrer">here</a>, 2023-12-18.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="tools">Tools<a href="#tools" class="hash-link" aria-label="Direct link to Tools" title="Direct link to Tools">‚Äã</a></h2>
<ul>
<li><a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">ü§ó Hugging Face</a>: The GitHub of Large Language Models<!-- -->
<ul>
<li><a href="https://huggingface.co/datasets" target="_blank" rel="noopener noreferrer">Datasets</a>, pipelines, tokenizers, and <a href="https://huggingface.co/models" target="_blank" rel="noopener noreferrer">models</a>.</li>
<li><a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener noreferrer">ü§ó Hugging Face Transformers</a>.</li>
<li><a href="https://huggingface.co/docs/transformers/transformers_agents" target="_blank" rel="noopener noreferrer">Transformers Agent</a>.</li>
</ul>
</li>
<li><a href="https://www.nltk.org/" target="_blank" rel="noopener noreferrer">NLTK</a>.</li>
<li><a href="https://spacy.io/" target="_blank" rel="noopener noreferrer">SpaCy</a>.</li>
<li><a href="https://radimrehurek.com/gensim/" target="_blank" rel="noopener noreferrer">Gensim</a>.</li>
<li><a href="https://pypi.org/project/openai/" target="_blank" rel="noopener noreferrer">OpenAI</a>.<!-- -->
<ul>
<li>OpenAI <a href="https://platform.openai.com/docs/api-reference" target="_blank" rel="noopener noreferrer">API Reference</a>.</li>
</ul>
</li>
<li><a href="https://github.com/hwchase17/langchain" target="_blank" rel="noopener noreferrer">ü¶úÔ∏èüîó LangChain</a>.</li>
<li>Microsoft <a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener noreferrer">DeepSpeed</a>: Optimization library.</li>
<li><a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">llama.cpp</a>: Port of Facebook&#x27;s LLaMA model in C/C++.</li>
<li><a href="https://ollama.ai/" target="_blank" rel="noopener noreferrer">Ollama</a>.</li>
<li><a href="https://python.langchain.com/docs/guides/langsmith/" target="_blank" rel="noopener noreferrer">LangSmith</a>.</li>
<li><a href="https://github.com/explodinggradients/ragas" target="_blank" rel="noopener noreferrer">Ragas</a>: Evaluation framework for Retrieval Augmented Generation (RAG) pipelines.</li>
<li><a href="https://github.com/bigscience-workshop/petals" target="_blank" rel="noopener noreferrer">Petals</a> a system for inference and fine-tuning of large models
collaboratively by joining the resources of multiple parties [<a href="https://arxiv.org/pdf/2209.01188.pdf" target="_blank" rel="noopener noreferrer">Paper</a>].</li>
<li><a href="https://www.mendable.ai/" target="_blank" rel="noopener noreferrer">Mendable.ai</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Ô∏è-langchain">ü¶úÔ∏èüîó LangChain<a href="#Ô∏è-langchain" class="hash-link" aria-label="Direct link to ü¶úÔ∏èüîó LangChain" title="Direct link to ü¶úÔ∏èüîó LangChain">‚Äã</a></h3>
<p><a href="https://python.langchain.com/docs/get_started/introduction.html" target="_blank" rel="noopener noreferrer">ü¶úÔ∏èüîó LangChain</a> is a framework
for developing applications powered by language models.</p>
<p>Released in late 2022. Useful for multi-stage reasoning, LLM-based workflows</p>
<p>The core idea of the library is that we can ‚Äúchain‚Äù together different components to create more advanced use cases around LLMs. Chains may consist of multiple components from several modules:</p>
<ul>
<li><strong>Prompt templates</strong>: Prompt templates are templates for different types of prompts. Like ‚Äúchatbot‚Äù style templates, ELI5 question-answering, etc</li>
<li><strong>LLMs</strong>: Large language models like GPT-3, BLOOM, etc</li>
<li><strong>Agents</strong>: Agents use LLMs to decide what actions should be taken. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations.</li>
<li><strong>Memory</strong>: Short-term memory, long-term memory.</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="https://api.python.langchain.com/en/latest/api_reference.html" target="_blank" rel="noopener noreferrer">API Reference</a>.</li>
<li><a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener noreferrer">Github</a>.</li>
<li><a href="https://blog.langchain.dev/" target="_blank" rel="noopener noreferrer">Blog</a>.</li>
<li><a href="https://www.pinecone.io/learn/series/langchain/" target="_blank" rel="noopener noreferrer">LangChain AI Handbook</a>.</li>
<li><a href="https://github.com/logspace-ai/langflow" target="_blank" rel="noopener noreferrer">‚õìÔ∏è Langflow</a>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llamaindex">LlamaIndex<a href="#llamaindex" class="hash-link" aria-label="Direct link to LlamaIndex" title="Direct link to LlamaIndex">‚Äã</a></h3>
<p><a href="https://gpt-index.readthedocs.io/" target="_blank" rel="noopener noreferrer">LlamaIndex</a> is a toolkit for building LLM-powered applications over custom data. It consists of two stages: the indexing stage, where the knowledge base is prepared using data connectors and indexes, and the querying stage, where relevant context is retrieved from the knowledge base to assist the LLM in responding to a question. LlamaIndex provides building blocks such as retrievers, node postprocessors, and response synthesizers, as well as pipelines like query engines, chat engines, and agents.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="others">Others<a href="#others" class="hash-link" aria-label="Direct link to Others" title="Direct link to Others">‚Äã</a></h3>
<ul>
<li><a href="https://github.com/microsoft/LLMLingua" target="_blank" rel="noopener noreferrer">LLMLingua</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="evaluation">Evaluation<a href="#evaluation" class="hash-link" aria-label="Direct link to Evaluation" title="Direct link to Evaluation">‚Äã</a></h2>
<p>A good language will model will have high accuracy and low perplexity.
Accuracy = next word is right or wrong. Perplexity = how confident was that choice.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/BLEU" target="_blank" rel="noopener noreferrer">BLEU</a> (bilingual evaluation understudy) is an algorithm for evaluating the quality of
text which has been machine-translated from one natural language to another.</li>
<li><a href="https://aclanthology.org/W04-1013.pdf" target="_blank" rel="noopener noreferrer">ROUGE</a> for summarization.
See also <a href="https://huggingface.co/spaces/evaluate-metric/bleu" target="_blank" rel="noopener noreferrer">here</a>.</li>
<li>sacreBLEU, TER, ChrF, ChrF++, BERTScore, METEOR, and Semantic Similarity</li>
<li><a href="https://github.com/openai/evals" target="_blank" rel="noopener noreferrer">Evals</a> is a framework for evaluating LLMs and LLM systems,
and an open-source registry of benchmarks.</li>
<li><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" target="_blank" rel="noopener noreferrer">ü§ó Open LLM Leaderboard</a>.<!-- -->
<ul>
<li><a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard" target="_blank" rel="noopener noreferrer">What&#x27;s going on with the Open LLM Leaderboard?</a>.</li>
</ul>
</li>
<li><a href="https://huggingface.co/spaces/bigcode/multilingual-code-evals" target="_blank" rel="noopener noreferrer">ü§ó Multilingual Code Models Evaluation</a>.</li>
<li><a href="https://www.mosaicml.com/llm-evaluation" target="_blank" rel="noopener noreferrer">Mosaic LLM Evaluation Leaderboard</a>.</li>
<li><a href="https://github.com/microsoft/promptbench" target="_blank" rel="noopener noreferrer">A unified evaluation framework for large language models</a>.</li>
<li><a href="https://chat.lmsys.org/?arena" target="_blank" rel="noopener noreferrer">Chatbot Arena: Benchmarking LLMs in the Wild</a></li>
</ul>
<p>See also <a href="https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI" target="_blank" rel="noopener noreferrer">Local LLM Comparison &amp; Colab Links</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="papers">Papers<a href="#papers" class="hash-link" aria-label="Direct link to Papers" title="Direct link to Papers">‚Äã</a></h2>
<ul>
<li><strong>Attention is all you need</strong>, NIPS 2017. [<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">Paper</a>].</li>
<li><strong>A Survey of Large Language Models</strong>, arXiv 2023. [<a href="https://arxiv.org/abs/2303.18223" target="_blank" rel="noopener noreferrer">Paper</a>, <a href="https://github.com/RUCAIBox/LLMSurvey" target="_blank" rel="noopener noreferrer">GitHub</a>].</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="instructions">Instructions<a href="#instructions" class="hash-link" aria-label="Direct link to Instructions" title="Direct link to Instructions">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="using-llama-2-on-mac-m1m2">Using Llama 2 on Mac M1/M2<a href="#using-llama-2-on-mac-m1m2" class="hash-link" aria-label="Direct link to Using Llama 2 on Mac M1/M2" title="Direct link to Using Llama 2 on Mac M1/M2">‚Äã</a></h3>
<blockquote>
<p>Created 2023-08-26. Last updated 2023-08-26.</p>
</blockquote>
<ol>
<li>Download the original version of<a href="https://github.com/facebookresearch/llama" target="_blank" rel="noopener noreferrer">Llama</a> and extract it to a <code>llama-main</code> folder.</li>
<li>Download the cpu version from <a href="https://github.com/krychu/llama" target="_blank" rel="noopener noreferrer">https://github.com/krychu/llama</a>, extract it and replace files in the <code>llama-main</code> folder.</li>
<li>Go to the <code>llama-main</code> folder.</li>
<li>Follow the instructions in the <a href="https://github.com/facebookresearch/llama" target="_blank" rel="noopener noreferrer">README</a> to run the <code>download.sh</code> script.
<a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" rel="noopener noreferrer">Request a new download link</a>.
Then run the <code>download.sh</code> script, passing the URL provided when prompted to start the download.</li>
<li>Create a virtual environment with <code>python3 -m venv env</code> and activate it with <code>source env/bin/activate</code>.</li>
<li>Install the cpu version of pytorch with <code>python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu</code>.</li>
<li>Install dependencies of llama with <code>python3 -m pip install -e .</code>.</li>
<li>Run <code>torchrun</code> like below.</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ git clone https://github.com/facebookresearch/llama</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ mv llama llama-main</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ git clone https://github.com/krychu/llama</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ cp -r llama/ llama-main/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ cd llama-main</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ chmod u+x download.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ ./download.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Enter the URL from email: https://download.llamameta.net/*?Policy=eyJTdGF0Z...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ python3 -m venv env</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ source env/bin/activate</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ python3 -m pip install torch torchvision torchaudio \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --index-url https://download.pytorch.org/whl/cpu</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ python3 -m pip install -e .</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ torchrun --nproc_per_node 1 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  example_text_completion.py \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --ckpt_dir llama-2-7b/ \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --tokenizer_path tokenizer.model \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --max_seq_len 128 --max_batch_size 1 #(instead of 4)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>I get a failure with this message: <code>RuntimeError: Distributed package doesn&#x27;t have NCCL built in</code></p>
<p>Instead, people are using <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">https://github.com/ggerganov/llama.cpp</a>, which is a Port of Facebook&#x27;s LLaMA model in C/C++.</p>
<p>Found this link <a href="https://gist.github.com/cedrickchee/e8d4cb0c4b1df6cc47ce8b18457ebde0" target="_blank" rel="noopener noreferrer">https://gist.github.com/cedrickchee/e8d4cb0c4b1df6cc47ce8b18457ebde0</a>. Will try this.</p>
<p>Reference:</p>
<ul>
<li><a href="https://github.com/facebookresearch/llama" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/llama</a></li>
<li><a href="https://github.com/facebookresearch/llama/issues/433#issuecomment-1650002750" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/llama/issues/433#issuecomment-1650002750</a></li>
<li><a href="https://github.com/krychu/llama" target="_blank" rel="noopener noreferrer">https://github.com/krychu/llama</a></li>
<li><a href="https://github.com/aggiee/llama-v2-mps" target="_blank" rel="noopener noreferrer">https://github.com/aggiee/llama-v2-mps</a></li>
<li><a href="https://ryandam.net/blog/2023/8/2/using-llama2/index.html" target="_blank" rel="noopener noreferrer">https://ryandam.net/blog/2023/8/2/using-llama2/index.html</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="courses">Courses<a href="#courses" class="hash-link" aria-label="Direct link to Courses" title="Direct link to Courses">‚Äã</a></h2>
<ul>
<li><a href="https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/" target="_blank" rel="noopener noreferrer">The Full Stack LLM Bootcamp</a>.</li>
<li>Standford <a href="https://stanford-cs324.github.io/winter2022/" target="_blank" rel="noopener noreferrer">CS324 - Large Language Models</a>.</li>
<li><a href="https://docs.cohere.com/docs/llmu" target="_blank" rel="noopener noreferrer">cohere</a></li>
<li><a href="https://learnprompting.org/" target="_blank" rel="noopener noreferrer">Learn Prompting</a>.</li>
<li><a href="https://www.wandb.courses/courses/building-llm-powered-apps" target="_blank" rel="noopener noreferrer">Building LLM-Powered Apps</a> from WandB.</li>
<li><a href="https://learn.activeloop.ai/courses/langchain" target="_blank" rel="noopener noreferrer">LangChain &amp; Vector Databases in Production</a> from ActiveLoop.</li>
<li><a href="https://rentry.org/llm-training" target="_blank" rel="noopener noreferrer">The Novice&#x27;s LLM Training Guide</a></li>
<li><a href="https://github.com/mlabonne/llm-course?tab=readme-ov-file" target="_blank" rel="noopener noreferrer">llm-course</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-blogs">LLM Blogs<a href="#llm-blogs" class="hash-link" aria-label="Direct link to LLM Blogs" title="Direct link to LLM Blogs">‚Äã</a></h2>
<ul>
<li><a href="https://eugeneyan.com/" target="_blank" rel="noopener noreferrer">Eugene Yan</a>.</li>
<li><a href="https://huyenchip.com/" target="_blank" rel="noopener noreferrer">Chip Huyen</a>.</li>
<li><a href="https://medium.com/@iamleonie" target="_blank" rel="noopener noreferrer">Leonie Monigatti</a>.</li>
<li><a href="https://lilianweng.github.io/" target="_blank" rel="noopener noreferrer">Lilian Weng</a>.</li>
<li><a href="https://logankilpatrick.medium.com/" target="_blank" rel="noopener noreferrer">Logan Kilpatrick</a>.</li>
<li><a href="https://magazine.sebastianraschka.com/" target="_blank" rel="noopener noreferrer">Sebastian Raschka</a>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="see-also">See also<a href="#see-also" class="hash-link" aria-label="Direct link to See also" title="Direct link to See also">‚Äã</a></h2>
<ul>
<li><a href="https://huggingface.co/blog/2023-in-llms" target="_blank" rel="noopener noreferrer">2023, year of open LLMs</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/gpt">GPT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/llama">Llama</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/references/llms/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/references/ml/mlx"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">MLX: Array framework for Apple silicon</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/references/llms/llama"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Llama</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#architectures" class="table-of-contents__link toc-highlight">Architectures</a></li><li><a href="#models" class="table-of-contents__link toc-highlight">Models</a><ul><li><a href="#bert" class="table-of-contents__link toc-highlight">BERT</a></li><li><a href="#databricks-dolly" class="table-of-contents__link toc-highlight">Databricks Dolly</a></li><li><a href="#eleutherai-pythia" class="table-of-contents__link toc-highlight">EleutherAI Pythia</a></li><li><a href="#facebook-llama" class="table-of-contents__link toc-highlight">Facebook Llama</a></li><li><a href="#mistral-7b" class="table-of-contents__link toc-highlight">Mistral-7B</a></li><li><a href="#mosaic-mpt" class="table-of-contents__link toc-highlight">Mosaic MPT</a></li><li><a href="#openai-gpt" class="table-of-contents__link toc-highlight">OpenAI GPT</a></li><li><a href="#poe" class="table-of-contents__link toc-highlight">Poe</a></li><li><a href="#stanford-alpaca" class="table-of-contents__link toc-highlight">Stanford Alpaca</a></li><li><a href="#tii-falcon" class="table-of-contents__link toc-highlight">TII Falcon</a></li><li><a href="#stablelm" class="table-of-contents__link toc-highlight">StableLM</a></li><li><a href="#vicuna" class="table-of-contents__link toc-highlight">Vicuna</a></li><li><a href="#wizardlm" class="table-of-contents__link toc-highlight">WizardLM</a></li><li><a href="#tinyllama" class="table-of-contents__link toc-highlight">TinyLlama</a></li></ul></li><li><a href="#chatbot" class="table-of-contents__link toc-highlight">ChatBot</a></li><li><a href="#code-llms" class="table-of-contents__link toc-highlight">Code LLMs</a></li><li><a href="#leaderboard" class="table-of-contents__link toc-highlight">Leaderboard</a></li><li><a href="#common-nlp-tasks" class="table-of-contents__link toc-highlight">Common NLP tasks</a></li><li><a href="#choose-the-right-llm" class="table-of-contents__link toc-highlight">Choose the right LLM</a></li><li><a href="#training-llms" class="table-of-contents__link toc-highlight">Training LLMs</a><ul><li><a href="#fine-tuning" class="table-of-contents__link toc-highlight">Fine-tuning</a></li></ul></li><li><a href="#embeddings" class="table-of-contents__link toc-highlight">Embeddings</a></li><li><a href="#techniques" class="table-of-contents__link toc-highlight">Techniques</a></li><li><a href="#tools" class="table-of-contents__link toc-highlight">Tools</a><ul><li><a href="#Ô∏è-langchain" class="table-of-contents__link toc-highlight">ü¶úÔ∏èüîó LangChain</a></li><li><a href="#llamaindex" class="table-of-contents__link toc-highlight">LlamaIndex</a></li><li><a href="#others" class="table-of-contents__link toc-highlight">Others</a></li></ul></li><li><a href="#evaluation" class="table-of-contents__link toc-highlight">Evaluation</a></li><li><a href="#papers" class="table-of-contents__link toc-highlight">Papers</a></li><li><a href="#instructions" class="table-of-contents__link toc-highlight">Instructions</a><ul><li><a href="#using-llama-2-on-mac-m1m2" class="table-of-contents__link toc-highlight">Using Llama 2 on Mac M1/M2</a></li></ul></li><li><a href="#courses" class="table-of-contents__link toc-highlight">Courses</a></li><li><a href="#llm-blogs" class="table-of-contents__link toc-highlight">LLM Blogs</a></li><li><a href="#see-also" class="table-of-contents__link toc-highlight">See also</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2024 Alain Boisvert. Construit avec Docusaurus.</div></div></div></footer></div>
</body>
</html>