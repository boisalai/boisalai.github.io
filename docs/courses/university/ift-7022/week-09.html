<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-courses/university/ift-7022/week-09" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">9 Deep NLP - Introduction aux modèles Transformers | Alain Boisvert</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://boisalai.github.io/docs/courses/university/ift-7022/week-09"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="9 Deep NLP - Introduction aux modèles Transformers | Alain Boisvert"><meta data-rh="true" name="description" content="Références :"><meta data-rh="true" property="og:description" content="Références :"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://boisalai.github.io/docs/courses/university/ift-7022/week-09"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/courses/university/ift-7022/week-09" hreflang="en"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/courses/university/ift-7022/week-09" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Alain Boisvert RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Alain Boisvert Atom Feed">



<link rel="stylesheet" href="/fonts/font-awesome/fontawesome.css">
<link rel="stylesheet" href="/fonts/font-awesome/solid.css">
<link rel="stylesheet" href="/fonts/font-awesome/regular.css">
<link rel="stylesheet" href="/fonts/font-awesome/brands.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.6520ee1d.css">
<script src="/assets/js/runtime~main.089b18cf.js" defer="defer"></script>
<script src="/assets/js/main.61e0bf1e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Alain Boisvert</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" target="" href="/docs/courses/university/ift-7022">IFT-7022</a><a class="navbar__item navbar__link" target="" href="/docs/courses/university/gif-7005">GIF-7005</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/boisalai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-github"> </a><a href="https://www.linkedin.com/in/alain-boisvert-98b058156/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-linkedin-in"> </a><a href="mailto:ay.boisvert@gmail.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-solid fa-envelope"> </a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Alain Boisvert</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/cv">Curriculum vitæ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/learning">Learning path</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/certificates">Certificates</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/courses/university/gif-7005">Courses</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/courses/university/gif-7005">Université Laval</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/courses/university/gif-7005">GIF-7005 Introduction à l&#x27;apprentissage automatique</a><button aria-label="Expand sidebar category &#x27;GIF-7005 Introduction à l&#x27;apprentissage automatique&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/docs/courses/university/ift-7022">IFT-7022 Traitement automatique de la langue naturelle</a><button aria-label="Collapse sidebar category &#x27;IFT-7022 Traitement automatique de la langue naturelle&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-01">1 Expressions régulières</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-02">2 Prétraitement de textes et distance minimale d&#x27;édition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-03">3 Modèles de langue N-grammes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-04">4 Classification de textes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-05">5 Sémantique vectorielle (représentation des mots)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-06">6 Deep NLP + Plongements de mots (word embeddings) + intro aux réseaux de neurones</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-07">7 Étiquetage de séquences - analyse grammaticale (Part of speech tagging) et reconnaissance d&#x27;entités nommées</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-08-partie-1">8 Deep NLP - Réseaux récurrents pour le traitement de séquences (RNN, GRU, LSTM)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-08-partie-2">8 Notions de base pour les RNN, GRU et LSTM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/courses/university/ift-7022/week-09">9 Deep NLP - Introduction aux modèles Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-10">10 Deep NLP - Plongements contextuels et modèles encodeurs pré-entraînés</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-11">11 Deep NLP - Traduction automatique et modèle encodeur-décodeur</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-12">12 Deep NLP - Systèmes question-réponse (QA)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-13">13 Deep NLP - Introduction aux LLMs, prompting et instruct tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-1">Travail pratique 1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-2">Travail pratique 2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-3">Travail pratique 3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/exam-intra">Examen de mi-session</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/exam-final">Examen final</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/gif-7105">GIF-7105 Photographie algorithmique</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/courses/deeplearning-ai/p01-chatgpt-building-systems">DeepLearning.AI</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/courses/activeloop/rag-for-production">Activeloop</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/fine-tuning-llms">Training and fine-tuning LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/python-gpu">Best practices CPU and GPU</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/models/test">Interactive models</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/references/links">References</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Courses</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Université Laval</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/courses/university/ift-7022"><span itemprop="name">IFT-7022 Traitement automatique de la langue naturelle</span></a><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">9 Deep NLP - Introduction aux modèles Transformers</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>9 Deep NLP - Introduction aux modèles Transformers</h1>
<p>Références :</p>
<ul>
<li><a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf" target="_blank" rel="noopener noreferrer">Chapitre 10</a>, sections 10.1 et 10.2</li>
<li><a href="https://huggingface.co/docs/transformers/main/en/index" target="_blank" rel="noopener noreferrer">Transformers</a></li>
<li><a href="https://pytorch.org/hub/huggingface_pytorch-transformers/" target="_blank" rel="noopener noreferrer">PyTorch-Transformers</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="plan-de-la-présentation">Plan de la présentation<a href="#plan-de-la-présentation" class="hash-link" aria-label="Direct link to Plan de la présentation" title="Direct link to Plan de la présentation">​</a></h2>
<ul>
<li>Lacunes des réseaux récurrents</li>
<li>Transformers – la base</li>
<li>Transformers – les autres composantes</li>
<li>Modèle de langue avec un <em>transformer</em></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="lacunes-des-réseaux-récurrents">Lacunes des réseaux récurrents<a href="#lacunes-des-réseaux-récurrents" class="hash-link" aria-label="Direct link to Lacunes des réseaux récurrents" title="Direct link to Lacunes des réseaux récurrents">​</a></h2>
<ul>
<li>Traitement de <strong>gauche à droite</strong>
<ul>
<li>Le modèle bidirectionnel est une alternative</li>
</ul>
</li>
<li>Les <strong>dépendances</strong> entre mots<!-- -->
<ul>
<li>Les liens entre les mots &quot;distants&quot; passent par des états cachés</li>
<li>Pour les mots lointains, plusieurs étapes avant d&#x27;établir un lien</li>
<li>Nécessite le calcul de plusieurs états cachés avec le réseau récurrent pour établir les liens</li>
<li>Pas certain que l&#x27;information se propage bien</li>
</ul>
</li>
<li>Traitement <strong>séquentiel</strong>
<ul>
<li>Difficile à paralléliser</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="transformers--idées-principales">Transformers – Idées principales<a href="#transformers--idées-principales" class="hash-link" aria-label="Direct link to Transformers – Idées principales" title="Direct link to Transformers – Idées principales">​</a></h2>
<ul>
<li>Une <strong>fenêtre de mots</strong>
<ul>
<li>Pas de restriction gauche-droite ou droite-gauche...</li>
<li>Traiter tous les mots conjointement<!-- -->
<ul>
<li>Pas de traitement séquentiel (récurrence)</li>
</ul>
</li>
</ul>
</li>
<li>Les dépendances entre les mots capturées par un <strong>mécanisme d&#x27;attention</strong></li>
</ul>
<p><img loading="lazy" alt="s71" src="/assets/images/s71-82de322cadc36d79abd4b5fc1a995d31.png" width="1882" height="1006" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="transformer---la-base">Transformer - La base<a href="#transformer---la-base" class="hash-link" aria-label="Direct link to Transformer - La base" title="Direct link to Transformer - La base">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="transformer---self-attention">Transformer - Self-attention<a href="#transformer---self-attention" class="hash-link" aria-label="Direct link to Transformer - Self-attention" title="Direct link to Transformer - Self-attention">​</a></h3>
<p><img loading="lazy" alt="s72" src="/assets/images/s72-5c774065fae3f42a9dd88571e5fe2248.png" width="1604" height="854" class="img_ev3q"></p>
<p><img loading="lazy" alt="s73" src="/assets/images/s73-108a6b74f94a6c7c79f59b9b335ef7db.png" width="1608" height="1076" class="img_ev3q"></p>
<p><img loading="lazy" alt="s74" src="/assets/images/s74-b6e97caacd786a15be8d4def1325e118.png" width="1660" height="898" class="img_ev3q"></p>
<p><img loading="lazy" alt="s75" src="/assets/images/s75-b64bfaa9562a8270e220a28175fb032a.png" width="1716" height="850" class="img_ev3q"></p>
<ul>
<li>Utilisation du produit scalaire pour les scores d&#x27;attention<!-- -->
<ul>
<li>Version simplifiée</li>
</ul>
</li>
<li>Un mot peut jouer 3 rôles :<!-- -->
<ul>
<li><strong>La requête (query)</strong> : Le mot cible, le centre d&#x27;attention</li>
<li><strong>La clé (key)</strong> : le mot de contexte comparé au mot cible actuel</li>
<li><strong>La valeur (value)</strong> : le vecteur qu&#x27;on récupère pour calculer l&#x27;output <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">y</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{y}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6886em;vertical-align:-0.2441em"></span><span class="mord"><span class="mord text"><span class="mord textbf">y</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2175em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span></span></span></span> du mot cible</li>
</ul>
</li>
<li>On n&#x27;a pas à se limiter à une seule représentation pour les 3 rôles<!-- -->
<ul>
<li>Idée : faire une transformation linéaire selon le rôle</li>
<li>Dans le graphique ci-dessous, il y a des erreurs d&#x27;indices. Nous devrions voir:</li>
</ul>
</li>
</ul>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>(5) </mtext><msub><mtext mathvariant="bold">v</mtext><mi>j</mi></msub><mo>=</mo><msup><mtext mathvariant="bold">W</mtext><mtext mathvariant="bold">V</mtext></msup><msub><mtext mathvariant="bold">x</mtext><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\text{(5) }\textbf{v}_j = \textbf{W}^\textbf{V} \textbf{x}_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord text"><span class="mord">(5) </span></span><span class="mord"><span class="mord text"><span class="mord textbf">v</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2054em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9193em"><span style="top:-3.139em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord textbf mtight">V</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord text"><span class="mord textbf">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>(2) </mtext><msub><mtext mathvariant="bold">k</mtext><mi>j</mi></msub><mo>=</mo><msup><mtext mathvariant="bold">W</mtext><mtext mathvariant="bold">K</mtext></msup><msub><mtext mathvariant="bold">x</mtext><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\text{(2) }\textbf{k}_j = \textbf{W}^\textbf{K} \textbf{x}_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord text"><span class="mord">(2) </span></span><span class="mord"><span class="mord text"><span class="mord textbf">k</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2054em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9193em"><span style="top:-3.139em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord textbf mtight">K</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord text"><span class="mord textbf">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span>
<p><img loading="lazy" alt="s76" src="/assets/images/s76-f522b402c07c0a2551beae7df0f1f3a2.png" width="1508" height="1038" class="img_ev3q"></p>
<p><img loading="lazy" alt="s77" src="/assets/images/s77-d33c96f619a357e5a81bb2fbd2f8fc74.png" width="1808" height="1084" class="img_ev3q"></p>
<p><img loading="lazy" alt="s78" src="/assets/images/s78-b47652fcef5242a5269f752e80f755ea.png" width="1306" height="1146" class="img_ev3q"></p>
<p><img loading="lazy" alt="s79" src="/assets/images/s79-547fa6450aa9745989b7e6cbc35ff241.png" width="1548" height="1098" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="transformer---bloc">Transformer - Bloc<a href="#transformer---bloc" class="hash-link" aria-label="Direct link to Transformer - Bloc" title="Direct link to Transformer - Bloc">​</a></h3>
<ul>
<li>La couche de self-attention fait partie d&#x27;un bloc de transformer<!-- -->
<ul>
<li>Un bloc de traitement qui converti les <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span> en <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></li>
</ul>
</li>
<li>Si on examine le calcul de l&#x27;attention<!-- -->
<ul>
<li>Il n&#x27;y a aucune non-linéarité</li>
<li>Une somme pondérée de vecteurs</li>
</ul>
</li>
<li>Pourtant la puissance des réseaux vient de la <strong>non-linéarité</strong></li>
<li>Solution :<!-- -->
<ul>
<li>Ajouter une couche <em>feedforward</em> (FFNN) après le calcul d&#x27;auto-attention</li>
</ul>
</li>
</ul>
<p><img loading="lazy" alt="s80" src="/assets/images/s80-9550f31c36c323d0f8293c15e2ef146a.png" width="1796" height="934" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="la-position-des-motsjetons">La position des mots/jetons<a href="#la-position-des-motsjetons" class="hash-link" aria-label="Direct link to La position des mots/jetons" title="Direct link to La position des mots/jetons">​</a></h3>
<ul>
<li>Un transformer traite tous les mots en même temps sans notion de position<!-- -->
<ul>
<li>Pas de notion de séquence comme dans RNN</li>
</ul>
</li>
<li>De plus, le calcul d&#x27;attention est invariant à la position des mots<!-- -->
<ul>
<li>On peut changer la position de 2 mots et on obtient la même somme pondérée en sortie</li>
</ul>
</li>
<li>Solution : ajouter la position du mot en entrée du réseau<!-- -->
<ul>
<li>Chaque position est représentée par un vecteur de plongement (<em>embedding</em>)</li>
</ul>
</li>
</ul>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mtext>emb</mtext><mo stretchy="false">(</mo><mi>j</mi><mi>e</mi><mi>t</mi><mi>o</mi><msub><mi>n</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mtext>emb</mtext><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>o</mi><msub><mi>n</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_i = \text{emb}(jeton_i) + \text{emb}(position_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">emb</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05724em">j</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">emb</span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord mathnormal">i</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>
<p><img loading="lazy" alt="s81" src="/assets/images/s81-6c3e31748ae39c517e4e9e8fca2c2ece.png" width="1128" height="800" class="img_ev3q"></p>
<ul>
<li>D&#x27;où viennent les plongements de position?<!-- -->
<ul>
<li>Dans la version originale, on utilise une combinaison de fonctions sinusoïdales</li>
</ul>
</li>
<li>En pratique: on apprend ces vecteurs à l&#x27;entraînement</li>
<li>Certains modèles n&#x27;utilisent pas la position absolue des jetons<!-- -->
<ul>
<li>Mais plutôt la position relative entre les mots.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="attention-multi-têtes">Attention multi-têtes<a href="#attention-multi-têtes" class="hash-link" aria-label="Direct link to Attention multi-têtes" title="Direct link to Attention multi-têtes">​</a></h3>
<ul>
<li>Les différents mots d&#x27;une phrase peuvent être reliés entre eux de différentes façons.</li>
<li>Difficile pour un seul bloc <em>transformer</em> de capturer ces différents types de relations.</li>
<li><strong><em>Multihead self-attention layers</em></strong>
<ul>
<li>Différentes couches d&#x27;attention ayant leurs propres paramètres <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mtext mathvariant="bold">W</mtext><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><msubsup><mtext mathvariant="bold">W</mtext><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><msubsup><mtext mathvariant="bold">W</mtext><mi>i</mi><mi>V</mi></msubsup></mrow><annotation encoding="application/x-tex">\textbf{W}_i^K, \textbf{W}_i^Q, \textbf{W}_i^V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em"></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9173em"><span style="top:-2.453em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.139em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em"><span style="top:-2.4231em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9173em"><span style="top:-2.453em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.139em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span></span></span></span></li>
<li>Elles projettent les vecteurs dans différentes directions.</li>
</ul>
</li>
<li>On concatène ensemble les vecteurs produits par ces différentes couches.</li>
</ul>
<p><img loading="lazy" alt="s82" src="/assets/images/s82-f39f70ae14e61615b9d8413f151c353f.png" width="1600" height="1014" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="transformers---les-autres-composantes">Transformers - Les autres composantes<a href="#transformers---les-autres-composantes" class="hash-link" aria-label="Direct link to Transformers - Les autres composantes" title="Direct link to Transformers - Les autres composantes">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mise-à-léchelle-du-score">Mise à l&#x27;échelle du score<a href="#mise-à-léchelle-du-score" class="hash-link" aria-label="Direct link to Mise à l&#x27;échelle du score" title="Direct link to Mise à l&#x27;échelle du score">​</a></h3>
<ul>
<li>Le calcul de score est un produit scalaire<!-- -->
<ul>
<li>On peut obtenir de grandes valeurs (positives ou négatives)</li>
<li>Problème de stabilité avec le softmax</li>
</ul>
</li>
</ul>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>score</mtext><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">x</mtext><mi>i</mi></msub><mo separator="true">,</mo><msub><mtext mathvariant="bold">x</mtext><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mtext mathvariant="bold">q</mtext><mi>i</mi></msub><mo>⋅</mo><msub><mtext mathvariant="bold">k</mtext><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\text{score}(\textbf{x}_i, \textbf{x}_j) = \textbf{q}_i \cdot \textbf{k}_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord text"><span class="mord">score</span></span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord textbf">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord text"><span class="mord textbf">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6886em;vertical-align:-0.2441em"></span><span class="mord"><span class="mord text"><span class="mord textbf">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2175em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord text"><span class="mord textbf">k</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span>
<ul>
<li>Le problème est encore plus sévère si la taille des vecteurs est plus longue (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span>)</li>
<li>Une mise à l&#x27;échelle du produit scalaire permet d&#x27;éviter cela<!-- -->
<ul>
<li>On normalise en fonction de la longueur des vecteurs</li>
</ul>
</li>
</ul>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>score</mtext><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">x</mtext><mi>i</mi></msub><mo separator="true">,</mo><msub><mtext mathvariant="bold">x</mtext><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msub><mtext mathvariant="bold">q</mtext><mi>i</mi></msub><mo>⋅</mo><msub><mtext mathvariant="bold">k</mtext><mi>j</mi></msub></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\text{score}(\textbf{x}_i, \textbf{x}_j) = \frac{\textbf{q}_i \cdot \textbf{k}_j}{\sqrt{d_k}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord text"><span class="mord">score</span></span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord textbf">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord text"><span class="mord textbf">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.3014em;vertical-align:-0.93em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em"><span style="top:-2.2528em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord text"><span class="mord textbf">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2175em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord text"><span class="mord textbf">k</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="connexions-résiduelles">Connexions résiduelles<a href="#connexions-résiduelles" class="hash-link" aria-label="Direct link to Connexions résiduelles" title="Direct link to Connexions résiduelles">​</a></h3>
<ul>
<li>Les transformers sont difficiles à entraîner <em>from scratch</em></li>
<li>Les concepteurs ont ajouté quelques mesures pour faciliter l&#x27;entraînement des modèles</li>
<li><strong>Connexion résiduelle</strong>
<ul>
<li>passer de l&#x27;information d&#x27;une couche inférieure à une couche supérieure</li>
<li>sans traverser une couche intermédiaire</li>
</ul>
</li>
<li>Avantage :<!-- -->
<ul>
<li>Permettre au gradient de passer une couche facilite l&#x27;apprentissage</li>
<li>Ça donne également aux couches supérieures un accès direct aux informations</li>
</ul>
</li>
<li>Ci-dessous, illustration d&#x27;une connexion résiduelle dans un bloc de transformer</li>
</ul>
<p><img loading="lazy" alt="s83" src="/assets/images/s83-d18a6a7e375c8725a1c23db8ebfb5960.png" width="1796" height="1000" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="normalisation-de-couche">Normalisation de couche<a href="#normalisation-de-couche" class="hash-link" aria-label="Direct link to Normalisation de couche" title="Direct link to Normalisation de couche">​</a></h3>
<ul>
<li>Une autre mesure pour améliorer la performance à l&#x27;entraînement des modèles<!-- -->
<ul>
<li><em>Layer normalization</em></li>
</ul>
</li>
<li>Les réseaux de neurones sont sensibles aux très grandes/petites valeurs</li>
<li>Solution : On normalise toutes les valeurs à la sortie d&#x27;une couche<!-- -->
<ul>
<li>Normalisation <em>z-score</em> où <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span> est la moyenne des valeurs en sortie de la couche et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span></span> est l&#x27;écart-type de ces valeurs.</li>
</ul>
</li>
</ul>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Normalisation z-score </mtext><mover accent="true"><mtext mathvariant="bold">x</mtext><mo>^</mo></mover><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mtext mathvariant="bold">x</mtext><mo>−</mo><mi mathvariant="bold">μ</mi><mo stretchy="false">)</mo></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex">\text{Normalisation z-score } \hat{\textbf{x}} = \frac{(\textbf{x}-\mathbf{\mu})}{\sigma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7079em"></span><span class="mord text"><span class="mord">Normalisation z-score </span></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7079em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord text"><span class="mord textbf">x</span></span></span><span style="top:-3.0134em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.113em;vertical-align:-0.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mopen">(</span><span class="mord text"><span class="mord textbf">x</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">μ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<ul>
<li>On utilise souvent une version paramétrée</li>
</ul>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo>=</mo><mi>γ</mi><mover accent="true"><mtext mathvariant="bold">x</mtext><mo>^</mo></mover><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">LayerNorm = \gamma \hat{\textbf{x}} + \beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em">yer</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal" style="margin-right:0.02778em">or</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.9023em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7079em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord text"><span class="mord textbf">x</span></span></span><span style="top:-3.0134em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bloc-de-transformer">Bloc de transformer<a href="#bloc-de-transformer" class="hash-link" aria-label="Direct link to Bloc de transformer" title="Direct link to Bloc de transformer">​</a></h3>
<p>Si on met toutes les composantes ensemble:</p>
<p><img loading="lazy" alt="s84" src="/assets/images/s84-274a53cad1f53daddabcfe2052b4228c.png" width="1412" height="954" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="lacunes-des-transformers">Lacunes des transformers<a href="#lacunes-des-transformers" class="hash-link" aria-label="Direct link to Lacunes des transformers" title="Direct link to Lacunes des transformers">​</a></h2>
<ul>
<li>Principalement deux lacunes<!-- -->
<ul>
<li>Le calcul d&#x27;attention est <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></li>
<li>La taille de la fenêtre<!-- -->
<ul>
<li>La taille habituelle varie entre ~500-1000 jetons</li>
<li>Comment prendre en compte de longs documents?</li>
</ul>
</li>
</ul>
</li>
<li>Des variantes ont été proposées<!-- -->
<ul>
<li>Par ex. <em>LongFormer</em>, <em>BigBird</em></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="exemple--modèle-de-langue">Exemple – Modèle de langue<a href="#exemple--modèle-de-langue" class="hash-link" aria-label="Direct link to Exemple – Modèle de langue" title="Direct link to Exemple – Modèle de langue">​</a></h2>
<p><img loading="lazy" alt="s85" src="/assets/images/s85-39c667b0c5332ec382ff68ca6abae657.png" width="1702" height="770" class="img_ev3q"></p>
<ul>
<li>Problème de causalité:<!-- -->
<ul>
<li>Pour entraîner un modèle de langue, il faut éviter de voir les prochains mots</li>
<li>Un transformer donne accès à tous les mots</li>
</ul>
</li>
<li>Solution : masquer ces mots</li>
</ul>
<p><img loading="lazy" alt="s86" src="/assets/images/s86-13a361cb132dad32f3cc69de92dc3b9e.png" width="1192" height="606" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<ul>
<li>Les <em>transformers</em> sont des réseaux :<!-- -->
<ul>
<li>Non récurrents</li>
<li>qui s&#x27;appuient sur un mécanisme d&#x27;auto-attention (<em>self-attention</em>)</li>
</ul>
</li>
<li>Ils permettent de &quot;transformer&quot; des mots en tenant compte des autres mots de leur contexte qui sont pertinents</li>
<li>Un bloc de <em>transformer</em> contient une couche d&#x27;attention, une couche FFNN avec des connexions résiduelles.</li>
<li>Une normalisation de couche (<em>layer normalization</em>) facilite l&#x27;entraînement du réseau</li>
<li>Un <em>transformer</em> contient plusieurs blocs</li>
<li>On verra dans les prochaines sections quelques architectures et différentes applications</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/university/ift-7022/week-09.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/courses/university/ift-7022/week-08-partie-2"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">8 Notions de base pour les RNN, GRU et LSTM</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/courses/university/ift-7022/week-10"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">10 Deep NLP - Plongements contextuels et modèles encodeurs pré-entraînés</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#plan-de-la-présentation" class="table-of-contents__link toc-highlight">Plan de la présentation</a></li><li><a href="#lacunes-des-réseaux-récurrents" class="table-of-contents__link toc-highlight">Lacunes des réseaux récurrents</a></li><li><a href="#transformers--idées-principales" class="table-of-contents__link toc-highlight">Transformers – Idées principales</a></li><li><a href="#transformer---la-base" class="table-of-contents__link toc-highlight">Transformer - La base</a><ul><li><a href="#transformer---self-attention" class="table-of-contents__link toc-highlight">Transformer - Self-attention</a></li><li><a href="#transformer---bloc" class="table-of-contents__link toc-highlight">Transformer - Bloc</a></li><li><a href="#la-position-des-motsjetons" class="table-of-contents__link toc-highlight">La position des mots/jetons</a></li><li><a href="#attention-multi-têtes" class="table-of-contents__link toc-highlight">Attention multi-têtes</a></li></ul></li><li><a href="#transformers---les-autres-composantes" class="table-of-contents__link toc-highlight">Transformers - Les autres composantes</a><ul><li><a href="#mise-à-léchelle-du-score" class="table-of-contents__link toc-highlight">Mise à l&#39;échelle du score</a></li><li><a href="#connexions-résiduelles" class="table-of-contents__link toc-highlight">Connexions résiduelles</a></li><li><a href="#normalisation-de-couche" class="table-of-contents__link toc-highlight">Normalisation de couche</a></li><li><a href="#bloc-de-transformer" class="table-of-contents__link toc-highlight">Bloc de transformer</a></li></ul></li><li><a href="#lacunes-des-transformers" class="table-of-contents__link toc-highlight">Lacunes des transformers</a></li><li><a href="#exemple--modèle-de-langue" class="table-of-contents__link toc-highlight">Exemple – Modèle de langue</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Alain Boisvert. Construit avec Docusaurus.</div></div></div></footer></div>
</body>
</html>