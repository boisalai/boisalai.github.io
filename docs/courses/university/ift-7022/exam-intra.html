<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-courses/university/ift-7022/exam-intra" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">Examen de mi-session | Alain Boisvert</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://boisalai.github.io/docs/courses/university/ift-7022/exam-intra"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Examen de mi-session | Alain Boisvert"><meta data-rh="true" name="description" content="- Quand? le 17 oct. 2023 de 18h30 à 21h45"><meta data-rh="true" property="og:description" content="- Quand? le 17 oct. 2023 de 18h30 à 21h45"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://boisalai.github.io/docs/courses/university/ift-7022/exam-intra"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/courses/university/ift-7022/exam-intra" hreflang="en"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/courses/university/ift-7022/exam-intra" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Alain Boisvert RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Alain Boisvert Atom Feed">



<link rel="stylesheet" href="/fonts/font-awesome/fontawesome.css">
<link rel="stylesheet" href="/fonts/font-awesome/solid.css">
<link rel="stylesheet" href="/fonts/font-awesome/regular.css">
<link rel="stylesheet" href="/fonts/font-awesome/brands.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.6520ee1d.css">
<script src="/assets/js/runtime~main.089b18cf.js" defer="defer"></script>
<script src="/assets/js/main.61e0bf1e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Alain Boisvert</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" target="" href="/docs/courses/university/ift-7022">IFT-7022</a><a class="navbar__item navbar__link" target="" href="/docs/courses/university/gif-7005">GIF-7005</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/boisalai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-github"> </a><a href="https://www.linkedin.com/in/alain-boisvert-98b058156/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-linkedin-in"> </a><a href="mailto:ay.boisvert@gmail.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-solid fa-envelope"> </a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Alain Boisvert</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/cv">Curriculum vitæ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/learning">Learning path</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/certificates">Certificates</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/courses/university/gif-7005">Courses</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/courses/university/gif-7005">Université Laval</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/courses/university/gif-7005">GIF-7005 Introduction à l&#x27;apprentissage automatique</a><button aria-label="Expand sidebar category &#x27;GIF-7005 Introduction à l&#x27;apprentissage automatique&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/docs/courses/university/ift-7022">IFT-7022 Traitement automatique de la langue naturelle</a><button aria-label="Collapse sidebar category &#x27;IFT-7022 Traitement automatique de la langue naturelle&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-01">1 Expressions régulières</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-02">2 Prétraitement de textes et distance minimale d&#x27;édition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-03">3 Modèles de langue N-grammes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-04">4 Classification de textes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-05">5 Sémantique vectorielle (représentation des mots)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-06">6 Deep NLP + Plongements de mots (word embeddings) + intro aux réseaux de neurones</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-07">7 Étiquetage de séquences - analyse grammaticale (Part of speech tagging) et reconnaissance d&#x27;entités nommées</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-08-partie-1">8 Deep NLP - Réseaux récurrents pour le traitement de séquences (RNN, GRU, LSTM)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-08-partie-2">8 Notions de base pour les RNN, GRU et LSTM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-09">9 Deep NLP - Introduction aux modèles Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-10">10 Deep NLP - Plongements contextuels et modèles encodeurs pré-entraînés</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-11">11 Deep NLP - Traduction automatique et modèle encodeur-décodeur</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-12">12 Deep NLP - Systèmes question-réponse (QA)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-13">13 Deep NLP - Introduction aux LLMs, prompting et instruct tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-1">Travail pratique 1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-2">Travail pratique 2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-3">Travail pratique 3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/courses/university/ift-7022/exam-intra">Examen de mi-session</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/exam-final">Examen final</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/gif-7105">GIF-7105 Photographie algorithmique</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/courses/deeplearning-ai/p01-chatgpt-building-systems">DeepLearning.AI</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/courses/activeloop/rag-for-production">Activeloop</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/fine-tuning-llms">Training and fine-tuning LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/python-gpu">Best practices CPU and GPU</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/models/test">Interactive models</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/references/links">References</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Courses</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Université Laval</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/courses/university/ift-7022"><span itemprop="name">IFT-7022 Traitement automatique de la langue naturelle</span></a><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Examen de mi-session</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Examen de mi-session</h1>
<ul>
<li>Quand? le 17 oct. 2023 de 18h30 à 21h45</li>
<li>Durée : 3 h 10 min</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="directives">Directives<a href="#directives" class="hash-link" aria-label="Direct link to Directives" title="Direct link to Directives">​</a></h2>
<p>Cet examen évaluera votre compréhension de la matière des 6 premières semaines du cours.
L&#x27;examen sera effectué à l&#x27;aide d&#x27;un questionnaire sur MonPortail durant la plage horaire du cours
(mardi soir de 18h30 à 21h20). Donc la présence des étudiants sur le campus universitaire n&#x27;est pas requise.</p>
<p>Toute documentation est permise pour l&#x27;examen (par ex. livre du cours, notes de cours, capsules vidéo).
Cependant veuillez noter que le temps est limité et qu&#x27;il faut un bon niveau de compréhension de la matière
pour compléter l&#x27;examen durant la période allouée. Veuillez également noter que les retours arrière sur
les sections de l&#x27;examen sont permis et que la composition des examens est aléatoire (N questions parmi
les M que nous avons préparées).</p>
<p>Le contenu de l&#x27;examen porte sur les thèmes suivants (à confirmer après le cours du 10 octobre) :</p>
<ul>
<li>Expressions régulières (ch. 2)<!-- -->
<ul>
<li>regular expressions 101</li>
<li>Untitled Pattern</li>
<li>Regular Expressions Cheat Sheet</li>
</ul>
</li>
<li>Tokénisation de mots et segmentation de phrases (ch. 2)</li>
<li>Normalisation de mots - stemming, lemmatisation, analyse morphologique (ch. 2)</li>
<li>Modèles de langue N-grammes (ch.3)<!-- -->
<ul>
<li>Construction de modèles, estimation des probabilités, lissage, perplexité, évaluation</li>
</ul>
</li>
<li>Classification de textes (ch. 4-5)<!-- -->
<ul>
<li>Naive bayes, représentation de texte (BOW), type d’attributs</li>
<li>Régression logistique, apprentissage par descente de gradient (ajustement des poids), régression multinomiale</li>
<li>Évaluation, métriques, validation croisée</li>
</ul>
</li>
<li>Réseaux de neurones (ch.7)<!-- -->
<ul>
<li>Feedforward (MLP), fonctions d’activation, représentation intermédiaire</li>
<li>Propagation avant, époque et minibatch</li>
<li>Modèle de langue neuronal et classification de texte</li>
</ul>
</li>
<li>Sémantique lexicale et plongements de mots (ch.6)<!-- -->
<ul>
<li>Cooccurrences, Similarité distributionnelle, word embeddings, vecteur dense, Word2Vec, analogies de mots, Similarité sémantique entre les mots</li>
<li>Pointwise Mutual Information ou information mutuelle marginale</li>
</ul>
</li>
<li>Étiquetage de séquences et analyse grammaticale (ch. 8)<!-- -->
<ul>
<li>Classe de mots, POS tags, étiquetage</li>
<li>HMM, Viterbi, CRF-MEMM, choix d&#x27;attributs</li>
<li>Entités nommées, étiquetage BIO vs. IO, évaluation</li>
</ul>
</li>
<li>Réseaux de neurones récurrents (ch.9) - <strong>NE SERA PAS ÉVALUÉ À L&#x27;EXAMEN DE MI-SESSION</strong></li>
<li>Les notebooks - compréhension des résultats présentés<!-- -->
<ul>
<li>Week 1: Conversion de questions à la forme affirmative (Regex)</li>
<li>Week 2:<!-- -->
<ul>
<li>Exemples avec NLTK pour l&#x27;analyse de textes</li>
<li>Exemples avec Spacy pour l&#x27;analyse de textes</li>
</ul>
</li>
<li>Week 3: Modèles N-grammes de mots avec NLTK</li>
<li>Week 4: Classification de textes avec Scikit-learn</li>
<li>Week 5: Analyse de cooccurrences et de collocations</li>
<li>Week 6:<!-- -->
<ul>
<li>Réseaux de neurones #1 : Classification avec un réseau à 1 seule couche (bow_one_layer_nn.ipynb)</li>
<li>Réseaux de neurones #2 : Classification de documents avec un réseau feedforward multicouches (bow_nlp.ipynb)</li>
<li>Réseaux de neurones #3 : Classification de documents avec un réseau multicouches et des embeddings (mlp_embeddings.ipynb)</li>
<li>Gensim et les plongements de mots (word embeddings) préentraînés (word_embeddings.ipynb)</li>
<li>Plongements de mots préentraînés fastText (fasttext_embeddings.ipynb)</li>
</ul>
</li>
<li>Week 7: Analyse grammaticale avec NLTK et Spacy (pos_tagging.ipynb)</li>
</ul>
</li>
<li>Le premier travail</li>
<li>Les exemples de calculs numériques.<!-- -->
<ul>
<li>probabilités n-grammes</li>
<li>classification avec un modèle naïf bayésien</li>
<li>descente du gradien et ajustement de poids</li>
<li>Cooccurrences</li>
<li>similarité textuelle</li>
<li>Viterbi et <a href="https://docs.google.com/spreadsheets/d/16IFFca4_kpr7gYX58fWA8TbQMUl9LDWfErB9keU7F2c/edit#gid=0" target="_blank" rel="noopener noreferrer">Google Sheet</a></li>
</ul>
</li>
<li>À vérifier<!-- -->
<ul>
<li>Similarité distributionnelle n&#x27;est plus étudiée dans le cours pourtant il est dans la liste des sujet à l&#x27;examen.</li>
</ul>
</li>
<li>Quatre types de question<!-- -->
<ul>
<li>Vrai ou faux et choix multiples</li>
<li>Questions à développement (8-10)<!-- -->
<ul>
<li>Habituellement court, max 1-2 phrases.</li>
<li>Rarement un paragraphe.</li>
</ul>
</li>
<li>Mise en situation<!-- -->
<ul>
<li>Quelle technique devriez-vous utiliser pour...</li>
</ul>
</li>
<li>Résolution de problème (2-3)<!-- -->
<ul>
<li>Des calculs<!-- -->
<ul>
<li>Probabilités N-grammes</li>
<li>Naives Bayes</li>
<li>Régression logistique</li>
<li>Viterbi</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="questions">Questions<a href="#questions" class="hash-link" aria-label="Direct link to Questions" title="Direct link to Questions">​</a></h2>
<p>Peux-tu répondre à ces questions concernant NLP? Pour chaque question, tu dois répondre par Vrai ou Faux.</p>
<p><strong>Question 1</strong> : Pour certains algorithmes d&#x27;analyse grammaticale et de reconnaissance d&#x27;entités nommées, la forme d&#x27;un mot (<em>word shape</em>) est un
attribut très utile  pour étiqueter des mots appartenant à des classes fermées.</p>
<ul>
<li><strong>Faux</strong> : C&#x27;est utile pour étiqueter les mots de classes ouvertes (noms, adjectifs, adverbes, verbes). Ça n&#x27;apporte rien pour
les mots de classes fermées (prépositions, conjonctions, déterminants, etc.).</li>
</ul>
<p><strong>Question 2</strong> :
Pour l&#x27;analyse grammaticale, il est possible d&#x27;obtenir un étiquetage de type Universal dependencies autant avec NTLK que Spacy.</p>
<ul>
<li><strong>Faux</strong> : Cela est présenté dans un le notebook sur l&#x27;analyse grammaticale. NLTK --&gt; Penn treebank seulement.</li>
</ul>
<p><strong>Question 3</strong> :
Pour entraîner un analyseur grammatical de type HMM, on a besoin d&#x27;un corpus qui contient les états cachés qui correspondent aux  étiquettes de mots (les parts of speech). <strong>Vrai</strong></p>
<p><strong>Question 4</strong> :
Le plongement Word2Vec d&#x27;un mot est un vecteur numérique qui est une représentation dense des contextes dans lesquels on retrouve ce mot.  <strong>Vrai</strong></p>
<p><strong>Question 5</strong> :
La fonction sigmoïde utilisée dans la régression logistique pour la classification binaire offre l&#x27;avantage d&#x27;être dérivable, ce qui permet de calculer le gradient de la fonction de perte (entropie croisée) et de mettre à jour les paramètres wi et b. <strong>Vrai</strong></p>
<p><strong>Question 6</strong> :
Une représentation par sac de mots ne permet pas de préserver l’ordre des jetons d’un texte. <strong>Vrai</strong></p>
<p><strong>Question 7</strong> :
L’analyse grammaticale est une tâche facile à résoudre parce seulement 15% des mots sont ambigus.</p>
<ul>
<li><strong>Faux</strong> - L&#x27;analyse grammaticale est complexe en raison de la nature ambiguë et flexible de la langue humaine. Plus de mots peuvent être ambigus, et l&#x27;ambiguïté n&#x27;est pas le seul défi.
La plupart des occurences de mot correspondent au POS tag le plus fréquent de ce mot.</li>
</ul>
<p><strong>Question 8</strong> :
Dans les modèles de langue N-grammes, si on estime la probabilité du mot inconnu &quot;&lt;UNK&gt;&quot;, il n&#x27;est pas nécessaire de faire de lissage.</p>
<ul>
<li><strong>Faux</strong> - Le lissage est souvent nécessaire dans les modèles N-grammes pour gérer les mots inconnus et éviter les probabilités nulles, même lorsque le mot &quot;&lt;UNK&gt;&quot; est utilisé.
Même si on donne  une probabilité à un mot rare,  on ne l&#x27;a probablement pas vu dans certains bigrammes ou trigrammes. Et sans lissage, la probabilité du bigramme ou du trigramme  serait nulle.</li>
</ul>
<p><strong>Question 9</strong> :
La fonction Softmax permet de convertir des valeurs numériques (positives ou négatives) en des valeurs de probabilité entre 0 et 1.  <strong>Vrai</strong></p>
<p><strong>Question 10</strong> :
Un vecteur de contexte distributionnel permet de représenter un mot cible à l&#x27;aide de ses cooccurrences. De plus, ce vecteur nous aide à trouver d&#x27;autres mots qui peuvent être utilisés dans des contextes similaires à ceux du mot cible.</p>
<ul>
<li><strong>Vrai</strong> : Presque une définition complète de ce que sont les vecteurs de contexte distributionnels.</li>
</ul>
<p><strong>Question 11</strong> :
Un CountVectorizer permet de créer des attributs qui correspondent à des N-grammes de caractères. Il permet également de générer des attributs à partir des N-grammes de mots.</p>
<ul>
<li><strong>Vrai</strong> : Cette fonctionnalité est présentée dans un notebook sur le site du cours. C&#x27;est également le type d&#x27;attributs qu&#x27;on utilise pour l&#x27;identification de langue d&#x27;un document.</li>
</ul>
<p><strong>Question 12</strong> :
Si on construit un modèle de langue avec un réseau de neurones, on n&#x27;a pas besoin de faire de lissage parce que le pouvoir prédictif du modèle neuronal est plus élevé que celui d&#x27;un modèle N-gramme.</p>
<ul>
<li><strong>Vrai</strong></li>
</ul>
<p><strong>Question 13</strong> :
Le résultat de la lemmatisation de intelligence artificielle  est intellig artifici.</p>
<ul>
<li><strong>Faux</strong> - La lemmatisation de &quot;intelligence artificielle&quot; serait plus probablement &quot;intelligence artificiel&quot;, selon le contexte et les règles de lemmatisation.
C&#x27;est du stemming, pas de la lemmatisation.</li>
</ul>
<p><strong>Question 14</strong> :
Si on fait la lemmatisation de &quot;être ou ne pas être&quot; , on obtient la même chose. <strong>Vrai</strong></p>
<p><strong>Question 15</strong> :
L&#x27;expression régulière suivante est bonne et efficace pour extraire des déterminants dans un texte français.
regex_pattern = &quot;[le|la|les|l&#x27;|un|une|des|du|au|aux] &quot;</p>
<ul>
<li><strong>Faux</strong> - L&#x27;expression régulière correcte serait plutôt quelque chose comme &quot;le|la|les|l&#x27;|un|une|des|du|au|aux\b&quot;. L&#x27;original ne capture pas les déterminants de manière efficace en raison de l&#x27;utilisation incorrecte des crochets et de l&#x27;absence de délimitation des mots. C&#x27;est une énumération de mots, pas une expression régulière.</li>
</ul>
<p><strong>Question 16</strong> :
Un one-hot vector est un vecteur ayant une valeur de 1 pour toutes les cellules correspondant aux mots d&#x27;un texte.</p>
<ul>
<li><strong>Faux</strong> - Un &quot;one-hot vector&quot; est un vecteur où une seule position est marquée avec 1 et toutes les autres positions sont 0.
one-hot vector --&gt; représentation pour 1 seul mot (representation of a single word)</li>
</ul>
<p><strong>Question 17</strong> :
La normalisation permet de réduire le nombre de types d’un corpus et la taille du vocabulaire d’une application. <strong>Vrai</strong></p>
<p><strong>Question 18</strong> :
On peut toujours trouver le lexème d’un mot dans un dictionnaire.</p>
<ul>
<li><strong>Faux</strong> - Lexeme = stemming = no dictionary.</li>
</ul>
<p><strong>Question 19</strong> :
La forme de surface d&#x27;un mot peut être dérivée ou infléchie. <strong>Vrai</strong></p>
<p><strong>Question 20</strong> :
Les synonymes sont souvent des cooccurrences. Êtes-vous d’accord ?</p>
<ul>
<li><strong>Faux</strong> - Les synonymes ne sont pas nécessairement des cooccurrences. Les synonymes sont des mots ayant des significations similaires, tandis que les cooccurrences sont typiquement des mots qui apparaissent fréquemment ensemble. Les synonymes n&#x27;ont pas tendance à se retrouver dans les mêmes phrases.</li>
</ul>
<p>Formulez une réponse courte, claire et concise à la question suivante. Maximum 4 phrases.</p>
<p><strong>Question 21</strong> : Dans quel cas devrait-on faire un encodage IOB des séquences d’entraînement au lieu d’un encodage IO?</p>
<ul>
<li>Lorsqu&#x27;on doit distinguer clairement les limites des entités adjacentes ou multi-mots dans les séquences d&#x27;entraînement.</li>
<li>Lorsque des entités nommées de même type se suivent dans des textes.</li>
</ul>
<p><strong>Question 22</strong> : Associer les plongements de mots préentraînés au type de gestion de mots inconnus, soit Word2Vec, Spacy ou FastText.</p>
<p><strong>Question 22-A</strong> : Je construis une représentation à partir des caractères du mot inconnu.</p>
<p>La méthode que vous décrivez, &quot;construire une représentation à partir des caractères du mot inconnu&quot;, correspond à la façon dont FastText gère les mots inconnus.</p>
<p>FastText se différencie par sa capacité à créer des vecteurs de mots pour des mots inconnus en prenant en compte la morphologie, c&#x27;est-à-dire en analysant les sous-mots ou n-grammes de caractères. Ainsi, même si le mot entier n&#x27;a jamais été vu durant l&#x27;entraînement, sa représentation peut être construite à partir de ses sous-composants, ce qui est particulièrement utile pour les mots avec des fautes de frappe, des conjugaisons ou des formes fléchies qui n&#x27;ont pas été observées lors de l&#x27;entraînement.</p>
<p><strong>Question 22-B</strong> : Je ne construis pas de représentation pour les mots inconnus.</p>
<p>La méthode &quot;je ne construis pas de représentation pour les mots inconnus&quot; est associée à &quot;Word2Vec.&quot;</p>
<p>Word2Vec, l&#x27;un des modèles de plongement lexical les plus connus, ne génère des vecteurs que pour les mots qu&#x27;il a vus pendant la phase d&#x27;entraînement. Si un mot n&#x27;a pas été vu lors de l&#x27;entraînement, Word2Vec n&#x27;est pas en mesure de construire un vecteur pour ce mot à partir de zéro et, dans de tels cas, souvent un vecteur spécial pour les mots inconnus (comme un vecteur de zéros ou un vecteur spécial &quot;&lt;UNK&gt;&quot;) doit être utilisé. Cela contraste avec des approches comme FastText qui peuvent générer des vecteurs pour des mots inconnus basés sur leurs sous-unités.</p>
<p><strong>Question 22-C</strong> : Je construis un vecteur rempli de zéros pour les mots inconnus.</p>
<p>La méthode &quot;je construis un vecteur rempli de zéros pour les mots inconnus&quot; peut être associée à &quot;Spacy.&quot;</p>
<p>Spacy, une bibliothèque populaire pour le traitement du langage naturel, utilise des vecteurs préentraînés pour la représentation des mots. Lorsqu&#x27;il rencontre un mot inconnu (c&#x27;est-à-dire un mot qui n&#x27;était pas présent dans le vocabulaire lors de l&#x27;entraînement des vecteurs de mots), Spacy attribue un vecteur de zéros à ce mot. C&#x27;est une différence importante par rapport à des modèles comme FastText, qui génèrent des vecteurs pour des mots inconnus en utilisant des n-grammes de caractères, permettant une certaine représentation même pour les mots non vus lors de l&#x27;entraînement.</p>
<p><strong>Question 23</strong> : Dans Word2Vec, maximiser la probabilité équivaut à maximiser la similarité des vecteurs de mot et de contexte. Quelle est la principale partie de l&#x27;architecture Word2Vec qui mène à ce résultat .</p>
<p>Dans Word2Vec, c&#x27;est la couche de projection cachée qui, en optimisant les poids pendant l&#x27;entraînement, permet de maximiser la similarité des vecteurs de mots et de contexte.</p>
<p><strong>Question 24</strong> : Dans le travail pratique #1, vous pouviez utiliser soit la perplexité ou le logprob pour choisir l&#x27;ordre des mots complétant un proverbe. Est-ce que les deux mesures donnent le même choix de proverbe ? Pourquoi ?.</p>
<ul>
<li>Ils donnent le même résultat. La perplexité est normalisée en fonction du nombre de mot (N). Les résultats pourraient être différents seulement si les différentes versions de proverbes n&#x27;avaient pas toutes la même longueur (c.-à-d. si les proverbes complétés n&#x27;avaient pas tous le même nombre de mots). Ce n&#x27;est pas le cas - tous les proverbes candidats ont la même longueur.</li>
</ul>
<p><strong>Question 25</strong> : Pour le POS tagging, pourquoi est-ce que les modèles CRF (MEMM) donnent souvent de meilleurs résultats que les modèles HMM ?</p>
<ul>
<li>La capacité des CRF (MEMM) à utiliser des caractéristiques complexes (ex. attributs comme les préfixes, suffixes, majuscules), et à considérer l&#x27;ensemble de la séquence d&#x27;entrée les rend généralement plus performants que les HMM</li>
<li>Ils permettent de prendre en compte plus d&#x27;attributs qui décrivent le contexte et la morphologie des mots (par ex. préfixe, suffixe, word shape).</li>
</ul>
<p><strong>Question 26</strong> : Si nous avons le choix entre deux corpus de contenus différents pour construire un modèle (par ex. un modèle N-grammes ou Naive Bayes), devrait-on toujours choisir le corpus le plus gros? Pourquoi ?</p>
<ul>
<li>Non, un corpus plus grand n&#x27;est pas toujours meilleur. La qualité, la pertinence, et la représentativité des données sont importantes. Un grand corpus non pertinent ou biaisé peut détériorer la performance du modèle.</li>
<li>Si le contenu du petit corpus est plus pertinent à la tâche à accomplir (par ex. des critiques de films pour construire un analyseur de sentiment), on devrait alors le choisir. Sinon on devrait choisir le plus gros.</li>
</ul>
<p><strong>Question 27</strong> : Dans Word2vec, à quoi correspond la taille des plongements de mots ?  Et à quoi correspondent les valeurs du plongement d&#x27;un mot ?</p>
<ul>
<li>Taille :  Nombre de neurones dans la couche cachée.</li>
<li>Valeurs : Les poids reliant un mot cible de la couche d&#x27;entrée à la couche cachée.</li>
</ul>
<p><strong>Question 28</strong> :</p>
<p>Lesquelles des 3 paires de mots suivantes sont des cooccurrences ?</p>
<table><thead><tr><th>μ</th><th>σ</th><th>Mot 1</th><th>Mot 2</th></tr></thead><tbody><tr><td>0.17</td><td>2.49</td><td>clustering</td><td>classification</td></tr><tr><td>2.03</td><td> 1.25</td><td>nations</td><td>Canada</td></tr><tr><td>0.97</td><td>0.37</td><td>Nova</td><td>Scotia</td></tr></tbody></table>
<p>Une cooccurrence dans le contexte du traitement du langage naturel se réfère généralement à des mots qui apparaissent fréquemment ensemble dans un texte. Les chiffres μ (moyenne) et σ (écart-type) semblent indiquer la proximité statistique des mots dans un corpus, probablement leur fréquence de cooccurrence et la consistance de cette cooccurrence.</p>
<ul>
<li><strong>clustering | classification</strong>: μ est faible, mais σ est élevé, indique une faible cooccurrence. Les mots sont thématiquement liés mais ne cooccurrent pas nécessairement directement.</li>
<li><strong>nations | Canada</strong>: Un μ élevé et un σ moyen suggèrent que ces termes apparaissent probablement souvent ensemble, avec une variabilité modérée. Ils sont des cooccurrences, car &quot;Canada&quot; est souvent discuté dans le contexte des &quot;nations&quot;.</li>
<li><strong>Nova | Scotia</strong>: Avec un μ modéré et un σ faible, ces mots cooccurrent presque toujours (&quot;Nova Scotia&quot; est une province canadienne), indiquant une cooccurrence forte et consistante.</li>
</ul>
<p><strong>Question 29</strong> : Peux-tu compléter cette phrase?
Si on utilisait un modèle HMM pour faire de la lemmatisation, les observations seraient _________ et les états cachés seraient __________.</p>
<ul>
<li>Observations = Les mots tels qu&#x27;ils apparaissent dans le texte</li>
<li>États cachés = Les lemmes, soit la forme de base des mots</li>
</ul>
<p><strong>Question 30</strong> : Pour la tâche 1 du travail pratique #1, avez-vous utilisé 1 seule expression régulière ou plusieurs? Pourquoi? Soyez spécifique.</p>
<ul>
<li>On s&#x27;attend pour ce problème qu&#x27;on utilise plusieurs regex car il est difficile d&#x27;en construire une seule qui prend en compte tous les cas possibles et toutes les informations à extraire.</li>
</ul>
<p><strong>Question 31</strong> : Apprentissage des poids d&#x27;un modèle de régression logistique</p>
<p>On retrouve dans le chapitre de Jurafsky et Martin sur la régression logistique un exemple de descente de gradient.  À la fin de cet exemple, les poids du modèle sont [.15, .1, .05]. - An example of gradient descent can be found in Jurafsky and Martin&#x27;s chapter on logistic regression. At the end of this example, the model weights are [.15, .1, .05].</p>
<p>En utilisant l&#x27;observation suivante - Using the following observation :</p>
<ul>
<li>x1 = Compte de mots positifs = Count of positive lexicon words = 1</li>
<li>x2 = Compte de mots négatifs = Count of negative lexicon words = 3</li>
<li>y = 0  (critique négative - negative review)</li>
</ul>
<p>La 2e étape de descente de gradient est calculée ainsi.</p>
<p>Soit w = [w1, w2] = [.15, .1], b=.05, x = [x1, x2] = [1, 3] et y = 0. On doit calculer ∇L qui correspond à une matrice 3x1 où les éléments sont respectivement</p>
<p>σ(wx+b)-y)x1 = σ((.15×1+.1×3+.05)-0) × 1 = 0.6225</p>
<p>σ(wx+b)-y)x2 = σ((.15×1+.1×3+.05)-0) × 3 = 1.8674</p>
<p>σ(wx+b)-y) = σ((.15×1+.1×3+.05)-0) = = 0.6225</p>
<p>Ça donne la matrice 3x1 [0.6225, 1.8674, 0.6225] transposée.</p>
<p>Ensuite, nous avons η = .1 et on calcul les nouveaux paramètres θ2 ainsi :</p>
<p>w1 = w1 - η × 0.6225 = .15 - 0.1 × 0.6225 = 0.0878</p>
<p>w2 = w2 - η × 1.8674 = .1 - 0.1 × 1.8674 = -0.0867</p>
<p>b = b - η × 0.6225 = .05 - 0.1 × 0.6225 = -0.0122</p>
<p>Que pensez-vous des changements de poids ? Les poids changent beaucoup entre l&#x27;étape 1 et l&#x27;étape 2, cela suggère que nous sommes encore loin de la convergence. Faudrait faire encore plusieurs itérations avant de voir les poids se stabiliser.</p>
<ul>
<li>Le réponse suggérée est : Apprendre à partir d&#x27;un exemple négatif prédit comme positif diminue les poids.</li>
</ul>
<p><strong>Question 32</strong> : Modèles N-grammes</p>
<p>Supposons que l’on veut construire des modèles N-grammes à partir d’un corpus constitué des 3 phrases normalisées suivantes - Let&#x27;s suppose we want to build N-gram models from a corpus consisting of the following 3 normalized sentences:</p>
<ul>
<li>&lt;s&gt; le québec est une province du canada &lt;/s&gt;</li>
<li>&lt;s&gt; le québec a une population de huit millions de personnes &lt;/s&gt;</li>
<li>&lt;s&gt; le québec est un peu plus petit que le mexique &lt;/s&gt;</li>
</ul>
<p>Quelles sont les valeurs de probabilité suivantes - What are the following probability values?</p>
<ul>
<li>PMLE(petit) , PMLE(grand), PMLE(mexique | le), PMLE(a | le québec), PMLE(est | le mexique)</li>
<li>PLAPLACE(petit), PLAPLACE(grand), PLAPLACE (mexique | le) , PLAPLACE (a | le québec), PLAPLACE (est | le mexique)</li>
</ul>
<p>Réponse :</p>
<ul>
<li>N = 33, incluant les marqueurs &lt;s&gt; et &lt;/s&gt;</li>
<li>|V| = 20 mots différents, incluant les marqueurs  &lt;s&gt; et  &lt;/s&gt;</li>
<li>PMLE(petit) = 1/33</li>
<li>PMLE(grand) = 0/33</li>
<li>PMLE(mexique | le) = 1/4</li>
<li>PMLE(a | le québec) = 1/3</li>
<li>PMLE(est | le mexique) = 0/1</li>
<li>PLAPLACE(petit) = (1+1)/(33+20) = 2/53</li>
<li>PLAPLACE(grand) = (0+1)/(33+20) = 1/53</li>
<li>PLAPLACE (mexique | le) = (1+1)/(4+33) = 2/37</li>
<li>PLAPLACE (a | le québec) = (1+1)/(3+33) = 2/36</li>
<li>PLAPLACE (est | le mexique) = (0+1)/(1+33) = 1/34</li>
</ul>
<p>Commentaire de l&#x27;enseignant : |V| = 21. Erreur au dénominateur des PLaplace bigrammes et trigrammes - +33 au lieu de +21.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/university/ift-7022/exam-intra.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/courses/university/ift-7022/travail-3"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Travail pratique 3</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/courses/university/ift-7022/exam-final"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Examen final</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#directives" class="table-of-contents__link toc-highlight">Directives</a></li><li><a href="#questions" class="table-of-contents__link toc-highlight">Questions</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Alain Boisvert. Construit avec Docusaurus.</div></div></div></footer></div>
</body>
</html>