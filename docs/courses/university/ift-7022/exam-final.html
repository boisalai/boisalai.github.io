<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-courses/university/ift-7022/exam-final" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">Examen final du 12 décembre 2023 | Alain Boisvert</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://boisalai.github.io/docs/courses/university/ift-7022/exam-final"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Examen final du 12 décembre 2023 | Alain Boisvert"><meta data-rh="true" name="description" content="L&#x27;examen final, d&#x27;une durée de 3h00, portera sur la matière présentée après le premier examen (17 octobre 2023, week-08)."><meta data-rh="true" property="og:description" content="L&#x27;examen final, d&#x27;une durée de 3h00, portera sur la matière présentée après le premier examen (17 octobre 2023, week-08)."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://boisalai.github.io/docs/courses/university/ift-7022/exam-final"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/courses/university/ift-7022/exam-final" hreflang="en"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/courses/university/ift-7022/exam-final" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Alain Boisvert RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Alain Boisvert Atom Feed">



<link rel="stylesheet" href="/fonts/font-awesome/fontawesome.css">
<link rel="stylesheet" href="/fonts/font-awesome/solid.css">
<link rel="stylesheet" href="/fonts/font-awesome/regular.css">
<link rel="stylesheet" href="/fonts/font-awesome/brands.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.e0a8b603.css">
<script src="/assets/js/runtime~main.9f2d5a51.js" defer="defer"></script>
<script src="/assets/js/main.ae67d273.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Alain Boisvert</b></a><a class="navbar__item navbar__link" target="" href="/docs/courses/university/glo-7030">GLO-7030</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/boisalai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-github"> </a><a href="https://www.linkedin.com/in/alain-boisvert-98b058156/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-linkedin-in"> </a><a href="mailto:ay.boisvert@gmail.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-solid fa-envelope"> </a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Alain Boisvert</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/cv">Curriculum vitæ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/learning">Learning path</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/certificates">Certificates</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/courses/university/gif-7005">Courses</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/courses/university/gif-7005">Université Laval</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/courses/university/gif-7005">GIF-7005 Introduction à l&#x27;apprentissage automatique</a><button aria-label="Expand sidebar category &#x27;GIF-7005 Introduction à l&#x27;apprentissage automatique&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/docs/courses/university/ift-7022">IFT-7022 Traitement automatique de la langue naturelle</a><button aria-label="Collapse sidebar category &#x27;IFT-7022 Traitement automatique de la langue naturelle&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-01">1 Expressions régulières</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-02">2 Prétraitement de textes et distance minimale d&#x27;édition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-03">3 Modèles de langue N-grammes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-04">4 Classification de textes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-05">5 Sémantique vectorielle (représentation des mots)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-06">6 Deep NLP + Plongements de mots (word embeddings) + intro aux réseaux de neurones</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-07">7 Étiquetage de séquences - analyse grammaticale (Part of speech tagging) et reconnaissance d&#x27;entités nommées</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-08-partie-1">8 Deep NLP - Réseaux récurrents pour le traitement de séquences (RNN, GRU, LSTM)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-08-partie-2">8 Notions de base pour les RNN, GRU et LSTM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-09">9 Deep NLP - Introduction aux modèles Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-10">10 Deep NLP - Plongements contextuels et modèles encodeurs pré-entraînés</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-11">11 Deep NLP - Traduction automatique et modèle encodeur-décodeur</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-12">12 Deep NLP - Systèmes question-réponse (QA)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-13">13 Deep NLP - Introduction aux LLMs, prompting et instruct tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-1">Travail pratique 1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-2">Travail pratique 2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-3">Travail pratique 3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/exam-intra">Examen de mi-session</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item hidden"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/courses/university/ift-7022/exam-final">Examen final</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/courses/university/glo-7030">GLO-7030 Apprentissage par réseaux de neurones profonds</a><button aria-label="Expand sidebar category &#x27;GLO-7030 Apprentissage par réseaux de neurones profonds&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/courses/deeplearning-ai/sp01-chatgpt-building-systems">DeepLearning.AI</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/courses/activeloop/rag-for-production">Activeloop</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/fine-tuning-llms">Training and fine-tuning LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/python-gpu">Best practices CPU and GPU</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/models/revdisp">Interactive models</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/references/links">References</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Courses</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Université Laval</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/courses/university/ift-7022"><span itemprop="name">IFT-7022 Traitement automatique de la langue naturelle</span></a><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Examen final</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Examen final du 12 décembre 2023</h1>
<p>L&#x27;examen final, d&#x27;une durée de 3h00, portera sur la matière présentée après le premier examen (17 octobre 2023, week-08).
Les concepts, techniques et modèles à étudier en préparation de l&#x27;examen sont (version définitive) :</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="lectures">Lectures<a href="#lectures" class="hash-link" aria-label="Direct link to Lectures" title="Direct link to Lectures">​</a></h2>
<ul>
<li>Livre de Jurafsky (pages 193 à 303)<!-- -->
<ul>
<li>Week-6: Chapitre 7 - Réseaux de neurones et NLP</li>
<li>Week-8: Chapitre 9 - section 9.1 à 9.6 - Réseaux de neurones récurrents</li>
<li>Week 9: Chapitre 10 , sections 10.1 et 10.2</li>
<li>Week-10: Chapitre 11, sections 11.1 à 11.3 (sauf 11.2.2 et 11.3.4)</li>
<li>Week-11: Jurafsky et Martin, chapitre 13 de l&#x27;édition actuelle</li>
<li>Week-12: Jurfaky et Martin, 3e édition, chapitre 14, sections 14.1, 14.2, 14.4.2, 14.5 et 14.7.</li>
<li>Week-14: Jurafsky et Martin, 3e édition, chapitre 21, sections 21.1 et 21.2.</li>
<li>Ajouter le chapitre 23 sur les systèmes de questions/réponses</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="contenu-de-lexamen">Contenu de l&#x27;examen<a href="#contenu-de-lexamen" class="hash-link" aria-label="Direct link to Contenu de l&#x27;examen" title="Direct link to Contenu de l&#x27;examen">​</a></h2>
<ul>
<li>Deep NLP - Retour sur les plongements préentraînés de mots<!-- -->
<ul>
<li>Word2Vec, Glove, FastText, embeddings de Spacy</li>
<li>Modèles récurrents</li>
</ul>
</li>
<li>Récurrence, RNN, LSTM, modèle bidirectionnel, forces, lacunes, utilisation pour des applications NLP.<!-- -->
<ul>
<li>Deep NLP - Modèle transformer</li>
<li>Mécanisme d&#x27;attention multi-têtes, calcul d&#x27;attention, bloc d&#x27;attention, embeddings, position des jetons, couches résiduelles et normalisation de couche.</li>
<li>Modèle causal vs. bidirectionnel.</li>
<li>Forces, lacunes, utilisation pour des applications NLP.</li>
</ul>
</li>
<li>Deep NLP - Plongements contextuels et modèles de langue préentraînés<!-- -->
<ul>
<li>Encodeur, plongements contextuels, caractéristiques de Bert</li>
<li>Entraînement - préentraînement vs. fine-tuning</li>
<li>Aspects pratiques (par ex. tokenisation)</li>
<li>Autres modèles d&#x27;encodeurs</li>
</ul>
</li>
<li>Deep NLP - Génération de textes<!-- -->
<ul>
<li>Tâches de génération, décodeur vs. encodeur</li>
<li>Génération autorégressive, sélection des mots générés</li>
<li>Utilisation de différents modèles pour la génération, entraînement des modèles.</li>
</ul>
</li>
<li>Deep NLP - Traduction automatique et modèles encodeur-décodeur (Seq2Seq)<!-- -->
<ul>
<li>Caractéristiques linguistiques et nature d&#x27;une bonne traduction, idée générale de la traduction statistique et phrase-based, alignement de mots</li>
<li>Encodeur, décodeur, contexte et conditionnement, version récurrente avec ou sans attention, version transformer, décodage</li>
<li>Tokenisation, alignement de phrases, évaluation</li>
</ul>
</li>
<li>Deep NLP - Systèmes question-réponse<!-- -->
<ul>
<li>Recherche d&#x27;information: poids de mots et scores de documents, prétraitement, index inversé, modèle booléen, modèle vectoriel, modèle neuronal, évaluation</li>
<li>Sélection de réponses: Reading comprehension  avec un modèle span-based, avec un modèle de langue, à partir de connaissances, évaluation.</li>
<li>Extraction de relations</li>
<li>Relation vs. entités nommées</li>
<li>Approches pour l&#x27;extraction de relations.</li>
</ul>
</li>
<li>Travail pratique #2<!-- -->
<ul>
<li>Voir /Users/alain/Documents/Learning/2023A-IFT-7022/TP-2/officiel</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deep-nlp---retour-sur-les-plongements-préentraînés-de-mots">Deep NLP - Retour sur les plongements préentraînés de mots<a href="#deep-nlp---retour-sur-les-plongements-préentraînés-de-mots" class="hash-link" aria-label="Direct link to Deep NLP - Retour sur les plongements préentraînés de mots" title="Direct link to Deep NLP - Retour sur les plongements préentraînés de mots">​</a></h2>
<p>Dans le domaine du traitement du langage naturel (NLP), les plongements préentraînés de mots, ou &quot;word embeddings&quot; en anglais, sont une technique permettant de représenter les mots sous forme de vecteurs dans un espace continu. Ces vecteurs sont conçus de manière à capturer des aspects sémantiques et syntaxiques des mots. Voici quelques points clés à propos des plongements préentraînés :</p>
<ol>
<li><strong>Représentation dense</strong> : Contrairement aux représentations de mots traditionnelles, comme le one-hot encoding, les plongements de mots utilisent des vecteurs denses, généralement de quelques centaines de dimensions. Cela permet de réduire la dimensionnalité et de capturer des informations plus riches sur les mots.</li>
<li><strong>Apprentissage sur de grands corpus</strong> : Les plongements sont généralement appris à partir de très grands corpus de texte, comme Wikipedia ou des ensembles de données Web. Pendant l&#x27;apprentissage, le modèle apprend à associer des mots avec des contextes similaires à des vecteurs similaires.</li>
<li><strong>Capturer des relations sémantiques et syntaxiques</strong> : Les plongements de mots sont conçus pour refléter les relations sémantiques et syntaxiques entre les mots. Par exemple, dans un bon plongement, les mots &quot;roi&quot; et &quot;reine&quot; seront plus proches l&#x27;un de l&#x27;autre que &quot;roi&quot; et &quot;pomme&quot;.</li>
<li><strong>Modèles populaires</strong> : Des modèles tels que Word2Vec, GloVe et FastText sont parmi les plus connus pour créer des plongements de mots. Chaque modèle a une approche légèrement différente pour apprendre les relations entre les mots.</li>
<li><strong>Utilisation dans des tâches de NLP</strong> : Les plongements préentraînés sont souvent utilisés comme couche d&#x27;entrée dans des modèles de traitement du langage naturel pour des tâches comme la classification de texte, la traduction automatique, ou la reconnaissance d&#x27;entités nommées.</li>
<li><strong>Transférabilité</strong> : Un avantage majeur des plongements préentraînés est qu&#x27;ils peuvent être transférés d&#x27;un domaine à un autre, ce qui permet d&#x27;utiliser des connaissances apprises sur un grand ensemble de données dans des tâches spécifiques avec des données plus limitées.</li>
</ol>
<p>En résumé, les plongements préentraînés de mots sont un élément fondamental dans de nombreux systèmes modernes de traitement du langage naturel, offrant une manière riche et efficace de représenter le langage.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="word2vec-glove-fasttext-embeddings-de-spacy">Word2Vec, Glove, FastText, embeddings de Spacy<a href="#word2vec-glove-fasttext-embeddings-de-spacy" class="hash-link" aria-label="Direct link to Word2Vec, Glove, FastText, embeddings de Spacy" title="Direct link to Word2Vec, Glove, FastText, embeddings de Spacy">​</a></h2>
<p>Bien sûr, je vais expliquer Word2Vec, GloVe, FastText et les embeddings de spaCy, en soulignant leurs principales caractéristiques et différences.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-word2vec">1. Word2Vec<a href="#1-word2vec" class="hash-link" aria-label="Direct link to 1. Word2Vec" title="Direct link to 1. Word2Vec">​</a></h3>
<ul>
<li><strong>Développé par</strong> : Google</li>
<li><strong>Méthodologie</strong> : Word2Vec utilise des réseaux de neurones pour apprendre les représentations vectorielles des mots à partir de grands corpus de texte. Il existe deux architectures principales : CBOW (Continuous Bag of Words) et Skip-gram. CBOW prédit un mot en fonction de son contexte, tandis que Skip-gram fait l&#x27;inverse, en prédisant le contexte à partir d&#x27;un mot.</li>
<li><strong>Points forts</strong> : Capte des relations sémantiques et syntaxiques complexes. Relativement efficace en termes de calcul.</li>
<li><strong>Limitations</strong> : Ne gère pas bien les mots rares ou inconnus.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-glove-global-vectors-for-word-representation">2. GloVe (Global Vectors for Word Representation)<a href="#2-glove-global-vectors-for-word-representation" class="hash-link" aria-label="Direct link to 2. GloVe (Global Vectors for Word Representation)" title="Direct link to 2. GloVe (Global Vectors for Word Representation)">​</a></h3>
<ul>
<li><strong>Développé par</strong> : Université de Stanford</li>
<li><strong>Méthodologie</strong> : GloVe combine les avantages des approches basées sur la matrice de cooccurrence des mots et ceux des modèles prédictifs comme Word2Vec. Il utilise une matrice de cooccurrence globale des mots pour apprendre les vecteurs de mots, en essayant de minimiser la différence entre le produit scalaire des vecteurs de mots et le logarithme de leurs probabilités de cooccurrence.</li>
<li><strong>Points forts</strong> : Capable de capturer des relations globales entre les mots. Efficace pour représenter les relations linéaires entre les mots.</li>
<li><strong>Limitations</strong> : Moins efficace pour capturer des contextes locaux.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-fasttext">3. FastText<a href="#3-fasttext" class="hash-link" aria-label="Direct link to 3. FastText" title="Direct link to 3. FastText">​</a></h3>
<ul>
<li><strong>Développé par</strong> : Facebook</li>
<li><strong>Méthodologie</strong> : FastText étend l&#x27;approche de Word2Vec en représentant chaque mot comme un sac de sous-chaînes de caractères (n-grammes). Cela permet au modèle de mieux gérer les mots hors vocabulaire et les mots rares.</li>
<li><strong>Points forts</strong> : Efficace pour les langues avec une riche morphologie. Peut gérer les mots hors vocabulaire.</li>
<li><strong>Limitations</strong> : Les vecteurs générés peuvent être plus grands et plus lents à traiter que ceux de Word2Vec.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-embeddings-de-spacy">4. Embeddings de spaCy<a href="#4-embeddings-de-spacy" class="hash-link" aria-label="Direct link to 4. Embeddings de spaCy" title="Direct link to 4. Embeddings de spaCy">​</a></h3>
<ul>
<li><strong>Développé par</strong> : spaCy (outil de NLP)</li>
<li><strong>Méthodologie</strong> : spaCy propose des embeddings préentraînés pour différentes langues, souvent basés sur des modèles comme GloVe ou des transformateurs comme BERT. Ces embeddings sont intégrés dans les modèles de traitement du langage de spaCy.</li>
<li><strong>Points forts</strong> : Facilité d&#x27;utilisation et intégration dans un pipeline de NLP complet. Bonne performance sur diverses tâches de NLP.</li>
<li><strong>Limitations</strong> : Dépend de la qualité des modèles préentraînés fournis par spaCy. Moins flexible si l&#x27;on souhaite personnaliser les embeddings.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="comparaisons-clés">Comparaisons clés<a href="#comparaisons-clés" class="hash-link" aria-label="Direct link to Comparaisons clés" title="Direct link to Comparaisons clés">​</a></h3>
<ul>
<li><strong>Approche d&#x27;apprentissage</strong> : Word2Vec et FastText se concentrent sur les contextes locaux, tandis que GloVe se concentre sur les statistiques globales de cooccurrence. spaCy utilise une variété de sources pour ses embeddings.</li>
<li><strong>Gestion des mots rares ou inconnus</strong> : FastText excelle dans ce domaine grâce à sa méthode de traitement des n-grammes.</li>
<li><strong>Utilisation et intégration</strong> : spaCy offre la facilité d&#x27;intégration et d&#x27;utilisation dans des applications de NLP, tandis que les autres nécessitent une configuration et une intégration plus manuelles.</li>
<li><strong>Flexibilité</strong> : Word2Vec, GloVe et FastText offrent une plus grande flexibilité pour l&#x27;entraînement sur des corpus spécifiques, tandis que spaCy dépend de modèles préentraînés.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rnn-modèles-récurrents">RNN (modèles récurrents)<a href="#rnn-modèles-récurrents" class="hash-link" aria-label="Direct link to RNN (modèles récurrents)" title="Direct link to RNN (modèles récurrents)">​</a></h3>
<ul>
<li>Lacunes des réseaux récurrents<!-- -->
<ul>
<li>Traitement de gauche à droite. Le modèle bidirectionnel est une alternative.</li>
<li>Les dépendances entres les mots lointains sont difficiles à établir.</li>
<li>Traitement séquentiel difficile à paralléliser.</li>
</ul>
</li>
<li>Lacunes des RNNs<!-- -->
<ul>
<li>Il est difficile d&#x27;entraîner des RNNs. Durant la rétro propagation, le signal d&#x27;erreur d&#x27;atténue rapidement. gradient évanescent.</li>
<li>Dans les RNN traditionnels, le gradient utilisé pour la mise à jour des poids peut devenir très petit, rendant l&#x27;apprentissage inefficace sur de longues séquences. Les LSTM et les GRU ont été conçus pour surmonter ce problème.</li>
<li>L&#x27;information d&#x27;une unité tend à être locale. Basé principalement sur les entrées récentes</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lstm-modèle-bidirectionnel-forces-lacunes-utilisation-pour-des-applications-nlp">LSTM, modèle bidirectionnel, forces, lacunes, utilisation pour des applications NLP<a href="#lstm-modèle-bidirectionnel-forces-lacunes-utilisation-pour-des-applications-nlp" class="hash-link" aria-label="Direct link to LSTM, modèle bidirectionnel, forces, lacunes, utilisation pour des applications NLP" title="Direct link to LSTM, modèle bidirectionnel, forces, lacunes, utilisation pour des applications NLP">​</a></h3>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="modèle-transformer">Modèle transformer<a href="#modèle-transformer" class="hash-link" aria-label="Direct link to Modèle transformer" title="Direct link to Modèle transformer">​</a></h2>
<ul>
<li>La dépendances entre les mots sont capturées par un <strong>mécanisme d&#x27;attention</strong></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mécanisme-dattention">Mécanisme d&#x27;attention<a href="#mécanisme-dattention" class="hash-link" aria-label="Direct link to Mécanisme d&#x27;attention" title="Direct link to Mécanisme d&#x27;attention">​</a></h3>
<ol>
<li><strong>Attention de base</strong> : L&#x27;attention est un mécanisme qui permet au modèle de se concentrer sur certaines parties de l&#x27;entrée lorsqu&#x27;il traite ou génère du texte. Dans le contexte du traitement du langage, cela signifie que le modèle peut accorder plus d&#x27;importance à certains mots lors de la compréhension ou de la génération de phrases.</li>
<li><strong>Calcul d&#x27;attention</strong> : L&#x27;attention est calculée en utilisant trois ensembles de vecteurs pour chaque mot : les clés (keys), les valeurs (values) et les requêtes (queries). L&#x27;idée est de calculer le degré d&#x27;attention (poids) que chaque mot (requête) devrait accorder à tous les autres mots (clés) et d&#x27;utiliser ces poids pour produire une représentation pondérée (valeur).</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="mécanisme-dattention-multi-têtes">Mécanisme d&#x27;attention multi-têtes<a href="#mécanisme-dattention-multi-têtes" class="hash-link" aria-label="Direct link to Mécanisme d&#x27;attention multi-têtes" title="Direct link to Mécanisme d&#x27;attention multi-têtes">​</a></h4>
<ol>
<li><strong>Multi-têtes</strong> : Dans un Transformer, l&#x27;attention est divisée en plusieurs &quot;têtes&quot;. Chaque tête d&#x27;attention apprend à se concentrer sur différentes parties de l&#x27;entrée. Par exemple, une tête peut se concentrer sur le contexte syntaxique, tandis qu&#x27;une autre peut se concentrer sur des aspects sémantiques spécifiques.</li>
<li><strong>Avantages</strong> : Cette approche permet au modèle de capturer plusieurs types de relations, comme les relations syntaxiques et sémantiques, de manière plus efficace et plus riche qu&#x27;avec une seule tête d&#x27;attention.</li>
<li><strong>Fonctionnement</strong> : Chaque tête d&#x27;attention effectue son propre calcul d&#x27;attention en parallèle, puis les sorties de toutes les têtes sont combinées et linéairement transformées pour produire le résultat final.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="impact-dans-les-modèles-transformers">Impact dans les modèles transformers<a href="#impact-dans-les-modèles-transformers" class="hash-link" aria-label="Direct link to Impact dans les modèles transformers" title="Direct link to Impact dans les modèles transformers">​</a></h4>
<ol>
<li><strong>Compréhension contextuelle améliorée</strong> : Le mécanisme d&#x27;attention multi-têtes permet aux Transformers de comprendre le contexte d&#x27;une manière plus nuancée, ce qui est crucial pour des tâches telles que la compréhension de texte, la traduction automatique, et la génération de texte.</li>
<li><strong>Flexibilité et puissance</strong> : La capacité à se concentrer simultanément sur différents aspects d&#x27;une séquence rend les Transformers particulièrement puissants et flexibles pour traiter des séquences de données complexes.</li>
<li><strong>Efficacité en parallèle</strong> : Les calculs d&#x27;attention multi-têtes peuvent être effectués en parallèle, ce qui rend les Transformers très efficaces en termes de calcul, particulièrement adaptés pour l&#x27;entraînement sur des GPU.</li>
</ol>
<p>En résumé, le mécanisme d&#x27;attention multi-têtes est un élément crucial qui contribue à la puissance et à l&#x27;efficacité des modèles Transformers, leur permettant de capturer des relations complexes et variées dans les données séquentielles.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="calcul-dattention-dans-un-transformer">Calcul d&#x27;attention dans un transformer<a href="#calcul-dattention-dans-un-transformer" class="hash-link" aria-label="Direct link to Calcul d&#x27;attention dans un transformer" title="Direct link to Calcul d&#x27;attention dans un transformer">​</a></h3>
<p>Dans un modèle Transformer, le calcul de l&#x27;attention est effectué par le mécanisme d&#x27;attention, qui est au cœur de l&#x27;architecture. Le mécanisme d&#x27;attention le plus couramment utilisé dans les Transformers est l&#x27;attention dite &quot;Scaled Dot-Product Attention&quot;. Voici comment elle est calculée :</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="éléments-de-base">Éléments de base<a href="#éléments-de-base" class="hash-link" aria-label="Direct link to Éléments de base" title="Direct link to Éléments de base">​</a></h4>
<ol>
<li><strong>Queries (Q)</strong>, <strong>Keys (K)</strong>, et <strong>Values (V)</strong> : Ce sont trois ensembles de vecteurs obtenus à partir des entrées du modèle. En pratique, pour un mot donné dans une séquence, son vecteur d&#x27;entrée est transformé en vecteurs Q, K et V par multiplication avec trois matrices de poids distinctes (apprentissables).</li>
<li><strong>Score d&#x27;Attention</strong> : Le score d&#x27;attention entre deux mots est calculé en prenant le produit scalaire entre le vecteur de requête correspondant à un mot et le vecteur de clé d&#x27;un autre mot. Cela donne une mesure de la compatibilité ou de la pertinence entre les mots.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="calcul-de-lattention-voir-page-182">Calcul de l&#x27;attention (voir page 182)<a href="#calcul-de-lattention-voir-page-182" class="hash-link" aria-label="Direct link to Calcul de l&#x27;attention (voir page 182)" title="Direct link to Calcul de l&#x27;attention (voir page 182)">​</a></h4>
<ol>
<li><strong>Produit Scalaire entre Q et K</strong> : On commence par calculer le produit scalaire entre les vecteurs de requête et de clé. Cela permet d&#x27;évaluer dans quelle mesure les caractéristiques de chaque mot sont alignées.</li>
<li><strong>Mise à l&#x27;échelle</strong> : Les scores obtenus sont ensuite divisés par la racine carrée de la dimension des clés (pour éviter que le produit scalaire ne devienne trop grand si la dimension est élevée, ce qui pourrait entraîner des gradients instables pendant l&#x27;apprentissage).</li>
<li><strong>Softmax</strong> : On applique ensuite la fonction softmax aux scores d&#x27;attention mis à l&#x27;échelle. Cela convertit les scores en valeurs de probabilité, permettant au modèle de déterminer quels mots devraient recevoir le plus d&#x27;attention.</li>
<li><strong>Multiplication avec V</strong> : Les scores d&#x27;attention softmax sont ensuite multipliés avec les vecteurs de valeur. Cela permet d&#x27;obtenir une représentation pondérée de l&#x27;entrée, où l&#x27;importance de chaque mot est ajustée en fonction des scores d&#x27;attention.</li>
<li><strong>Sortie</strong> : Enfin, les sorties de l&#x27;étape précédente (pour toutes les têtes d&#x27;attention, dans le cas de l&#x27;attention multi-têtes) sont combinées et passent par une couche linéaire finale pour produire le résultat du mécanisme d&#x27;attention.</li>
</ol>
<p>Voici les formules mathématiques LaTeX pour le calcul de l&#x27;attention dans un modèle Transformer, spécifiquement pour le mécanisme d&#x27;attention &quot;Scaled Dot-Product Attention&quot; :</p>
<p>Étape 1. <strong>Calcul des Scores d&#x27;Attention</strong> :</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4684em;vertical-align:-0.95em"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em"><span style="top:-2.2528em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span></span></span></span></span>
<p>Ici, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span>, et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span></span></span></span> sont les matrices des requêtes, des clés, et des valeurs, respectivement. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> représente la dimension des clés.</p>
<p>Étape 2. <strong>Produit Scalaire entre Q et K</strong> :</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi mathvariant="normal">⊤</mi></msup></mrow><annotation encoding="application/x-tex">QK^\top</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0935em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span></span></span></span>
<p>Cette opération calcule le produit scalaire entre les vecteurs de requête et de clé.</p>
<p>Étape 3. <strong>Mise à l&#x27;Échelle</strong> :</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\frac{QK^\top}{\sqrt{d_k}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4561em;vertical-align:-0.93em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.2528em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<p>Les scores sont divisés par la racine carrée de la dimension des clés ( d_k ) pour éviter que les gradients deviennent trop grands lors de l&#x27;apprentissage.</p>
<p>Étape 4. <strong>Application de Softmax</strong> :</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4684em;vertical-align:-0.95em"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em"><span style="top:-2.2528em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span></span></span></span></span>
<p>La fonction softmax est appliquée pour normaliser les scores d&#x27;attention, les transformant en probabilités.</p>
<p>Étape 5. <strong>Multiplication par V</strong> :</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mo fence="true">(</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">\left(\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\right) V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4684em;vertical-align:-0.95em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em"><span style="top:-2.2528em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span></span></span></span></span>
<p>Les scores d&#x27;attention normalisés sont multipliés avec la matrice des valeurs pour obtenir la sortie finale de l&#x27;attention.</p>
<p>Dans le cas de l&#x27;attention multi-têtes, ce processus est répété pour chaque tête d&#x27;attention, avec des matrices de poids distinctes pour <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span>, et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span></span></span></span> dans chaque tête. Les sorties de toutes les têtes sont ensuite combinées et passent par une couche linéaire pour produire le résultat final.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="résumé">Résumé<a href="#résumé" class="hash-link" aria-label="Direct link to Résumé" title="Direct link to Résumé">​</a></h4>
<p>Le mécanisme d&#x27;attention dans les Transformers permet au modèle de se concentrer sur différentes parties d&#x27;une séquence lors du traitement, en ajustant l&#x27;importance accordée à chaque mot en fonction du contexte. Ce calcul d&#x27;attention est crucial pour permettre aux Transformers de gérer efficacement les longues dépendances et de comprendre les contextes complexes dans les séquences de données.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bloc-dattention">Bloc d&#x27;attention<a href="#bloc-dattention" class="hash-link" aria-label="Direct link to Bloc d&#x27;attention" title="Direct link to Bloc d&#x27;attention">​</a></h3>
<p>Dans un modèle Transformer, un &quot;bloc d&#x27;attention&quot; se réfère à une composante spécifique de l&#x27;architecture qui utilise le mécanisme d&#x27;attention pour traiter les données. Un bloc d&#x27;attention est essentiellement une unité de calcul qui permet au modèle de se concentrer sur différentes parties d&#x27;une séquence d&#x27;entrée et d&#x27;ajuster son traitement en fonction du contexte. Voici les éléments clés d&#x27;un bloc d&#x27;attention :</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="composantes-dun-bloc-dattention">Composantes d&#x27;un bloc d&#x27;attention<a href="#composantes-dun-bloc-dattention" class="hash-link" aria-label="Direct link to Composantes d&#x27;un bloc d&#x27;attention" title="Direct link to Composantes d&#x27;un bloc d&#x27;attention">​</a></h4>
<ol>
<li><strong>Mécanisme d&#x27;attention</strong> : Le cœur du bloc d&#x27;attention est le mécanisme d&#x27;attention lui-même, souvent un mécanisme d&#x27;attention multi-têtes. Ce mécanisme permet au modèle de calculer et d&#x27;appliquer des scores d&#x27;attention aux entrées.</li>
<li><strong>Têtes d&#x27;attention multiples</strong> : Dans l&#x27;attention multi-têtes, plusieurs têtes d&#x27;attention fonctionnent en parallèle, chacune se concentrant sur différentes parties de l&#x27;information dans les données d&#x27;entrée. Cela permet au modèle de capturer divers aspects du contexte.</li>
<li><strong>Combinaison des têtes d&#x27;attention</strong> : Les sorties de toutes les têtes d&#x27;attention sont combinées (souvent par concaténation) et passent ensuite par une couche linéaire pour produire un vecteur de sortie unique pour le bloc d&#x27;attention.</li>
<li><strong>Connexion résiduelle et normalisation</strong> : Typiquement, le bloc d&#x27;attention inclut également une connexion résiduelle autour du mécanisme d&#x27;attention, suivie d&#x27;une couche de normalisation. La connexion résiduelle aide à éviter le problème de disparition du gradient en permettant au flux d&#x27;information de contourner le mécanisme d&#x27;attention.</li>
<li><strong>Feed-Forward Neural Network</strong> : Après l&#x27;attention et la normalisation, il y a généralement un petit réseau de neurones feed-forward (dense) pour une transformation supplémentaire des données.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="fonctionnement-dun-bloc-dattention">Fonctionnement d&#x27;un bloc d&#x27;attention<a href="#fonctionnement-dun-bloc-dattention" class="hash-link" aria-label="Direct link to Fonctionnement d&#x27;un bloc d&#x27;attention" title="Direct link to Fonctionnement d&#x27;un bloc d&#x27;attention">​</a></h4>
<ul>
<li>Le bloc d&#x27;attention prend en entrée une séquence de vecteurs (par exemple, des représentations de mots dans une phrase).</li>
<li>Chaque tête d&#x27;attention dans le bloc calcule indépendamment des scores d&#x27;attention pour ces vecteurs, permettant au modèle de se concentrer sur différentes parties de la séquence.</li>
<li>Les sorties des têtes d&#x27;attention sont combinées pour former une représentation unifiée, qui est ensuite passée à travers d&#x27;autres couches de traitement.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="importance-dans-les-transformers">Importance dans les transformers<a href="#importance-dans-les-transformers" class="hash-link" aria-label="Direct link to Importance dans les transformers" title="Direct link to Importance dans les transformers">​</a></h4>
<ul>
<li>Les blocs d&#x27;attention permettent aux modèles Transformers de traiter efficacement des séquences de données, en accordant une attention proportionnelle aux différents éléments de la séquence en fonction de leur pertinence.</li>
<li>Cette capacité de se concentrer sur différentes parties d&#x27;une séquence rend les Transformers particulièrement puissants pour des tâches comme la compréhension de texte, la traduction automatique, et la génération de langage.</li>
</ul>
<p>En résumé, les blocs d&#x27;attention sont des composantes cruciales qui confèrent aux modèles Transformers leur capacité unique à traiter des séquences de manière contextuellement sensible et dynamique.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="embeddings">Embeddings<a href="#embeddings" class="hash-link" aria-label="Direct link to Embeddings" title="Direct link to Embeddings">​</a></h3>
<p>Dans un modèle Transformer, les &quot;embeddings&quot; (plongements en français) sont utilisés pour convertir les données d&#x27;entrée, telles que des mots ou des phrases, en vecteurs denses de nombres réels. Ces vecteurs denses contiennent des informations codées sur les éléments d&#x27;entrée de manière à ce que des relations sémantiques et syntaxiques soient représentées dans un espace vectoriel. Voici quelques points clés concernant les embeddings dans les modèles Transformers :</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="rôle-des-embeddings">Rôle des Embeddings<a href="#rôle-des-embeddings" class="hash-link" aria-label="Direct link to Rôle des Embeddings" title="Direct link to Rôle des Embeddings">​</a></h4>
<ol>
<li><strong>Représentation dense</strong> : Les embeddings transforment les données d&#x27;entrée, souvent des éléments discrets comme des mots ou des caractères, en vecteurs denses. Cela permet de passer d&#x27;une représentation de haute dimensionnalité et sparse (comme le one-hot encoding) à une représentation de basse dimensionnalité et dense.</li>
<li><strong>Information sémantique et syntaxique</strong> : Les embeddings sont conçus pour incorporer des informations sémantiques et syntaxiques sur les mots ou les phrases, ce qui signifie que des éléments de langage similaires auront des embeddings similaires.</li>
<li><strong>Point de départ pour le traitement</strong> : Dans un modèle Transformer, les embeddings sont le point de départ pour le traitement par les couches suivantes du modèle. Ils fournissent la représentation initiale sur laquelle le modèle va construire pour réaliser des tâches plus complexes.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="types-dembeddings-dans-un-transformer">Types d&#x27;embeddings dans un transformer<a href="#types-dembeddings-dans-un-transformer" class="hash-link" aria-label="Direct link to Types d&#x27;embeddings dans un transformer" title="Direct link to Types d&#x27;embeddings dans un transformer">​</a></h4>
<ol>
<li><strong>Embeddings de token</strong> : Ils représentent les mots ou les sous-unités de mots (comme les morceaux de mots dans les modèles utilisant le sous-échantillonnage des mots). Chaque token unique dans le vocabulaire est mappé à un vecteur d&#x27;embedding distinct.</li>
<li><strong>Embeddings positionnels</strong> : Les Transformers n&#x27;ayant pas de mécanisme inhérent pour gérer l&#x27;ordre des éléments dans une séquence, des embeddings positionnels sont ajoutés aux embeddings de token pour fournir des informations sur la position de chaque élément dans la séquence.</li>
<li><strong>Embeddings de segment</strong> : Dans certaines tâches, comme la compréhension de texte avec plusieurs phrases, les embeddings de segment peuvent être utilisés pour distinguer différents segments ou parties d&#x27;entrée.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="importance-dans-les-transformers-252">Importance dans les transformers (.252)<a href="#importance-dans-les-transformers-252" class="hash-link" aria-label="Direct link to Importance dans les transformers (.252)" title="Direct link to Importance dans les transformers (.252)">​</a></h4>
<ul>
<li>Les embeddings sont essentiels pour transformer des données d&#x27;entrée brutes, qui sont souvent non numériques et de grande dimension, en une forme adaptée pour le traitement par les couches de réseaux neuronaux.</li>
<li>Ils permettent de capturer des nuances de langage et de contexte dès la première étape du traitement dans le modèle Transformer.</li>
</ul>
<p>En conclusion, les embeddings dans un modèle Transformer sont fondamentaux pour la représentation initiale des données d&#x27;entrée, fournissant une base riche en informations pour les opérations d&#x27;attention et le traitement ultérieur par le modèle.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="connexion-résiduelle">Connexion résiduelle<a href="#connexion-résiduelle" class="hash-link" aria-label="Direct link to Connexion résiduelle" title="Direct link to Connexion résiduelle">​</a></h3>
<p>La connexion résiduelle aide à éviter le problème de disparition du gradient en permettant au flux d&#x27;information de contourner le mécanisme d&#x27;attention.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="normalisation-de-couche">Normalisation de couche<a href="#normalisation-de-couche" class="hash-link" aria-label="Direct link to Normalisation de couche" title="Direct link to Normalisation de couche">​</a></h3>
<p>La normalisation de couche (Layer Normalization) est une technique importante utilisée dans les modèles de type Transformer pour stabiliser et accélérer l&#x27;entraînement des réseaux de neurones. Voici une explication de ce concept et son rôle dans les Transformers :</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="quest-ce-que-la-normalisation-de-couche-">Qu&#x27;est-ce que la normalisation de couche ?<a href="#quest-ce-que-la-normalisation-de-couche-" class="hash-link" aria-label="Direct link to Qu&#x27;est-ce que la normalisation de couche ?" title="Direct link to Qu&#x27;est-ce que la normalisation de couche ?">​</a></h4>
<ol>
<li><strong>But</strong> : La normalisation de couche vise à normaliser les activations à l&#x27;intérieur d&#x27;une couche du réseau. Elle aide à réduire le problème de &quot;covariate shift&quot;, où la distribution des entrées d&#x27;une couche change en raison des ajustements des paramètres des couches précédentes pendant l&#x27;entraînement.</li>
<li><strong>Fonctionnement</strong> : Contrairement à la normalisation par lots (Batch Normalization), qui normalise les données à travers le mini-lot, la normalisation de couche normalise les données à travers les caractéristiques pour chaque échantillon individuellement. Pour chaque échantillon, elle calcule la moyenne et l&#x27;écart-type pour la normalisation sur toutes les caractéristiques.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="formule-de-la-normalisation-de-couche">Formule de la normalisation de couche<a href="#formule-de-la-normalisation-de-couche" class="hash-link" aria-label="Direct link to Formule de la normalisation de couche" title="Direct link to Formule de la normalisation de couche">​</a></h4>
<p>La normalisation de couche est généralement réalisée en suivant ces étapes :</p>
<ol>
<li>Calcul de la moyenne et de l&#x27;écart-type pour chaque échantillon individuellement sur toutes les caractéristiques de la couche.</li>
<li>Normalisation des activations en soustrayant la moyenne et en divisant par l&#x27;écart-type.</li>
<li>Application d&#x27;une transformation affine avec des paramètres apprenables pour chaque caractéristique.</li>
</ol>
<p>Mathématiquement, pour un vecteur d&#x27;activation <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span>, la normalisation de couche est donnée par :</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mi>γ</mi><mrow><mo fence="true">(</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac><mo fence="true">)</mo></mrow><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">y = \gamma \left( \frac{x - \mu}{\sigma} \right) + \beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>
<p>où <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span> est la moyenne, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span></span> est l&#x27;écart-type, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span> est un paramètre de mise à l&#x27;échelle, et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span> est un paramètre de décalage, tous deux apprenables.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="rôle-dans-les-transformers">Rôle dans les transformers<a href="#rôle-dans-les-transformers" class="hash-link" aria-label="Direct link to Rôle dans les transformers" title="Direct link to Rôle dans les transformers">​</a></h4>
<ul>
<li><strong>Stabilité de l&#x27;entraînement</strong> : En normalisant les activations, la normalisation de couche contribue à stabiliser et à accélérer l&#x27;entraînement des modèles.</li>
<li><strong>Indépendance vis-à-vis de la taille du mini-lot</strong> : Contrairement à la normalisation par lots, la normalisation de couche n&#x27;est pas dépendante de la taille du mini-lot, ce qui est particulièrement utile pour les modèles comme les Transformers où des mini-lots de différentes tailles peuvent être utilisés.</li>
<li><strong>Amélioration de la performance</strong> : La normalisation de couche a été montrée pour améliorer la performance des modèles dans diverses tâches, en particulier dans les architectures de type Transformer.</li>
</ul>
<p>En conclusion, la normalisation de couche dans les Transformers est un élément crucial pour assurer un entraînement stable et efficace, et pour améliorer la performance générale du modèle.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="modèle-causal-vs-bidirectionnel">Modèle causal vs. bidirectionnel<a href="#modèle-causal-vs-bidirectionnel" class="hash-link" aria-label="Direct link to Modèle causal vs. bidirectionnel" title="Direct link to Modèle causal vs. bidirectionnel">​</a></h3>
<p>Dans le contexte des modèles Transformers, la distinction entre modèles &quot;causaux&quot; et &quot;bidirectionnels&quot; réfère à la manière dont l&#x27;information est traitée dans les séquences, en particulier concernant la direction dans laquelle les données sont prises en compte. Ces deux approches ont des implications importantes pour la manière dont les modèles comprennent et génèrent du langage.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="modèles-causaux">Modèles causaux<a href="#modèles-causaux" class="hash-link" aria-label="Direct link to Modèles causaux" title="Direct link to Modèles causaux">​</a></h4>
<ol>
<li><strong>Définition</strong> : Les modèles causaux, aussi appelés modèles à sens unique ou auto-régressifs, traitent les informations dans un ordre séquentiel, généralement de gauche à droite. Dans ce contexte, chaque token (mot ou symbole) ne peut être influencé que par les tokens précédents dans la séquence.</li>
<li><strong>Utilisation</strong> : Ces modèles sont typiquement utilisés pour la génération de texte, où chaque mot suivant dépend des mots précédents. Par exemple, lors de la génération de texte, le modèle prend en compte tous les mots précédents pour prédire le mot suivant.</li>
<li><strong>Exemples de Modèles</strong> : GPT (Generative Pretrained Transformer) de OpenAI est un exemple célèbre de modèle Transformer causal.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="modèles-bidirectionnels">Modèles bidirectionnels<a href="#modèles-bidirectionnels" class="hash-link" aria-label="Direct link to Modèles bidirectionnels" title="Direct link to Modèles bidirectionnels">​</a></h4>
<ol>
<li><strong>Définition</strong> : Les modèles bidirectionnels traitent les informations en prenant en compte à la fois les tokens précédents et suivants dans une séquence. Cela signifie que le contexte de chaque token est constitué de tous les autres tokens de la séquence.</li>
<li><strong>Utilisation</strong> : Ces modèles sont particulièrement utiles pour des tâches de compréhension du langage, telles que la réponse à des questions ou la classification de texte, où la compréhension du contexte complet est nécessaire.</li>
<li><strong>Exemples de Modèles</strong> : BERT (Bidirectional Encoder Representations from Transformers) est un exemple bien connu de modèle Transformer bidirectionnel.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="comparaison-et-implications">Comparaison et implications<a href="#comparaison-et-implications" class="hash-link" aria-label="Direct link to Comparaison et implications" title="Direct link to Comparaison et implications">​</a></h4>
<ul>
<li><strong>Flux d&#x27;Information</strong> : Dans les modèles causaux, l&#x27;information ne peut se déplacer que dans une direction (généralement du passé vers le futur), tandis que dans les modèles bidirectionnels, l&#x27;information circule dans les deux sens.</li>
<li><strong>Génération de Texte vs Compréhension de Texte</strong> : Les modèles causaux sont mieux adaptés à la génération de texte, car ils génèrent une sortie token par token. Les modèles bidirectionnels, quant à eux, excèlent dans des tâches de compréhension du texte, car ils considèrent le contexte global.</li>
<li><strong>Apprentissage et Utilisation</strong> : Les modèles bidirectionnels nécessitent souvent des stratégies d&#x27;apprentissage spécifiques, comme le &quot;Masked Language Modeling&quot; (MLM) utilisé dans BERT, où certains mots de l&#x27;entrée sont masqués et le modèle doit prédire ces mots en se basant sur le contexte.</li>
</ul>
<p>En résumé, la différence principale entre les modèles causaux et bidirectionnels dans l&#x27;architecture Transformer réside dans la manière dont ils traitent le contexte : séquentiellement pour les modèles causaux et de manière globale pour les modèles bidirectionnels. Cette différence détermine les types de tâches pour lesquelles chaque modèle est le mieux adapté.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="forces-et-lacunes-dans-les-applications-nlp">Forces et lacunes dans les applications NLP<a href="#forces-et-lacunes-dans-les-applications-nlp" class="hash-link" aria-label="Direct link to Forces et lacunes dans les applications NLP" title="Direct link to Forces et lacunes dans les applications NLP">​</a></h3>
<p>Les modèles Transformers, depuis leur introduction, ont révolutionné le domaine du traitement du langage naturel (NLP). Ils présentent plusieurs forces et lacunes qui les rendent adaptés à certaines applications tout en étant moins efficaces pour d&#x27;autres.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forces-des-transformers-en-nlp">Forces des Transformers en NLP<a href="#forces-des-transformers-en-nlp" class="hash-link" aria-label="Direct link to Forces des Transformers en NLP" title="Direct link to Forces des Transformers en NLP">​</a></h4>
<ol>
<li><strong>Compréhension du Contexte</strong> : Grâce à leur architecture, les Transformers sont capables de comprendre le contexte d&#x27;un texte de manière très efficace, ce qui est crucial pour des tâches comme la compréhension de texte, la traduction automatique, et la génération de texte.</li>
<li><strong>Traitement Parallèle</strong> : Contrairement aux modèles récurrents, les Transformers traitent l&#x27;ensemble d&#x27;une séquence d&#x27;entrée en parallèle, ce qui permet une accélération significative de l&#x27;entraînement et de l&#x27;inférence.</li>
<li><strong>Gestion des Dépendances à Longue Distance</strong> : Les Transformers peuvent gérer efficacement des dépendances à longue distance dans le texte, ce qui est un défi majeur pour d&#x27;autres types de modèles comme les RNN.</li>
<li><strong>Flexibilité et Adaptabilité</strong> : Les Transformers peuvent être adaptés à une grande variété de tâches de NLP en ajustant les couches supérieures du réseau ou en les utilisant comme des modèles pré-entraînés pour le fine-tuning.</li>
<li><strong>Performance de Pointe</strong> : Les Transformers ont établi de nouveaux standards de performance dans presque toutes les tâches de NLP, des benchmarks de compréhension de texte à la génération de langage naturel.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="lacunes-des-transformers-en-nlp">Lacunes des Transformers en NLP<a href="#lacunes-des-transformers-en-nlp" class="hash-link" aria-label="Direct link to Lacunes des Transformers en NLP" title="Direct link to Lacunes des Transformers en NLP">​</a></h4>
<ol>
<li><strong>Exigences Computationnelles Élevées</strong> : L&#x27;entraînement des Transformers, en particulier des grands modèles, nécessite une grande quantité de puissance de calcul et de données, ce qui peut être prohibitif.</li>
<li><strong>Complexité et Sur-paramétrisation</strong> : Ces modèles sont souvent très grands et complexes, ce qui peut entraîner des difficultés dans leur compréhension, leur interprétation et leur optimisation.</li>
<li><strong>Risques de Biais et de Toxicité</strong> : Comme ils sont souvent entraînés sur de vastes corpus de données Internet, les Transformers peuvent intégrer et amplifier des biais existants dans les données d&#x27;entraînement.</li>
<li><strong>Gestion des Données Incomplètes ou Bruitées</strong> : Les Transformers peuvent être moins robustes face à des données incomplètes, bruitées ou non standard, nécessitant des données bien nettoyées et structurées pour une performance optimale.</li>
<li><strong>Manque d&#x27;Explicabilité</strong> : Les décisions prises par les Transformers, en particulier par les modèles plus grands, peuvent être difficiles à interpréter, posant des défis pour l&#x27;explicabilité et la fiabilité.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h4>
<p>Les Transformers sont extrêmement puissants pour une large gamme de tâches de NLP, offrant une compréhension contextuelle avancée et des capacités de traitement parallèle. Cependant, leurs exigences en termes de ressources, leur complexité, et les défis liés à la gestion des biais et à l&#x27;explicabilité, doivent être pris en compte lors de leur utilisation dans des applications réelles. Des recherches sont en cours pour surmonter certaines de ces lacunes, notamment en développant des modèles plus efficaces et explicables.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="transformers---plongements-contextuels">Transformers - Plongements contextuels<a href="#transformers---plongements-contextuels" class="hash-link" aria-label="Direct link to Transformers - Plongements contextuels" title="Direct link to Transformers - Plongements contextuels">​</a></h3>
<ul>
<li>Word2Vec, Glove, FastText... fournissent une seule représentation par mot, qui ne varie pas en fonction du sens ou du contexte.</li>
<li>Les plongements contextuels réfèrent à un vecteur d&#x27;embedding qui prend en compte les mots environnants du texte.</li>
<li>C&#x27;est ce que font les transformers encodeurs<!-- -->
<ul>
<li>Un vecteur (embedding) en sortie pour chaque mot</li>
<li>Le mécanisme d&#x27;attention ajouter le contexte.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bert">BERT<a href="#bert" class="hash-link" aria-label="Direct link to BERT" title="Direct link to BERT">​</a></h3>
<p>Le modèle BERT (Bidirectional Encoder Representations from Transformers) est étroitement lié à la notion de Transformers encodeurs et de plongements contextuels. Ces concepts sont interdépendants et jouent des rôles cruciaux dans l&#x27;efficacité de BERT et d&#x27;autres modèles de traitement du langage naturel basés sur les Transformers. Voici comment ils sont interconnectés :</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="transformers-encodeurs">Transformers encodeurs<a href="#transformers-encodeurs" class="hash-link" aria-label="Direct link to Transformers encodeurs" title="Direct link to Transformers encodeurs">​</a></h4>
<ol>
<li><strong>Architecture des transformers</strong> : Les Transformers, introduits dans le papier &quot;Attention Is All You Need&quot;, sont composés de deux types principaux de blocs : les encodeurs et les décodeurs. Les encodeurs sont responsables de la compréhension du texte d&#x27;entrée, tandis que les décodeurs sont utilisés pour la génération de texte.</li>
<li><strong>Role dans BERT</strong> : BERT utilise uniquement la partie encodeur de l&#x27;architecture Transformer. Dans BERT, plusieurs couches d&#x27;encodeurs sont empilées pour traiter le texte d&#x27;entrée, chacune appliquant le mécanisme d&#x27;attention multi-têtes et des réseaux de neurones feed-forward pour encoder les informations contextuelles.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="plongements-contextuels">Plongements contextuels<a href="#plongements-contextuels" class="hash-link" aria-label="Direct link to Plongements contextuels" title="Direct link to Plongements contextuels">​</a></h4>
<ul>
<li><strong>Définition</strong> : Les plongements contextuels sont des représentations vectorielles de mots qui tiennent compte du contexte dans lequel chaque mot apparaît. Contrairement aux plongements de mots statiques (comme Word2Vec ou GloVe), où un mot a toujours le même vecteur indépendamment de son contexte, les plongements contextuels varient en fonction des mots environnants.</li>
<li><strong>BERT et plongements contextuels</strong> : BERT génère des plongements contextuels pour chaque mot d&#x27;entrée. Grâce à son architecture bidirectionnelle, BERT prend en compte l&#x27;ensemble du contexte d&#x27;une séquence (les mots avant et après le mot cible) pour produire ces plongements. Cela permet à BERT de comprendre le sens spécifique des mots en fonction de leur contexte.</li>
<li><strong>Encodeurs bidirectionnels</strong> :<!-- -->
<ul>
<li>Les encodeus bidirectionnels sont utilisés pour générer des représentations de mots contextualisées (<em>contextual embeddings</em>).</li>
<li>Un encodeur bidirectionnel de type transformer est préentraîné comme un modèle de langue masqué<!-- -->
<ul>
<li>Parfois avec d’autres tâches auxiliaires comme le NSP</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="liens-et-implications">Liens et implications<a href="#liens-et-implications" class="hash-link" aria-label="Direct link to Liens et implications" title="Direct link to Liens et implications">​</a></h4>
<ul>
<li><strong>Compréhension contextuelle</strong> : L&#x27;utilisation des encodeurs Transformer dans BERT lui permet de générer des plongements contextuels profonds. Cela signifie que chaque mot est encodé en tenant compte de l&#x27;ensemble du contexte de la phrase, ce qui rend ces représentations particulièrement riches et nuancées.</li>
<li><strong>Bidirectionalité</strong> : La capacité de BERT à traiter le contexte de manière bidirectionnelle (à la fois avant et après chaque mot dans une séquence) est une avancée majeure par rapport aux modèles précédents qui ne traitaient le texte que de manière unidirectionnelle.</li>
<li><strong>Applications polyvalentes</strong> : Grâce à ces plongements contextuels, BERT excelle dans une variété de tâches de NLP, telles que la compréhension de texte, la classification, la réponse aux questions, et la reconnaissance d&#x27;entités nommées.</li>
</ul>
<p>En conclusion, BERT intègre les concepts de Transformers encodeurs et de plongements contextuels pour créer un modèle capable de comprendre le langage de manière plus nuancée et contextuellement riche. Cette intégration a marqué une étape importante dans l&#x27;évolution des modèles de traitement du langage naturel.</p>
<p>BERT (Bidirectional Encoder Representations from Transformers) est lui-même un modèle de langue préentraîné et a joué un rôle crucial dans la popularisation de cette approche dans le domaine du traitement du langage naturel (NLP). Voici les liens clés entre BERT et les modèles de langue préentraînés :</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bert-comme-modèle-de-langue-préentraîné">BERT comme modèle de langue préentraîné<a href="#bert-comme-modèle-de-langue-préentraîné" class="hash-link" aria-label="Direct link to BERT comme modèle de langue préentraîné" title="Direct link to BERT comme modèle de langue préentraîné">​</a></h4>
<ul>
<li><strong>Préentraînement et fine-tuning</strong> : Comme d&#x27;autres modèles de langue, BERT est d&#x27;abord préentraîné sur un large corpus de texte dans une tâche non supervisée (ou faiblement supervisée) avant d&#x27;être fine-tuné pour des tâches spécifiques de NLP. Le préentraînement permet à BERT d&#x27;apprendre une compréhension générale du langage, tandis que le fine-tuning ajuste le modèle aux exigences spécifiques d&#x27;une tâche.</li>
<li><strong>Approches de préentraînement</strong> : BERT utilise deux tâches principales pour le préentraînement non supervisé : le <strong>Masked Language Model (MLM)</strong> (Modèle de langue masqué) où le modèle doit prédire des mots masqués dans une phrase, et la <strong>Next Sentence Prediction (NSP)</strong> (Prédiction de la prochaine phrase)  où le modèle prédit si une phrase donnée suit logiquement une autre phrase. Ces tâches aident BERT à développer une compréhension contextuelle du langage. Voir acétate page 104/329.</li>
<li>On peut réutiliser le modèle pour une application (page 129):<!-- -->
<ul>
<li>En ajoutant une tête de prédiction et</li>
<li>En ajustant les paramètres sur des exemples du domaine (fine-tuning)</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="impact-de-bert-sur-les-modèles-de-langue-préentraînés">Impact de BERT sur les modèles de langue préentraînés<a href="#impact-de-bert-sur-les-modèles-de-langue-préentraînés" class="hash-link" aria-label="Direct link to Impact de BERT sur les modèles de langue préentraînés" title="Direct link to Impact de BERT sur les modèles de langue préentraînés">​</a></h4>
<ol>
<li><strong>Modèle bidirectionnel</strong> : BERT a innové en étant l&#x27;un des premiers modèles de langue préentraînés à utiliser une approche bidirectionnelle, permettant une compréhension plus profonde du contexte en considérant à la fois les mots précédents et suivants dans une phrase.</li>
<li><strong>Amélioration des performances de NLP</strong> : BERT a établi de nouveaux standards de performance dans de nombreuses tâches de NLP, démontrant l&#x27;efficacité des modèles de langue préentraînés pour une large gamme d&#x27;applications.</li>
<li><strong>Influence sur les modèles suivants</strong> : Le succès de BERT a inspiré le développement de nombreux autres modèles de langue préentraînés, tels que GPT (de OpenAI), RoBERTa, DistilBERT, ALBERT, et plus encore, chacun apportant des améliorations et des variations à l&#x27;approche initiale de BERT.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="implications-pour-le-domaine-du-nlp">Implications pour le domaine du NLP<a href="#implications-pour-le-domaine-du-nlp" class="hash-link" aria-label="Direct link to Implications pour le domaine du NLP" title="Direct link to Implications pour le domaine du NLP">​</a></h4>
<ul>
<li><strong>Approche standard pour le NLP</strong> : L&#x27;approche de préentraînement suivie de fine-tuning, popularisée par BERT, est devenue la méthode standard pour développer des modèles de NLP performants.</li>
<li><strong>Nécessité de grandes ressources de calcul</strong> : Le préentraînement de modèles comme BERT nécessite d&#x27;importantes ressources de calcul, ce qui a des implications sur la recherche et le développement dans le domaine du NLP.</li>
<li><strong>Attention sur les biais et la généralisation</strong> : Avec la popularité des modèles comme BERT, il y a une prise de conscience accrue des problèmes de biais dans les données de préentraînement et de la nécessité de modèles capables de généraliser efficacement sur divers ensembles de données.</li>
</ul>
<p>En résumé, BERT représente une avancée majeure dans l&#x27;utilisation des modèles de langue préentraînés en NLP, en établissant un nouveau cadre pour le développement et l&#x27;application de techniques avancées dans ce domaine. Son approche bidirectionnelle et ses stratégies de préentraînement ont influencé de nombreux modèles subséquents et continuent de façonner le champ du traitement du langage naturel.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="quest-ce-que-mnli-ner-squad">Qu&#x27;est-ce que MNLI, NER, SQuAD?<a href="#quest-ce-que-mnli-ner-squad" class="hash-link" aria-label="Direct link to Qu&#x27;est-ce que MNLI, NER, SQuAD?" title="Direct link to Qu&#x27;est-ce que MNLI, NER, SQuAD?">​</a></h3>
<p>MNLI, NER et SQuAD sont trois tâches ou benchmarks importants dans le domaine du traitement du langage naturel (NLP), souvent utilisés pour évaluer les performances des modèles de NLP comme BERT, GPT, et autres architectures basées sur les Transformers. Voici un aperçu de chacun :</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-mnli-multi-genre-natural-language-inference">1. MNLI (Multi-Genre Natural Language Inference)<a href="#1-mnli-multi-genre-natural-language-inference" class="hash-link" aria-label="Direct link to 1. MNLI (Multi-Genre Natural Language Inference)" title="Direct link to 1. MNLI (Multi-Genre Natural Language Inference)">​</a></h4>
<ul>
<li><strong>Objectif</strong> : MNLI est un benchmark pour la tâche d&#x27;inférence de langage naturel. L&#x27;objectif est de prédire si une hypothèse (une phrase) est vraie (entailment), fausse (contradiction), ou indéterminée (neutre) en relation avec une prémisse donnée.</li>
<li><strong>Utilisation</strong> : MNLI est utilisé pour évaluer la capacité d&#x27;un modèle à comprendre et à raisonner sur le langage naturel. Il couvre un large éventail de genres de texte, y compris la conversation téléphonique, la littérature, et les articles de presse.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-ner-named-entity-recognition">2. NER (Named Entity Recognition)<a href="#2-ner-named-entity-recognition" class="hash-link" aria-label="Direct link to 2. NER (Named Entity Recognition)" title="Direct link to 2. NER (Named Entity Recognition)">​</a></h4>
<ul>
<li><strong>Objectif</strong> : NER est une tâche de NLP qui vise à identifier et classer les entités nommées dans un texte en catégories prédéfinies telles que les noms de personnes, les organisations, les lieux, les expressions de temps, les quantités, les valeurs monétaires, etc.</li>
<li><strong>Utilisation</strong> : NER est essentiel dans de nombreuses applications de NLP, telles que l&#x27;extraction d&#x27;informations, la recherche d&#x27;informations, la réponse aux questions, et l&#x27;analyse de sentiment. Il sert à évaluer la capacité d&#x27;un modèle à reconnaître et comprendre les entités dans le contexte d&#x27;une phrase ou d&#x27;un texte.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="3-squad-stanford-question-answering-dataset">3. SQuAD (Stanford Question Answering Dataset)<a href="#3-squad-stanford-question-answering-dataset" class="hash-link" aria-label="Direct link to 3. SQuAD (Stanford Question Answering Dataset)" title="Direct link to 3. SQuAD (Stanford Question Answering Dataset)">​</a></h4>
<ul>
<li><strong>Objectif</strong> : SQuAD est un benchmark pour les tâches de réponse aux questions basées sur le texte. Il consiste en un ensemble de questions posées par des humains sur un ensemble d&#x27;articles de Wikipedia, où la réponse à chaque question est un segment de texte (un span) de l&#x27;article correspondant.</li>
<li><strong>Utilisation</strong> : SQuAD teste la capacité d&#x27;un modèle à comprendre un passage de texte et à y trouver la réponse à une question spécifique. C&#x27;est un outil d&#x27;évaluation important pour les modèles de compréhension de texte et les systèmes de réponse aux questions.</li>
</ul>
<p>Ces benchmarks jouent un rôle crucial dans le développement et l&#x27;évaluation des modèles de NLP, permettant aux chercheurs et aux développeurs de mesurer les progrès et les performances de leurs systèmes dans des tâches de compréhension et de traitement du langage naturel.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bert-et-aspects-pratiques">BERT et aspects pratiques<a href="#bert-et-aspects-pratiques" class="hash-link" aria-label="Direct link to BERT et aspects pratiques" title="Direct link to BERT et aspects pratiques">​</a></h3>
<p>Bien sûr, discutons des aspects pratiques de l&#x27;utilisation de BERT, notamment la tokenisation, qui est une étape cruciale pour préparer les données pour le traitement par ce modèle. BERT, en tant que modèle de traitement du langage naturel basé sur l&#x27;architecture des Transformers, utilise des méthodes et des conventions spécifiques pour traiter le texte.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="tokenisation-wordpiece">Tokenisation WordPiece<a href="#tokenisation-wordpiece" class="hash-link" aria-label="Direct link to Tokenisation WordPiece" title="Direct link to Tokenisation WordPiece">​</a></h4>
<ul>
<li>Voir acétate 123 à 128.</li>
<li><strong>Tokenisation WordPiece</strong> : BERT utilise un algorithme de tokenisation appelé WordPiece. Cette méthode décompose le texte en un ensemble limité de tokens (morceaux de mots) communs. Par exemple, le mot &quot;unwatchable&quot; pourrait être décomposé en &quot;un&quot;, &quot;##watch&quot;, et &quot;##able&quot;.</li>
<li><strong>Tokens spéciaux</strong> :<!-- -->
<ul>
<li><strong>[CLS]</strong> : Chaque séquence d&#x27;entrée commence par un token spécial [CLS]. Pour les tâches de classification, la représentation de ce token est utilisée comme représentation agrégée de l&#x27;ensemble de la séquence.</li>
<li><strong>[SEP]</strong> : Utilisé pour séparer différentes séquences, par exemple, dans les tâches qui nécessitent deux entrées distinctes (comme la comparaison de phrases).</li>
<li><strong>[PAD]</strong> : Utilisé pour remplir les séquences jusqu&#x27;à une longueur fixe.</li>
</ul>
</li>
<li><strong>Longueur de séquence fixe</strong> : BERT nécessite que toutes les séquences d&#x27;entrée aient la même longueur. Si une séquence est trop courte, elle est remplie avec des tokens [PAD]. Si elle est trop longue, elle est tronquée.</li>
<li><strong>Entraînement avec la tokenisation WordPiece</strong>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>wordpieces</mtext><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\text{wordpieces}_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9386em;vertical-align:-0.2441em"></span><span class="mord"><span class="mord text"><span class="mord">wordpieces</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span></span></span></span> : commencer avec des caractères individuels.</li>
<li>En utilisant un corpus, répéter le processus suivant jusqu&#x27;à obtenir <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|V|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mord">∣</span></span></span></span> wordpieces (où <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|V|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mord">∣</span></span></span></span> est la taille désirée du vocabulaire) :<!-- -->
<ul>
<li>Entraîner un modèle de langue sur le corpus en utilisant les wordpieces actuels.</li>
<li>Considérer des paires de wordpieces adjacents et évaluer leur probabilité conjointe avec le modèle de langue.</li>
<li>Créer un nouveau wordpiece en fusionnant la paire de wordpieces qui, une fois combinée, augmente le plus la probabilité du modèle de langue sur le corpus.</li>
</ul>
</li>
</ul>
</li>
<li>Les scores sont calculés pour évaluer la probabilité de fusion de paires de tokens adjacents pour former de nouveaux tokens WordPiece plus grands. La formule générale pour calculer le score d&#x27;une paire de tokens est la suivante :</li>
</ul>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Score</mtext><mo stretchy="false">(</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub><mo separator="true">,</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mtext>Frequence de la paire </mtext><mo stretchy="false">(</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub><mo separator="true">,</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mrow><mtext>Frequence de </mtext><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><mtext>Frequence de </mtext><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Score}(token_1, token_2) = \frac{\text{Frequence de la paire } (token_1, token_2)}{\text{Frequence de } token_1 \times \text{Frequence de } token_2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">Score</span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.3074em;vertical-align:-0.8804em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord text"><span class="mord">Frequence de </span></span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord text"><span class="mord">Frequence de </span></span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord text"><span class="mord">Frequence de la paire </span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="embeddings-431">Embeddings (.431)<a href="#embeddings-431" class="hash-link" aria-label="Direct link to Embeddings (.431)" title="Direct link to Embeddings (.431)">​</a></h4>
<ul>
<li><strong>Plongements de token</strong> : Chaque token est converti en un vecteur d&#x27;embedding. Ces embeddings sont appris pendant le préentraînement de BERT.</li>
<li><strong>Plongements positionnels</strong> : Comme les Transformers n&#x27;ont pas de notion intrinsèque de l&#x27;ordre des mots dans une séquence, BERT ajoute des plongements positionnels aux embeddings de token pour fournir des informations sur la position des mots dans la séquence.</li>
<li><strong>Plongements de segment</strong> : Pour les tâches impliquant deux séquences distinctes (comme les questions et les contextes dans les tâches de réponse aux questions), BERT utilise des embeddings de segment pour distinguer les deux séquences.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="entraînement-et-fine-tuning">Entraînement et fine-tuning<a href="#entraînement-et-fine-tuning" class="hash-link" aria-label="Direct link to Entraînement et fine-tuning" title="Direct link to Entraînement et fine-tuning">​</a></h4>
<ul>
<li><strong>Préentraînement</strong> : BERT est préentraîné sur de vastes corpus de texte en utilisant deux tâches : le Masked Language Modeling (MLM) et le Next Sentence Prediction (NSP).</li>
<li><strong>Fine-tuning</strong> : Pour une tâche spécifique, BERT est fine-tuné avec des données supplémentaires. Cette étape ajuste les poids préentraînés pour les adapter à la tâche ciblée.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="utilisation-pratique">Utilisation pratique<a href="#utilisation-pratique" class="hash-link" aria-label="Direct link to Utilisation pratique" title="Direct link to Utilisation pratique">​</a></h4>
<ul>
<li><strong>Choix de la version de BERT</strong> : Il existe différentes versions de BERT (par exemple, BERT Base, BERT Large) avec différentes tailles et capacités. Le choix dépend des ressources disponibles et des exigences de la tâche.</li>
<li><strong>Ressources computationnelles</strong> : BERT est exigeant en termes de ressources computationnelles, notamment pour le fine-tuning. L&#x27;utilisation de GPU ou de TPU est recommandée pour accélérer l&#x27;entraînement.</li>
<li><strong>Intégration dans des pipelines de NLP</strong> : BERT peut être intégré dans des pipelines de NLP existants, en remplacement ou en complément des approches de traitement du langage traditionnelles.</li>
</ul>
<p>En conclusion, BERT apporte une approche sophistiquée et puissante pour le traitement du langage naturel, mais son utilisation pratique nécessite une compréhension de ses méthodes de tokenisation, de ses embeddings, ainsi que des considérations sur le préentraînement, le fine-tuning, et les ressources computationnelles nécessaires.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="autres-modèles-dencodeurs">Autres modèles d&#x27;encodeurs<a href="#autres-modèles-dencodeurs" class="hash-link" aria-label="Direct link to Autres modèles d&#x27;encodeurs" title="Direct link to Autres modèles d&#x27;encodeurs">​</a></h3>
<p>En plus de BERT, plusieurs autres modèles d&#x27;encodeurs ont été développés dans le domaine du traitement du langage naturel (NLP). Ces modèles, basés sur l&#x27;architecture des Transformers, se concentrent principalement sur l&#x27;encodage de séquences de texte pour produire des représentations riches et contextuelles. Voici quelques-uns des modèles d&#x27;encodeurs les plus notables :</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-roberta-robustly-optimized-bert-approach">1. RoBERTa (Robustly Optimized BERT Approach)<a href="#1-roberta-robustly-optimized-bert-approach" class="hash-link" aria-label="Direct link to 1. RoBERTa (Robustly Optimized BERT Approach)" title="Direct link to 1. RoBERTa (Robustly Optimized BERT Approach)">​</a></h4>
<ul>
<li><strong>Développeur</strong> : Facebook AI</li>
<li><strong>Améliorations par rapport à BERT</strong> : RoBERTa modifie le processus de préentraînement de BERT, en utilisant des lots plus grands, en éliminant la tâche de prédiction de la prochaine phrase (NSP), et en entraînant le modèle sur un corpus plus large pendant plus longtemps.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-distilbert">2. DistilBERT<a href="#2-distilbert" class="hash-link" aria-label="Direct link to 2. DistilBERT" title="Direct link to 2. DistilBERT">​</a></h4>
<ul>
<li><strong>Développeur</strong> : Hugging Face</li>
<li><strong>Caractéristique</strong> : DistilBERT est une version simplifiée et optimisée de BERT. Il est conçu pour avoir des performances proches de BERT tout en étant plus léger et plus rapide, ce qui est réalisé par une technique appelée &quot;distillation de connaissances&quot;.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="3-albert-a-lite-bert">3. ALBERT (A Lite BERT)<a href="#3-albert-a-lite-bert" class="hash-link" aria-label="Direct link to 3. ALBERT (A Lite BERT)" title="Direct link to 3. ALBERT (A Lite BERT)">​</a></h4>
<ul>
<li><strong>Développeur</strong> : Google Research</li>
<li><strong>Caractéristiques</strong> : ALBERT est une version allégée de BERT, qui utilise une factorisation de matrice dans les embeddings et des paramètres partagés entre les couches pour réduire drastiquement la taille du modèle et augmenter la vitesse d&#x27;entraînement.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="4-transformer-xl">4. Transformer-XL<a href="#4-transformer-xl" class="hash-link" aria-label="Direct link to 4. Transformer-XL" title="Direct link to 4. Transformer-XL">​</a></h4>
<ul>
<li><strong>Développeur</strong> : Google Brain et Carnegie Mellon University</li>
<li><strong>Spécialité</strong> : Transformer-XL est conçu pour améliorer la capacité des Transformers à gérer des séquences de texte longues. Il introduit un mécanisme qui permet de conserver l&#x27;information des segments de texte précédents, améliorant ainsi la compréhension du contexte sur de longues distances.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="5-ernie-enhanced-representation-through-knowledge-integration">5. ERNIE (Enhanced Representation through kNowledge Integration)<a href="#5-ernie-enhanced-representation-through-knowledge-integration" class="hash-link" aria-label="Direct link to 5. ERNIE (Enhanced Representation through kNowledge Integration)" title="Direct link to 5. ERNIE (Enhanced Representation through kNowledge Integration)">​</a></h4>
<ul>
<li><strong>Développeur</strong> : Baidu</li>
<li><strong>Caractéristique</strong> : ERNIE vise à améliorer la représentation du langage en intégrant des connaissances externes. Il est préentraîné sur des tâches qui nécessitent une compréhension des relations entre les entités et des concepts sémantiques.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="6-electra">6. ELECTRA<a href="#6-electra" class="hash-link" aria-label="Direct link to 6. ELECTRA" title="Direct link to 6. ELECTRA">​</a></h4>
<ul>
<li><strong>Développeur</strong> : Google Research</li>
<li><strong>Innovation</strong> : ELECTRA utilise une approche différente pour le préentraînement, connue sous le nom de &quot;Replaced Token Detection&quot;. Au lieu de prédire des tokens masqués, il est entraîné pour distinguer entre les tokens &quot;réels&quot; et les tokens &quot;remplacés&quot; (choisis par un autre petit modèle).</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="7-camembert">7. CamemBERT<a href="#7-camembert" class="hash-link" aria-label="Direct link to 7. CamemBERT" title="Direct link to 7. CamemBERT">​</a></h4>
<ul>
<li><strong>Origine et Langue Ciblée</strong> : CamemBERT est un modèle de langue basé sur l&#x27;architecture de BERT, spécifiquement formé sur la langue française. Il a été développé conjointement par plusieurs instituts de recherche français.</li>
<li><strong>Caractéristiques</strong> : Ce modèle utilise une approche similaire à celle de RoBERTa en termes de préentraînement et est optimisé pour mieux comprendre et traiter le français.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="8-flaubert">8. FlauBERT<a href="#8-flaubert" class="hash-link" aria-label="Direct link to 8. FlauBERT" title="Direct link to 8. FlauBERT">​</a></h4>
<ul>
<li><strong>Origine et Langue Ciblée</strong> : FlauBERT est également un modèle axé sur la langue française, développé par des chercheurs français.</li>
<li><strong>Caractéristiques</strong> : Il est conçu pour fournir une compréhension en profondeur du français, en utilisant une approche de préentraînement similaire à celle de BERT, mais sur un large corpus en français. FlauBERT est utilisé pour diverses tâches de NLP telles que la classification de texte, la compréhension de texte et l&#x27;analyse de sentiment.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="9-xlm-cross-lingual-language-model">9. XLM (Cross-lingual Language Model)<a href="#9-xlm-cross-lingual-language-model" class="hash-link" aria-label="Direct link to 9. XLM (Cross-lingual Language Model)" title="Direct link to 9. XLM (Cross-lingual Language Model)">​</a></h4>
<ul>
<li><strong>Objectif</strong> : XLM est un modèle de langue multilingue qui vise à améliorer la compréhension et la génération de texte dans plusieurs langues.</li>
<li><strong>Caractéristiques</strong> : XLM est important pour les tâches qui nécessitent une compréhension multilingue, comme la traduction automatique. Il utilise des techniques comme le préentraînement avec des paires de phrases dans différentes langues pour améliorer la compréhension contextuelle interlinguistique.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="10-xlm-roberta-xlm-r">10. XLM-Roberta (XLM-R)<a href="#10-xlm-roberta-xlm-r" class="hash-link" aria-label="Direct link to 10. XLM-Roberta (XLM-R)" title="Direct link to 10. XLM-Roberta (XLM-R)">​</a></h4>
<ul>
<li><strong>Origine et Objectif</strong> : XLM-Roberta est une extension de XLM et RoBERTa, développée par Facebook AI.</li>
<li><strong>Caractéristiques</strong> : XLM-R est un modèle multilingue qui a été préentraîné sur un corpus de texte massif couvrant un grand nombre de langues. Il adopte une méthode de préentraînement similaire à celle de RoBERTa et est particulièrement efficace pour les tâches de NLP dans diverses langues, surpassant les modèles antérieurs dans de nombreux benchmarks multilingues.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="décodeur-vs-encodeur-pour-la-génération-de-textes">Décodeur vs. encodeur pour la génération de textes<a href="#décodeur-vs-encodeur-pour-la-génération-de-textes" class="hash-link" aria-label="Direct link to Décodeur vs. encodeur pour la génération de textes" title="Direct link to Décodeur vs. encodeur pour la génération de textes">​</a></h3>
<p>Dans l&#x27;architecture des modèles de traitement du langage naturel basés sur les Transformers, la distinction entre encodeurs et décodeurs est fondamentale, surtout lorsqu&#x27;il s&#x27;agit de tâches de génération de texte. Voici comment chaque partie fonctionne et leur rôle dans la génération de texte :</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="encodeur">Encodeur<a href="#encodeur" class="hash-link" aria-label="Direct link to Encodeur" title="Direct link to Encodeur">​</a></h4>
<ol>
<li><strong>Traitement de l&#x27;entrée</strong> : Les encodeurs sont chargés de traiter la séquence d&#x27;entrée (par exemple, une phrase dans la langue source pour une tâche de traduction automatique) et de la transformer en une série de représentations vectorielles riches en informations contextuelles.</li>
<li><strong>Extraction de caractéristiques</strong> : Chaque couche de l&#x27;encodeur extrait différents niveaux de caractéristiques du texte et encode l&#x27;information sémantique et syntaxique dans des vecteurs de haute dimension.</li>
<li><strong>Pas de génération directe</strong> : Les encodeurs ne génèrent pas de texte par eux-mêmes ; ils préparent et fournissent le contexte pour les décodeurs afin qu&#x27;ils puissent générer la sortie appropriée.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="décodeur">Décodeur<a href="#décodeur" class="hash-link" aria-label="Direct link to Décodeur" title="Direct link to Décodeur">​</a></h4>
<ol>
<li><strong>Génération de la sortie</strong> : Les décodeurs prennent les représentations vectorielles fournies par les encodeurs et les utilisent pour générer la séquence de sortie, un élément à la fois. Pour une tâche de génération de texte, cela pourrait être la production d&#x27;une phrase dans la langue cible.</li>
<li><strong>Auto-régressif</strong> : Dans les modèles de génération de texte, les décodeurs sont généralement auto-régressifs, ce qui signifie qu&#x27;ils génèrent la séquence de sortie token par token, en utilisant les tokens précédemment générés comme contexte supplémentaire pour le prochain token.</li>
<li><strong>Attention croisée</strong> : Les décodeurs utilisent un mécanisme d&#x27;attention croisée qui leur permet de se concentrer sur différentes parties de la représentation de l&#x27;encodeur à chaque étape de la génération. Cela les aide à déterminer quelle partie de l&#x27;entrée est la plus pertinente pour prédire le prochain mot.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="génération-de-texte-avec-encodeurs-et-décodeurs">Génération de texte avec encodeurs et décodeurs<a href="#génération-de-texte-avec-encodeurs-et-décodeurs" class="hash-link" aria-label="Direct link to Génération de texte avec encodeurs et décodeurs" title="Direct link to Génération de texte avec encodeurs et décodeurs">​</a></h4>
<p>Dans le processus de génération de texte :</p>
<ul>
<li>L&#x27;<strong>encodeur</strong> lit et comprend le texte d&#x27;entrée (ou le contexte donné), produisant une série de vecteurs qui capturent le contexte de chaque mot ou token dans la séquence d&#x27;entrée.</li>
<li>Le <strong>décodeur</strong> commence ensuite avec un token de début de séquence et génère la suite du texte en se basant sur les vecteurs contextuels de l&#x27;encodeur ainsi que sur ce qu&#x27;il a déjà généré.</li>
</ul>
<p>Dans un modèle Transformer complet pour la génération de texte, comme dans la traduction automatique, les encodeurs et les décodeurs travaillent ensemble de manière séquentielle. L&#x27;encodeur traite d&#x27;abord toute la séquence d&#x27;entrée, et le décodeur prend ensuite le relais pour produire la sortie.</p>
<p>Les modèles de génération de texte comme GPT (Generative Pretrained Transformer) fonctionnent différemment, en ce qu&#x27;ils utilisent uniquement la partie décodeur du Transformer. Ils sont préentraînés pour prédire le prochain mot dans une séquence en se basant sur tous les mots précédents, ce qui leur permet de générer du texte de manière cohérente et contextuellement pertinente sans l&#x27;utilisation explicite d&#x27;un encodeur séparé.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="génération-autorégressive-sélection-des-mots-générés">Génération autorégressive, sélection des mots générés<a href="#génération-autorégressive-sélection-des-mots-générés" class="hash-link" aria-label="Direct link to Génération autorégressive, sélection des mots générés" title="Direct link to Génération autorégressive, sélection des mots générés">​</a></h3>
<p>La génération autorégressive est une approche séquentielle de la génération de texte où chaque nouveau mot ou token est généré un par un, et chaque token généré est conditionné par les tokens précédemment générés. Cette méthode est largement utilisée dans les modèles de traitement du langage naturel, tels que GPT (Generative Pretrained Transformer) et d&#x27;autres modèles basés sur les Transformers.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="fonctionnement-de-la-génération-autorégressive">Fonctionnement de la génération autorégressive<a href="#fonctionnement-de-la-génération-autorégressive" class="hash-link" aria-label="Direct link to Fonctionnement de la génération autorégressive" title="Direct link to Fonctionnement de la génération autorégressive">​</a></h4>
<ol>
<li><strong>Initialisation</strong> : Le processus commence avec un token initial, qui est souvent un token spécial indiquant le début d&#x27;une séquence, comme <code>[START]</code> ou simplement le premier mot d&#x27;une phrase.</li>
<li><strong>Génération séquentielle</strong> : À chaque étape, le modèle prédit le token suivant en se basant sur la séquence de tokens qu&#x27;il a déjà générée. C&#x27;est-à-dire que la probabilité du prochain token est conditionnée par tous les tokens précédents.</li>
<li><strong>Utilisation du modèle</strong> : Le modèle utilise ses paramètres internes, qui comprennent généralement des mécanismes d&#x27;attention et des réseaux de neurones, pour calculer la distribution de probabilité du prochain token.</li>
<li><strong>Choix du token</strong> : Une fois la distribution de probabilité obtenue, un token est sélectionné comme étant le prochain dans la séquence. Ce choix peut être fait de plusieurs manières, comme décrit ci-dessous.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="sélection-des-mots-générés">Sélection des mots générés<a href="#sélection-des-mots-générés" class="hash-link" aria-label="Direct link to Sélection des mots générés" title="Direct link to Sélection des mots générés">​</a></h4>
<ul>
<li><strong>Greedy Search (approche vorace)</strong> : Sélectionne le token le plus probable à chaque étape. C&#x27;est le moyen le plus direct, mais il peut conduire à des problèmes tels que le manque de variété et la répétition. Localement optimal.</li>
<li><strong>Beam Search</strong> : On retient les <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span> meilleurs candidats à chaque étape, où &quot;meilleur&quot; est défini par la probabilité. Le nombre de candidats conservés est le &quot;beam width&quot;. Cette méthode équilibre entre l&#x27;efficacité et la diversité.</li>
<li><strong>Sampling</strong> : Choisit le prochain token aléatoirement selon la distribution de probabilité. Cela augmente la diversité et la créativité dans la génération de texte mais peut parfois produire des résultats incohérents. Pas de garantie de trouver la meilleure traduction, mais on se rapproche de l&#x27;optimum global. Donc plus probable de trouver une meilleure traduction</li>
<li><strong>Top-k Sampling</strong> : Restreint le tirage aléatoire aux k tokens les plus probables, ce qui aide à éviter les erreurs tout en permettant une certaine variété.</li>
<li><strong>Top-p (ou Nucleus) Sampling</strong> : Au lieu de fixer un nombre k de candidats, le top-p sampling choisit un petit ensemble de tokens dont la somme cumulée des probabilités dépasse un seuil p. Cela permet de filtrer les tokens à faible probabilité tout en conservant la diversité dynamique.</li>
</ul>
<p>La génération autorégressive est un processus itératif qui s&#x27;arrête généralement lorsque le token de fin de séquence est généré ou après avoir atteint une longueur maximale prédéterminée pour la séquence.</p>
<p>En résumé, la génération autorégressive est une méthode puissante pour la génération de texte qui utilise la connaissance du contexte précédent pour générer des séquences de mots cohérentes et pertinentes. La sélection des mots générés est un aspect critique qui peut grandement influencer la qualité et la variabilité du texte produit.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="décodage-beam-search-page-175">Décodage Beam Search (page 175)<a href="#décodage-beam-search-page-175" class="hash-link" aria-label="Direct link to Décodage Beam Search (page 175)" title="Direct link to Décodage Beam Search (page 175)">​</a></h4>
<p>Le Beam Search est une stratégie de recherche heuristique utilisée dans la génération de séquences, comme la traduction automatique, pour trouver la séquence de mots la plus probable. Au lieu de générer un seul mot à la fois (comme avec la recherche gloutonne), le Beam Search maintient un nombre fixe de meilleures alternatives à chaque étape, connu sous le nom de &quot;beam width&quot;.</p>
<p>Voici les grandes lignes de l&#x27;approche Beam Search :</p>
<ol>
<li>À chaque étape de la génération, plusieurs chemins sont explorés, et non pas un seul.</li>
<li>Chaque chemin est évalué en fonction d&#x27;une fonction de score, souvent la probabilité log.</li>
<li>Seuls les &#x27;k&#x27; chemins les plus prometteurs sont conservés, où &#x27;k&#x27; est la largeur du faisceau (beam width).</li>
<li>Ce processus est répété jusqu&#x27;à ce que la fin de la séquence soit atteinte pour tous les chemins.</li>
<li>Le chemin avec le score le plus élevé à la fin du processus est choisi comme résultat final.</li>
</ol>
<p>Le Beam Search est un compromis entre la recherche exhaustive, qui est souvent impraticable, et la recherche gloutonne, qui peut manquer des séquences de haute qualité. En gardant plusieurs chemins possibles ouverts à chaque étape, le Beam Search augmente la probabilité de trouver une séquence de sortie plus optimale.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="traduction-automatique">Traduction automatique<a href="#traduction-automatique" class="hash-link" aria-label="Direct link to Traduction automatique" title="Direct link to Traduction automatique">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="caractéristiques-linguistiques">Caractéristiques linguistiques<a href="#caractéristiques-linguistiques" class="hash-link" aria-label="Direct link to Caractéristiques linguistiques" title="Direct link to Caractéristiques linguistiques">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="typologie-de-lordre-des-mots-page-134">Typologie de l&#x27;ordre des mots (page 134)<a href="#typologie-de-lordre-des-mots-page-134" class="hash-link" aria-label="Direct link to Typologie de l&#x27;ordre des mots (page 134)" title="Direct link to Typologie de l&#x27;ordre des mots (page 134)">​</a></h4>
<p>En linguistique, les termes SVO, SOV et VSO font référence à l&#x27;ordre typique des mots dans une phrase, en particulier l&#x27;ordre des constituants principaux : le sujet (S), le verbe (V) et l&#x27;objet (O).</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="svo-sujet-verbe-objet">SVO (Sujet-Verbe-Objet)<a href="#svo-sujet-verbe-objet" class="hash-link" aria-label="Direct link to SVO (Sujet-Verbe-Objet)" title="Direct link to SVO (Sujet-Verbe-Objet)">​</a></h5>
<ul>
<li><strong>Description</strong> : Dans les langues SVO, l&#x27;ordre typique des mots dans une phrase simple est sujet suivi du verbe, puis de l&#x27;objet.</li>
<li><strong>Exemples de Langues</strong> : L&#x27;anglais, le français, le chinois et le russe sont des exemples de langues où l&#x27;ordre SVO est prédominant. Par exemple, en anglais, on dirait &quot;She (S) eats (V) apples (O).&quot;</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="sov-sujet-objet-verbe">SOV (Sujet-Objet-Verbe)<a href="#sov-sujet-objet-verbe" class="hash-link" aria-label="Direct link to SOV (Sujet-Objet-Verbe)" title="Direct link to SOV (Sujet-Objet-Verbe)">​</a></h5>
<ul>
<li><strong>Description</strong> : Les langues SOV placent l&#x27;objet directement après le sujet et avant le verbe.</li>
<li><strong>Exemples de Langues</strong> : Le japonais, le coréen, le turc et le latin classique sont des langues typiquement SOV. Par exemple, en japonais, la construction serait équivalente à &quot;She (S) apples (O) eats (V).&quot;</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="vso-verbe-sujet-objet">VSO (Verbe-Sujet-Objet)<a href="#vso-verbe-sujet-objet" class="hash-link" aria-label="Direct link to VSO (Verbe-Sujet-Objet)" title="Direct link to VSO (Verbe-Sujet-Objet)">​</a></h5>
<ul>
<li><strong>Description</strong> : Dans les langues VSO, le verbe vient en premier, suivi du sujet, puis de l&#x27;objet.</li>
<li><strong>Exemples de Langues</strong> : L&#x27;arabe classique et le gallois sont des langues où l&#x27;ordre VSO est courant. Par exemple, en arabe classique, on pourrait trouver une structure de phrase qui se traduit par &quot;Eats (V) she (S) apples (O).&quot;</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="vos-verbe-objet-sujet">VOS (Verbe-Objet-Sujet)<a href="#vos-verbe-objet-sujet" class="hash-link" aria-label="Direct link to VOS (Verbe-Objet-Sujet)" title="Direct link to VOS (Verbe-Objet-Sujet)">​</a></h5>
<ul>
<li><strong>Description</strong> : Moins commun, cet ordre place le verbe en premier, suivi de l&#x27;objet puis du sujet.</li>
<li><strong>Exemples de Langues</strong> : Certaines langues austronésiennes utilisent cette structure.</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="ovs-objet-verbe-sujet-et-osv-objet-sujet-verbe">OVS (Objet-Verbe-Sujet) et OSV (Objet-Sujet-Verbe)<a href="#ovs-objet-verbe-sujet-et-osv-objet-sujet-verbe" class="hash-link" aria-label="Direct link to OVS (Objet-Verbe-Sujet) et OSV (Objet-Sujet-Verbe)" title="Direct link to OVS (Objet-Verbe-Sujet) et OSV (Objet-Sujet-Verbe)">​</a></h5>
<ul>
<li><strong>Description</strong> : Ces ordres de mots sont rares dans les langues naturelles. Ils placent l&#x27;objet avant le sujet et le verbe.</li>
<li><strong>Exemples de Langues</strong> : Il y a très peu de langues qui utilisent ces ordres comme structure principale.</li>
</ul>
<p>La typologie de l&#x27;ordre des mots est un aspect fondamental de la syntaxe des langues du monde. Elle influence la façon dont les phrases sont construites et comment les informations sont hiérarchisées et transmises. Bien que la plupart des langues tendent à favoriser un ordre des mots particulier, beaucoup d&#x27;entre elles peuvent être flexibles et permettent divers ordres pour exprimer différentes nuances, emphases ou pour des raisons stylistiques.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="position-des-mots-et-des-adjectifs">Position des mots et des adjectifs<a href="#position-des-mots-et-des-adjectifs" class="hash-link" aria-label="Direct link to Position des mots et des adjectifs" title="Direct link to Position des mots et des adjectifs">​</a></h4>
<p>En anglais : <em>green witch</em>, en français : sorcière verte. Voir page 135.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="mots-polysémiques">Mots polysémiques<a href="#mots-polysémiques" class="hash-link" aria-label="Direct link to Mots polysémiques" title="Direct link to Mots polysémiques">​</a></h4>
<p>Les mots polysémiques sont des mots qui ont plusieurs significations ou interprétations. La polysémie se produit lorsque le même mot a plusieurs sens liés, mais distincts, souvent en raison de l&#x27;évolution des significations au fil du temps. Un exemple classique est le mot &quot;feuille&quot;, qui peut désigner la partie d&#x27;une plante, mais aussi une page de papier.</p>
<p>Les langues naturelles sont pleines de mots polysémiques, et leur signification précise dans une phrase dépend généralement du contexte. Par exemple, le mot &quot;banc&quot; peut se référer à un siège long où plusieurs personnes peuvent s&#x27;asseoir, ou il peut désigner une entité financière (une banque).</p>
<p>La polysémie représente un défi de taille dans le traitement du langage naturel (NLP), car il faut souvent comprendre le contexte dans lequel le mot est utilisé pour déterminer sa signification précise. Les modèles de langue tels que BERT et ses variantes s&#x27;efforcent de contextualiser les mots polysémiques pour capturer le bon sens en fonction de leur environnement immédiat dans le texte.</p>
<p>Voir page 135.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="cadrage-verbalsatellitaire">Cadrage verbal/satellitaire<a href="#cadrage-verbalsatellitaire" class="hash-link" aria-label="Direct link to Cadrage verbal/satellitaire" title="Direct link to Cadrage verbal/satellitaire">​</a></h4>
<p>Le cadrage verbal, aussi connu sous les termes de &quot;typologie du cadrage de l&#x27;événement&quot; ou &quot;framing event typology&quot;, est un concept en linguistique qui décrit comment différentes langues expriment le mouvement ou l&#x27;action et comment elles intègrent les informations de trajectoire ou de direction dans leurs verbes et adverbes ou prépositions.</p>
<p>Il y a principalement deux types de cadrage dans les langues du monde :</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="cadrage-verbal-verb-framed">Cadrage Verbal (Verb-framed)<a href="#cadrage-verbal-verb-framed" class="hash-link" aria-label="Direct link to Cadrage Verbal (Verb-framed)" title="Direct link to Cadrage Verbal (Verb-framed)">​</a></h5>
<ul>
<li><strong>Description</strong> : Dans les langues à cadrage verbal, le verbe encode généralement l&#x27;aspect du mouvement ou de l&#x27;action et la direction ou la trajectoire est exprimée par un complément ou une préposition.</li>
<li><strong>Exemples de Langues</strong> : Les langues romanes comme le français et l&#x27;espagnol sont souvent citées comme exemples. Par exemple, en français, on utilise les verbes &quot;entrer&quot; et &quot;sortir&quot; pour exprimer la direction en plus de l&#x27;action : &quot;Il entre dans la maison&quot; ou &quot;Il sort de la voiture&quot;.</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="cadrage-satellitaire-satellite-framed">Cadrage Satellitaire (Satellite-framed)<a href="#cadrage-satellitaire-satellite-framed" class="hash-link" aria-label="Direct link to Cadrage Satellitaire (Satellite-framed)" title="Direct link to Cadrage Satellitaire (Satellite-framed)">​</a></h5>
<ul>
<li><strong>Description</strong> : Dans les langues à cadrage satellitaire, le verbe exprime généralement l&#x27;action, et la direction ou la trajectoire est souvent indiquée par un élément séparé, appelé satellite, qui peut être un adverbe ou une particule.</li>
<li><strong>Exemples de Langues</strong> : L&#x27;anglais est un exemple de langue à cadrage satellitaire. Par exemple, on dira &quot;He walks in/up&quot; où &quot;walks&quot; est le verbe indiquant l&#x27;action et &quot;in&quot; ou &quot;up&quot; est le satellite indiquant la direction.</li>
</ul>
<p>Le concept de cadrage verbal vs. satellitaire a été largement diffusé par le linguiste Leonard Talmy et est utilisé pour analyser comment différentes cultures et langues conceptualisent l&#x27;espace et le mouvement. Dans votre exemple, &quot;monter&quot; et &quot;descendre&quot; seraient des exemples de cadrage verbal, car le verbe lui-même indique la direction du mouvement. En contraste, des expressions comme &quot;go up&quot; ou &quot;go down&quot; en anglais sont des exemples de cadrage satellitaire, où le verbe &quot;go&quot; indique l&#x27;action de mouvement et les mots &quot;up&quot; ou &quot;down&quot; (qui sont des satellites) indiquent la direction.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nature-dune-bonne-traduction">Nature d&#x27;une bonne traduction<a href="#nature-dune-bonne-traduction" class="hash-link" aria-label="Direct link to Nature d&#x27;une bonne traduction" title="Direct link to Nature d&#x27;une bonne traduction">​</a></h3>
<p>Les traducteurs doivent trouver un compromis entre 2 facteurs:</p>
<ul>
<li>Fidélité (Faithfulness)<!-- -->
<ul>
<li>Est-ce que le sens de la traduction correspond à celui de la phrase originale ?</li>
<li>Est-ce que la traduction apporte la même information que la phrase originale ?</li>
</ul>
</li>
<li>Fluidité (Fluency or naturalness)<!-- -->
<ul>
<li>Est-ce que la traduction est lisible et grammaticale?</li>
<li>Est-ce que le texte est de bonne qualité ?</li>
</ul>
</li>
</ul>
<p>Voir page 137.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="traduction-statistique-et-phrase-based">Traduction statistique et phrase-based<a href="#traduction-statistique-et-phrase-based" class="hash-link" aria-label="Direct link to Traduction statistique et phrase-based" title="Direct link to Traduction statistique et phrase-based">​</a></h3>
<p>La traduction automatique en traitement du langage naturel (NLP) a subi de nombreuses évolutions, avec la traduction statistique ayant été l&#x27;une des approches les plus influentes avant l&#x27;avènement des méthodes basées sur le deep learning. Voici un aperçu de la traduction statistique et de la traduction basée sur des phrases :</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="traduction-statistique-statistical-machine-translation-smt">Traduction Statistique (Statistical Machine Translation, SMT)<a href="#traduction-statistique-statistical-machine-translation-smt" class="hash-link" aria-label="Direct link to Traduction Statistique (Statistical Machine Translation, SMT)" title="Direct link to Traduction Statistique (Statistical Machine Translation, SMT)">​</a></h4>
<p>La traduction statistique est une méthode qui utilise des modèles statistiques pour traduire du texte d&#x27;une langue vers une autre. Ces modèles sont généralement entraînés sur des corpus bilingues, c&#x27;est-à-dire des ensembles de textes parallèles qui sont des traductions l&#x27;un de l&#x27;autre. La traduction statistique repose sur plusieurs composants clés :</p>
<ol>
<li><strong>Modèle de langue</strong> : Un modèle de langue est entraîné sur un corpus de la langue cible et est utilisé pour évaluer la probabilité d&#x27;une séquence de mots dans cette langue. Il aide à générer des traductions grammaticalement correctes et fluides.</li>
<li><strong>Modèle de traduction</strong> : Un modèle de traduction apprend comment les mots et les phrases sont traduits d&#x27;une langue à une autre. Il est généralement basé sur l&#x27;<strong>alignement des mots</strong> et des phrases dans un corpus parallèle.</li>
<li><strong>Décodage</strong> : Le décodeur utilise à la fois le modèle de langue et le modèle de traduction pour sélectionner la meilleure traduction possible. Il recherche dans l&#x27;espace des traductions potentielles celle qui maximise une combinaison de probabilités de traduction et de probabilités linguistiques.</li>
<li><strong>Évaluation</strong> : Des métriques comme BLEU (Bilingual Evaluation Understudy) sont utilisées pour évaluer la qualité des traductions en comparant la traduction machine avec une ou plusieurs traductions humaines de référence.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="traduction-basée-sur-des-phrases-phrase-based-translation">Traduction basée sur des phrases (Phrase-Based Translation)<a href="#traduction-basée-sur-des-phrases-phrase-based-translation" class="hash-link" aria-label="Direct link to Traduction basée sur des phrases (Phrase-Based Translation)" title="Direct link to Traduction basée sur des phrases (Phrase-Based Translation)">​</a></h4>
<p>La traduction basée sur des phrases est une extension de la traduction statistique. Au lieu de se concentrer sur les mots individuellement, cette approche prend en compte des blocs de mots ou des phrases entières. Voici comment cela fonctionne :</p>
<ol>
<li><strong>Segments de phrases</strong> : Le modèle de traduction apprend à traduire des séquences de mots (des &quot;phrases&quot; en termes de SMT, qui peuvent ne pas correspondre à des phrases grammaticales complètes) plutôt que des mots individuels. Cela permet de mieux capturer le contexte local et de traiter les expressions idiomatiques.</li>
<li><strong>Alignement de phrases</strong> : Le modèle apprend comment des blocs de texte sont alignés dans le corpus parallèle, ce qui implique des considérations de syntaxe et de sémantique qui dépassent la traduction mot à mot.</li>
<li><strong>Recombinaison</strong> : Pendant la phase de décodage, le modèle recombine les segments de phrases traduits pour former une traduction complète de la phrase entière. Cette recombinaison doit respecter la grammaire de la langue cible et produire un texte naturel.</li>
</ol>
<p>Intuitions pour faire l’<strong>alignement de phrases</strong> :</p>
<ul>
<li>Utiliser la longueur en mots ou en caractères</li>
<li>Utiliser un modèle simple de traduction</li>
<li>Utiliser des plongements multilingues</li>
<li>Aligner avec de la programmation dynamique</li>
</ul>
<p>La traduction basée sur des phrases a représenté une amélioration significative par rapport à la traduction mot à mot, mais elle a ses limites, notamment la difficulté à gérer les dépendances à longue distance et les subtilités contextuelles plus larges. Avec l&#x27;avènement des modèles de traduction automatique neurale (NMT), tels que les modèles basés sur les Transformers, la traduction automatique a atteint des niveaux de qualité sans précédent, bien que les principes de la traduction statistique restent pertinents, en particulier pour comprendre l&#x27;évolution des méthodes de traduction automatique.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="modèles-de-traduction-dibm">Modèles de traduction d&#x27;IBM<a href="#modèles-de-traduction-dibm" class="hash-link" aria-label="Direct link to Modèles de traduction d&#x27;IBM" title="Direct link to Modèles de traduction d&#x27;IBM">​</a></h3>
<p>Les modèles de traduction d&#x27;IBM sont une série de modèles statistiques conçus pour l&#x27;alignement des mots dans le contexte de la traduction automatique statistique (SMT). Développés dans les années 1990 par l&#x27;équipe de recherche d&#x27;IBM, ces modèles ont posé les fondations de la SMT avant l&#x27;avènement des approches basées sur le deep learning. Ils sont généralement désignés par IBM Model 1, Model 2, et ainsi de suite, jusqu&#x27;à Model 5, chacun étant plus complexe et sophistiqué que le précédent.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ibm-model-1">IBM Model 1<a href="#ibm-model-1" class="hash-link" aria-label="Direct link to IBM Model 1" title="Direct link to IBM Model 1">​</a></h4>
<ul>
<li><strong>Simplicité</strong> : C&#x27;est le modèle le plus simple, supposant que chaque mot dans une phrase de la langue source peut être aligné avec n&#x27;importe quel mot dans la phrase de la langue cible avec une probabilité uniforme. Il ne tient pas compte de l&#x27;ordre des mots et se concentre uniquement sur l&#x27;apprentissage des probabilités de correspondance des mots (traduction des mots).</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ibm-model-2">IBM Model 2<a href="#ibm-model-2" class="hash-link" aria-label="Direct link to IBM Model 2" title="Direct link to IBM Model 2">​</a></h4>
<ul>
<li><strong>Introduction de l&#x27;Ordre</strong> : Pour surmonter la limitation de Model 1, Model 2 introduit un modèle d&#x27;alignement explicite qui tient compte de l&#x27;ordre des mots. Il associe une probabilité à chaque alignement potentiel en fonction de la position relative des mots dans les phrases source et cible.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ibm-model-3">IBM Model 3<a href="#ibm-model-3" class="hash-link" aria-label="Direct link to IBM Model 3" title="Direct link to IBM Model 3">​</a></h4>
<ul>
<li><strong>Ajout de Fertilité</strong> : Model 3 introduit le concept de fertilité, qui est le nombre de mots dans la langue cible qu&#x27;un mot dans la langue source est susceptible de produire. Il ajoute également des probabilités de nullité (pour les mots qui ne se traduisent pas directement) et des distorsions pour l&#x27;ordre des mots.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ibm-model-4">IBM Model 4<a href="#ibm-model-4" class="hash-link" aria-label="Direct link to IBM Model 4" title="Direct link to IBM Model 4">​</a></h4>
<ul>
<li><strong>Modélisation de l&#x27;Ordre Plus Sophistiquée</strong> : Model 4 améliore la modélisation de l&#x27;ordre des mots en tenant compte des classes de mots et en essayant de mieux gérer les langues avec des ordres de mots très différents. Il est particulièrement utile pour les langues où l&#x27;ordre des mots dans une traduction n&#x27;est pas directement aligné avec l&#x27;ordre dans la langue source.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ibm-model-5">IBM Model 5<a href="#ibm-model-5" class="hash-link" aria-label="Direct link to IBM Model 5" title="Direct link to IBM Model 5">​</a></h4>
<ul>
<li><strong>Contraintes d&#x27;Alignement</strong> : Model 5 ajoute des contraintes pour s&#x27;assurer que les alignements sont un à un, en éliminant les problèmes d&#x27;alignements multiples qui pourraient survenir dans les modèles précédents. Cela le rend plus précis mais aussi beaucoup plus complexe en termes de calcul.</li>
</ul>
<p>Chacun de ces modèles augmente en complexité et en capacité à modéliser les subtilités de la traduction entre deux langues. Les probabilités d&#x27;alignement sont généralement apprises à partir de données d&#x27;entraînement bilingues à l&#x27;aide d&#x27;un algorithme Expectation-Maximization (EM). Bien que ces modèles aient été en grande partie remplacés par des méthodes de traduction automatique neurale (NMT) plus modernes et plus efficaces, ils restent un jalon important dans l&#x27;histoire de la traduction automatique et continuent d&#x27;être un sujet d&#x27;étude pour comprendre le développement de la SMT.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="modèle-encodeur-décodeur-pour-la-traduction-page-144">Modèle encodeur-décodeur pour la traduction (page 144)<a href="#modèle-encodeur-décodeur-pour-la-traduction-page-144" class="hash-link" aria-label="Direct link to Modèle encodeur-décodeur pour la traduction (page 144)" title="Direct link to Modèle encodeur-décodeur pour la traduction (page 144)">​</a></h3>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="encodeur-décodeur-rnn">Encodeur-décodeur RNN<a href="#encodeur-décodeur-rnn" class="hash-link" aria-label="Direct link to Encodeur-décodeur RNN" title="Direct link to Encodeur-décodeur RNN">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="modèle-de-base-page-154">Modèle de base (page 154)<a href="#modèle-de-base-page-154" class="hash-link" aria-label="Direct link to Modèle de base (page 154)" title="Direct link to Modèle de base (page 154)">​</a></h4>
<p>Le diagramme que vous avez partagé illustre le concept d&#x27;un modèle de traduction automatique utilisant une architecture d&#x27;encodeur-décodeur avec des réseaux de neurones récurrents (RNN). Voici une explication de ce que représente ce diagramme et comment fonctionne ce type de modèle :</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="encodeur-716">Encodeur (.716)<a href="#encodeur-716" class="hash-link" aria-label="Direct link to Encodeur (.716)" title="Direct link to Encodeur (.716)">​</a></h5>
<ul>
<li><strong>Fonction</strong> : La partie inférieure du diagramme représente l&#x27;encodeur. Son rôle est de lire la séquence d&#x27;entrée (dans cet exemple, une phrase en anglais &quot;the green witch arrived&quot;) et de la convertir en une série de représentations vectorielles. Chaque mot est d&#x27;abord converti en vecteur via une couche d&#x27;embedding, puis traité par le RNN.</li>
<li><strong>Processus</strong> : L&#x27;encodeur RNN traite chaque mot séquentiellement, en tenant compte de l&#x27;état précédent du réseau pour maintenir une forme de contexte ou de mémoire à travers la séquence.</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="décodeur-721">Décodeur (.721)<a href="#décodeur-721" class="hash-link" aria-label="Direct link to Décodeur (.721)" title="Direct link to Décodeur (.721)">​</a></h5>
<ul>
<li><strong>Fonction</strong> : La partie supérieure représente le décodeur, qui génère la séquence de sortie (dans cet exemple, une traduction en espagnol &quot;llegó la bruja verde&quot;) mot par mot. Il commence par un état initial qui est la représentation vectorielle résultant de l&#x27;encodeur.</li>
<li><strong>Processus</strong> : Le décodeur RNN utilise également une couche d&#x27;embedding pour les mots qu&#x27;il génère. À chaque étape, il prédit le mot suivant en fonction de l&#x27;état actuel du réseau et du dernier mot généré.</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="processus-de-traduction">Processus de traduction<a href="#processus-de-traduction" class="hash-link" aria-label="Direct link to Processus de traduction" title="Direct link to Processus de traduction">​</a></h5>
<ol>
<li><strong>Début et Fin de Séquence</strong> : Les symboles <code>&lt;s&gt;</code> et <code>&lt;/s&gt;</code> sont utilisés pour indiquer le début et la fin de la séquence de sortie, respectivement.</li>
<li><strong>Prédiction</strong> : À chaque étape de décodage, un softmax est appliqué pour obtenir une distribution de probabilité sur le vocabulaire de la langue cible. Le mot avec la plus haute probabilité est choisi comme sortie à cette étape.</li>
</ol>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="fonction-de-perte-et-rétropropagation">Fonction de perte et rétropropagation<a href="#fonction-de-perte-et-rétropropagation" class="hash-link" aria-label="Direct link to Fonction de perte et rétropropagation" title="Direct link to Fonction de perte et rétropropagation">​</a></h5>
<ul>
<li><strong>Perte de Cross-Entropy</strong> : La perte (ou coût) est calculée pour chaque mot prédit en utilisant la perte de cross-entropy, qui mesure la différence entre la distribution de probabilité prédite par le modèle (<code>P(y)</code>) et la distribution réelle (le &quot;gold answer&quot;, c&#x27;est-à-dire la réponse attendue ou correcte). Pour chaque mot, la perte est <code>-log P(y)</code>, où <code>P(y)</code> est la probabilité prédite pour le mot correct.</li>
<li><strong>Perte Totale</strong> : La perte totale pour la séquence est la moyenne des pertes individuelles pour chaque mot dans la séquence de sortie.</li>
<li><strong>Rétropropagation dans le Temps (BPTT)</strong> : La méthode de &quot;Backpropagation Through Time&quot; est utilisée pour entraîner le RNN. Cela signifie que l&#x27;erreur calculée est propagée en arrière à travers les étapes de la séquence et dans le temps pour mettre à jour les poids du réseau.</li>
</ul>
<p>En résumé, le modèle encodeur-décodeur RNN est un système d&#x27;apprentissage séquentiel end-to-end pour les tâches de traduction automatique. L&#x27;encodeur crée une représentation contextuelle de la phrase source, et le décodeur génère la phrase cible un mot à la fois, en s&#x27;appuyant sur cette représentation et sur sa propre dynamique séquentielle. Le modèle est entraîné en ajustant ses paramètres pour minimiser la perte de cross-entropy sur un ensemble de données d&#x27;entraînement.</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="goulot-détranglement">Goulot d&#x27;étranglement<a href="#goulot-détranglement" class="hash-link" aria-label="Direct link to Goulot d&#x27;étranglement" title="Direct link to Goulot d&#x27;étranglement">​</a></h5>
<p>Voir acétates 155 et 156.</p>
<p>Le &quot;goulot d&#x27;étranglement&quot; dans un modèle encodeur-décodeur RNN se réfère à la limitation imposée par le fait que l&#x27;ensemble du contexte de la séquence d&#x27;entrée doit être compressé en un seul vecteur d&#x27;état fixe, généré par l&#x27;encodeur RNN, qui est ensuite utilisé par le décodeur RNN pour générer la séquence de sortie.</p>
<p>Dans un modèle RNN classique pour la traduction automatique ou d&#x27;autres tâches de génération de séquence, l&#x27;encodeur transforme la séquence d&#x27;entrée en une série de représentations cachées, et la dernière représentation cachée est censée capturer l&#x27;information de toute la séquence. Cependant, lorsque l&#x27;information doit traverser de longues distances, comme dans les phrases très longues, il peut être difficile pour cette dernière représentation cachée de conserver tous les détails pertinents. Cette difficulté est connue comme le problème de &quot;goulot d&#x27;étranglement&quot; car elle limite la quantité d&#x27;information qui peut être transmise au décodeur.</p>
<p>Le problème du goulot d&#x27;étranglement peut entraîner plusieurs problèmes, notamment :</p>
<ul>
<li><strong>Perte d&#x27;Information</strong> : Des informations cruciales peuvent être perdues si le vecteur d&#x27;état n&#x27;est pas capable de contenir tous les détails nécessaires de la séquence d&#x27;entrée.</li>
<li><strong>Difficultés avec les Longues Séquences</strong> : Les RNNs traditionnels ont du mal avec la dépendance à long terme, ce qui signifie que les mots ou les structures syntaxiques qui apparaissent au début d&#x27;une longue phrase peuvent ne pas être correctement pris en compte lors de la génération de la traduction.</li>
<li><strong>Gradient Vanishing/Exploding</strong> : Les problèmes de disparition ou d&#x27;explosion des gradients pendant la rétropropagation à travers de longues séquences peuvent aggraver le problème du goulot d&#x27;étranglement.</li>
</ul>
<p>Pour atténuer le problème du goulot d&#x27;étranglement, des architectures plus avancées ont été développées, notamment :</p>
<ul>
<li><strong>Attention Mechanism</strong> : Les mécanismes d&#x27;attention permettent au décodeur de &quot;regarder&quot; différentes parties de la séquence d&#x27;entrée à chaque étape de la génération de la séquence, ce qui aide à se concentrer sur les informations pertinentes sans être limité par un vecteur d&#x27;état fixe.</li>
<li><strong>Transformers</strong> : Le modèle Transformer utilise une approche entièrement basée sur l&#x27;attention, évitant le goulot d&#x27;étranglement en permettant à chaque mot de la séquence de sortie de dépendre directement de tous les mots de la séquence d&#x27;entrée.</li>
</ul>
<p>En éliminant le goulot d&#x27;étranglement, ces méthodes ont conduit à des améliorations significatives dans la qualité de la traduction automatique et d&#x27;autres tâches de génération de séquence.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="modèle-avec-attention">Modèle avec attention<a href="#modèle-avec-attention" class="hash-link" aria-label="Direct link to Modèle avec attention" title="Direct link to Modèle avec attention">​</a></h4>
<p>Voici une explication du mécanisme d&#x27;attention dans les modèles encodeur-décodeur RNN de manière simplifiée.</p>
<p>Imaginons que vous écoutiez une conversation dans une salle bruyante. Pour comprendre ce que dit une personne en particulier, vous devez &quot;atténuer&quot; les autres bruits et vous concentrer sur sa voix. C&#x27;est en quelque sorte ce que fait le mécanisme d&#x27;attention dans un modèle RNN.</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="fonctionnement-de-base">Fonctionnement de Base<a href="#fonctionnement-de-base" class="hash-link" aria-label="Direct link to Fonctionnement de Base" title="Direct link to Fonctionnement de Base">​</a></h5>
<p>Les RNNs sont des réseaux de neurones conçus pour traiter des séquences de données, comme des phrases dans une langue. Un modèle encodeur-décodeur RNN utilise deux RNNs pour réaliser des tâches telles que la traduction automatique : l&#x27;encodeur lit la phrase source et le décodeur génère la phrase cible.</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="problème-du-goulot-détranglement">Problème du goulot d’étranglement<a href="#problème-du-goulot-détranglement" class="hash-link" aria-label="Direct link to Problème du goulot d’étranglement" title="Direct link to Problème du goulot d’étranglement">​</a></h5>
<p>Sans attention, l&#x27;encodeur doit compresser toutes les informations de la phrase source dans un seul vecteur d&#x27;état final, et le décodeur doit l&#x27;utiliser pour générer toute la phrase cible. Pour de longues phrases, cela peut être difficile, car certaines informations peuvent se perdre — c&#x27;est le problème du goulot d&#x27;étranglement.</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="mécanisme-dattention-1">Mécanisme d&#x27;Attention<a href="#mécanisme-dattention-1" class="hash-link" aria-label="Direct link to Mécanisme d&#x27;Attention" title="Direct link to Mécanisme d&#x27;Attention">​</a></h5>
<p>Le mécanisme d&#x27;attention résout ce problème en permettant au décodeur de &quot;regarder&quot; toutes les parties de la phrase source à chaque étape de la génération de la phrase cible :</p>
<ul>
<li><strong>Vecteurs d&#x27;attention</strong> : Pendant que l&#x27;encodeur traite la phrase source, il produit une série de vecteurs cachés, un pour chaque mot ou token. Ces vecteurs contiennent des informations sur chaque mot et son contexte dans la phrase.</li>
<li><strong>Scores d&#x27;attention</strong> : Lors de la génération de chaque mot de la phrase cible, le décodeur calcule un ensemble de scores d&#x27;attention. Ces scores déterminent combien d&#x27;attention (c&#x27;est-à-dire d&#x27;importance) doit être accordée à chaque vecteur caché de l&#x27;encodeur.</li>
<li><strong>Contexte</strong> : Les scores d&#x27;attention sont utilisés pour créer un vecteur de contexte pondéré. Ce vecteur est une combinaison des vecteurs cachés de l&#x27;encodeur, pondérée par les scores d&#x27;attention. En gros, cela permet au décodeur de se concentrer plus sur les parties pertinentes de la phrase source.</li>
<li><strong>Génération de mots</strong> : Le décodeur utilise ensuite le vecteur de contexte pour générer le prochain mot de la phrase cible. Comme il utilise des informations contextuelles fraîches pour chaque nouveau mot, il peut mieux traduire ou générer du texte.</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="avantages">Avantages<a href="#avantages" class="hash-link" aria-label="Direct link to Avantages" title="Direct link to Avantages">​</a></h5>
<p>Le mécanisme d&#x27;attention permet donc une meilleure gestion des longues séquences et une meilleure traduction des phrases, car il prend en compte de manière dynamique l&#x27;ensemble de la phrase source tout au long du processus de traduction, plutôt que de se fier uniquement à un vecteur d&#x27;état fixe.</p>
<p>C&#x27;est une innovation majeure qui a contribué à l&#x27;amélioration significative des performances des systèmes de traduction automatique et d&#x27;autres tâches de génération de texte en NLP.</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="formules-mathématiques">Formules mathématiques<a href="#formules-mathématiques" class="hash-link" aria-label="Direct link to Formules mathématiques" title="Direct link to Formules mathématiques">​</a></h5>
<p>Dans un modèle d&#x27;attention, les scores d&#x27;attention sont généralement calculés à l&#x27;aide de la formule suivante :</p>
<p>Étape 1. <strong>Calcul des scores d&#x27;attention</strong> :</p>
<p>Il existe au moins trois méthodes pour calculer les scores d&#x27;attention :</p>
<ul>
<li>Produit scalaire</li>
<li>Fonction bilinéaire</li>
<li>MLP</li>
</ul>
<h6 class="anchor anchorWithStickyNavbar_LWe7" id="produit-scalaire">Produit scalaire<a href="#produit-scalaire" class="hash-link" aria-label="Direct link to Produit scalaire" title="Direct link to Produit scalaire">​</a></h6>
<p>Dans cette méthode, le score d&#x27;attention est calculé comme le produit scalaire des vecteurs de l&#x27;état caché du décodeur et de l&#x27;état caché de l&#x27;encodeur :
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msubsup><mi>s</mi><mi>t</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">score(s_t, h_i) = s_t^\top h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">score</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0961em;vertical-align:-0.247em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>
où <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">s_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> est l&#x27;état caché du décodeur à l&#x27;étape <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span>, et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> est l&#x27;état caché de l&#x27;encodeur pour le <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span>-ème mot d&#x27;entrée.</p>
<p>Après le calcul des scores, un softmax est généralement appliqué pour obtenir des poids d&#x27;attention normalisés qui sont ensuite utilisés pour calculer le vecteur de contexte.</p>
<h6 class="anchor anchorWithStickyNavbar_LWe7" id="fonction-bilinéaire">Fonction bilinéaire<a href="#fonction-bilinéaire" class="hash-link" aria-label="Direct link to Fonction bilinéaire" title="Direct link to Fonction bilinéaire">​</a></h6>
<p>La méthode bilinéaire introduit une matrice de poids <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span> dans le calcul du produit scalaire, permettant une interaction plus riche entre les états cachés :
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msubsup><mi>s</mi><mi>t</mi><mi mathvariant="normal">⊤</mi></msubsup><mi>W</mi><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">score(s_t, h_i) = s_t^\top W h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">score</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0961em;vertical-align:-0.247em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>.</p>
<p>Dans ce cas, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span> est une matrice apprenable qui transforme l&#x27;espace dans lequel le produit scalaire est calculé.</p>
<p>Après le calcul des scores, un softmax est généralement appliqué pour obtenir des poids d&#x27;attention normalisés qui sont ensuite utilisés pour calculer le vecteur de contexte.</p>
<h6 class="anchor anchorWithStickyNavbar_LWe7" id="mlp-perceptron-multicouche">MLP (perceptron multicouche)<a href="#mlp-perceptron-multicouche" class="hash-link" aria-label="Direct link to MLP (perceptron multicouche)" title="Direct link to MLP (perceptron multicouche)">​</a></h6>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msup><mi>v</mi><mi>T</mi></msup><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>W</mi><mn>1</mn></msub><msub><mi>h</mi><mi>i</mi></msub><mo>+</mo><msub><mi>W</mi><mn>2</mn></msub><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">score(s_t, h_i) = v^T \tanh(W_1 h_i + W_2 s_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">score</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>
<p>Ici, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">s_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> est l&#x27;état caché actuel du décodeur au temps <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span>, et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> est le <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span>-ème état caché de l&#x27;encodeur. Les matrices <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">W_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">W_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> sont des poids appris, et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span> est un vecteur de paramètres également appris.</p>
<p>Étape 2. <strong>Application de softmax pour normaliser les scores</strong> :</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>x</mi></msub></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\alpha_{t,i} = \frac{\exp(score(s_t, h_i))}{\sum_{j=1}^{T_x} \exp(score(s_t, h_j))}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.734em;vertical-align:-1.307em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.1288em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">score</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">score</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.307em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<p>Les <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{t,i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span> sont les poids d&#x27;attention normalisés (score d&#x27;attention) qui indiquent l&#x27;importance relative de chaque état caché de l&#x27;encodeur par rapport à l&#x27;état actuel du décodeur.</p>
<p>Étape 3. <strong>Calcul du vecteur de contexte</strong> :</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>c</mi><mi>t</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>x</mi></msub></munderover><msub><mi>α</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c_t = \sum_{i=1}^{T_x} \alpha_{t,i} h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.1171em;vertical-align:-1.2777em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8394em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3111em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>
<p>Le vecteur de contexte <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">c_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> est une somme pondérée des états cachés de l&#x27;encodeur, avec les poids étant les scores d&#x27;attention normalisés.</p>
<p>Étape 4. <strong>Génération du prochain état du décodeur et du mot de sortie</strong> :</p>
<ul>
<li>L&#x27;état suivant du décodeur <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">s_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span></span> est généralement calculé en fonction de l&#x27;état actuel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">s_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>, du vecteur de contexte <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">c_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>, et du dernier mot de sortie.</li>
<li>Pour générer le mot de sortie, le modèle peut utiliser un softmax sur l&#x27;état du décodeur pour obtenir une distribution de probabilité sur tous les mots possibles du vocabulaire.</li>
</ul>
<p>Ces étapes constituent le cœur du mécanisme d&#x27;attention dans les modèles RNN encodeur-décodeur et sont utilisées pour focaliser le décodeur sur les parties pertinentes de la séquence d&#x27;entrée lors de la génération de la séquence de sortie.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="évaluation-automatique">Évaluation automatique<a href="#évaluation-automatique" class="hash-link" aria-label="Direct link to Évaluation automatique" title="Direct link to Évaluation automatique">​</a></h2>
<ul>
<li>On utilise des traductions de référence fournies par des humains.</li>
<li>On compare les traductions proposées par le système avec les traductions de référence.</li>
<li>L’évaluation est basée sur la similarité avec les traductions de référence.</li>
<li>Différentes mesures :<!-- -->
<ul>
<li>BLEU</li>
<li>METEOR</li>
<li>chrF</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bleu">BLEU<a href="#bleu" class="hash-link" aria-label="Direct link to BLEU" title="Direct link to BLEU">​</a></h3>
<ul>
<li>Détermine le nombre de n-grammes de différentes tailles que:<!-- -->
<ul>
<li>le résultat de la traduction a en commun avec les traductions de référence.</li>
</ul>
</li>
<li>Calcule une mesure de précision modifiée à partir des n-grammes dans la traduction.</li>
<li>Voir page 194 pour un exemple de calcul de BLEU.</li>
</ul>
<p>Exemple de calcul de la mesure BLEU</p>
<ul>
<li><strong>Traduction candidate</strong>: &quot;Mary did not give a smack to a green witch&quot;</li>
<li><strong>Traductions de référence</strong>:<!-- -->
<ul>
<li><strong>Ref1</strong>: &quot;Mary did not slap the green witch&quot;</li>
<li><strong>Ref2</strong>: &quot;Mary did not smack the green witch&quot;</li>
<li><strong>Ref3</strong>: &quot;Mary did not hit a green sorceress&quot;</li>
</ul>
</li>
<li>Étape 1: Prétraitement: Toutes les phrases sont déjà segmentées en mots, et la normalisation n&#x27;est pas nécessaire dans ce cas.</li>
<li>Étape 2: Calcul des N-grammes: Nous allons considérer les unigrammes (N=1), bigrammes (N=2), trigrammes (N=3) et quadrigrammes (N=4).</li>
<li>Étape 3: Calcul de la Précision des N-grammes: Pour chaque n-gramme dans la traduction candidate, nous allons vérifier s&#x27;il apparaît dans les traductions de référence.</li>
<li>Étape 4: <strong>Pas appliqué</strong> Pénalité de Brièveté (BP): Nous calculons la longueur de la traduction candidate et la longueur de la traduction de référence la plus proche en termes de longueur.</li>
<li>Étape 5: Calcul du Score BLEU: Nous combinons les précisions des n-grammes et appliquons la pénalité de brièveté pour obtenir le score BLEU.</li>
</ul>
<p>Le calcul a donné un score BLEU de 0.0. Cela peut sembler surprenant, mais cela est dû à la façon dont le score BLEU est calculé, surtout pour les trigrammes et quadrigrammes. Si la traduction candidate ne contient aucun trigramme ou quadrigramme qui correspond exactement à ceux dans les traductions de référence, la précision pour ces n-grammes sera de 0, entraînant un score BLEU global de 0.</p>
<p>Dans notre cas, bien que la traduction candidate ait des unigrammes et des bigrammes en commun avec les traductions de référence, il semble qu&#x27;il n&#x27;y ait pas de correspondance exacte pour les trigrammes et quadrigrammes, ce qui conduit à ce résultat.</p>
<p>Cet exemple illustre une des critiques de la mesure BLEU : elle peut être très stricte, surtout avec des phrases plus longues et des structures de phrases variées. Il est important de prendre en compte ces limites lors de l&#x27;utilisation du score BLEU pour évaluer la qualité des traductions.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> nltk</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> nltk</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">translate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">bleu_score </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> sentence_bleu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> SmoothingFunction</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Traduction candidate</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">candidate </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Mary did not give a smack to a green witch&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">split</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Traductions de référence</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">references </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Mary did not slap the green witch&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">split</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Mary did not smack the green witch&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">split</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Mary did not hit a green sorceress&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">split</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Définir les poids pour les unigrammes uniquement</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">weights </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;unigramme&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;bigramme&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;trigramme&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;quadrigramme&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;N=2&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0.5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0.5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;N=4&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0.25</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0.25</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0.25</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0.25</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Calculer et afficher le score BLEU pour chaque combinaison de poids</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> weight </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> weights</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">items</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    score </span><span class="token operator">=</span><span class="token plain"> sentence_bleu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">references</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> candidate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> weights</span><span class="token operator">=</span><span class="token plain">weight</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">print</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">f&quot;Score BLEU (</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation">name</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">, sans pénalité de brièveté): </span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation">score</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token string-interpolation interpolation format-spec">.4f</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Le code précédent retourne ceci.</p>
<div class="language-txt codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-txt codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Score BLEU (unigramme, sans pénalité de brièveté): 0.7000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Score BLEU (bigramme, sans pénalité de brièveté): 0.4444</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Score BLEU (trigramme, sans pénalité de brièveté): 0.1250</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Score BLEU (quadrigramme, sans pénalité de brièveté): 0.0000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Score BLEU (N=2, sans pénalité de brièveté): 0.5578</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Score BLEU (N=4, sans pénalité de brièveté): 0.0000</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="systèmes-question-réponse--recherche-dinformation-voir-page-204">Systèmes question-réponse  (Recherche d&#x27;information) Voir page 204<a href="#systèmes-question-réponse--recherche-dinformation-voir-page-204" class="hash-link" aria-label="Direct link to Systèmes question-réponse  (Recherche d&#x27;information) Voir page 204" title="Direct link to Systèmes question-réponse  (Recherche d&#x27;information) Voir page 204">​</a></h2>
<ul>
<li>Recherche d&#x27;information: poids de mots et scores de documents, prétraitement, index inversé, modèle booléen, modèle vectoriel, modèle neuronal, évaluation</li>
<li>Sélection de réponses: Reading comprehension  avec un modèle span-based, avec un modèle de langue, à partir de connaissances, évaluation.</li>
<li>Extraction de relations</li>
<li>Relation vs. entités nommées</li>
<li>Approches pour l&#x27;extraction de relations.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="travail-pratique-2">Travail pratique #2<a href="#travail-pratique-2" class="hash-link" aria-label="Direct link to Travail pratique #2" title="Direct link to Travail pratique #2">​</a></h2>
<ul>
<li>Voir mes notebooks /Users/alain/Documents/Learning/2023A-IFT-7022/TP-2/officiel</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/university/ift-7022/exam-final.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/courses/university/ift-7022/exam-intra"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Examen de mi-session</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/courses/university/glo-7030"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">GLO-7030 Apprentissage par réseaux de neurones profonds</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#lectures" class="table-of-contents__link toc-highlight">Lectures</a></li><li><a href="#contenu-de-lexamen" class="table-of-contents__link toc-highlight">Contenu de l&#39;examen</a></li><li><a href="#deep-nlp---retour-sur-les-plongements-préentraînés-de-mots" class="table-of-contents__link toc-highlight">Deep NLP - Retour sur les plongements préentraînés de mots</a></li><li><a href="#word2vec-glove-fasttext-embeddings-de-spacy" class="table-of-contents__link toc-highlight">Word2Vec, Glove, FastText, embeddings de Spacy</a><ul><li><a href="#1-word2vec" class="table-of-contents__link toc-highlight">1. Word2Vec</a></li><li><a href="#2-glove-global-vectors-for-word-representation" class="table-of-contents__link toc-highlight">2. GloVe (Global Vectors for Word Representation)</a></li><li><a href="#3-fasttext" class="table-of-contents__link toc-highlight">3. FastText</a></li><li><a href="#4-embeddings-de-spacy" class="table-of-contents__link toc-highlight">4. Embeddings de spaCy</a></li><li><a href="#comparaisons-clés" class="table-of-contents__link toc-highlight">Comparaisons clés</a></li><li><a href="#rnn-modèles-récurrents" class="table-of-contents__link toc-highlight">RNN (modèles récurrents)</a></li><li><a href="#lstm-modèle-bidirectionnel-forces-lacunes-utilisation-pour-des-applications-nlp" class="table-of-contents__link toc-highlight">LSTM, modèle bidirectionnel, forces, lacunes, utilisation pour des applications NLP</a></li></ul></li><li><a href="#modèle-transformer" class="table-of-contents__link toc-highlight">Modèle transformer</a><ul><li><a href="#mécanisme-dattention" class="table-of-contents__link toc-highlight">Mécanisme d&#39;attention</a><ul><li><a href="#mécanisme-dattention-multi-têtes" class="table-of-contents__link toc-highlight">Mécanisme d&#39;attention multi-têtes</a></li><li><a href="#impact-dans-les-modèles-transformers" class="table-of-contents__link toc-highlight">Impact dans les modèles transformers</a></li></ul></li><li><a href="#calcul-dattention-dans-un-transformer" class="table-of-contents__link toc-highlight">Calcul d&#39;attention dans un transformer</a><ul><li><a href="#éléments-de-base" class="table-of-contents__link toc-highlight">Éléments de base</a></li><li><a href="#calcul-de-lattention-voir-page-182" class="table-of-contents__link toc-highlight">Calcul de l&#39;attention (voir page 182)</a></li><li><a href="#résumé" class="table-of-contents__link toc-highlight">Résumé</a></li></ul></li><li><a href="#bloc-dattention" class="table-of-contents__link toc-highlight">Bloc d&#39;attention</a><ul><li><a href="#composantes-dun-bloc-dattention" class="table-of-contents__link toc-highlight">Composantes d&#39;un bloc d&#39;attention</a></li><li><a href="#fonctionnement-dun-bloc-dattention" class="table-of-contents__link toc-highlight">Fonctionnement d&#39;un bloc d&#39;attention</a></li><li><a href="#importance-dans-les-transformers" class="table-of-contents__link toc-highlight">Importance dans les transformers</a></li></ul></li><li><a href="#embeddings" class="table-of-contents__link toc-highlight">Embeddings</a><ul><li><a href="#rôle-des-embeddings" class="table-of-contents__link toc-highlight">Rôle des Embeddings</a></li><li><a href="#types-dembeddings-dans-un-transformer" class="table-of-contents__link toc-highlight">Types d&#39;embeddings dans un transformer</a></li><li><a href="#importance-dans-les-transformers-252" class="table-of-contents__link toc-highlight">Importance dans les transformers (.252)</a></li></ul></li><li><a href="#connexion-résiduelle" class="table-of-contents__link toc-highlight">Connexion résiduelle</a></li><li><a href="#normalisation-de-couche" class="table-of-contents__link toc-highlight">Normalisation de couche</a><ul><li><a href="#quest-ce-que-la-normalisation-de-couche-" class="table-of-contents__link toc-highlight">Qu&#39;est-ce que la normalisation de couche ?</a></li><li><a href="#formule-de-la-normalisation-de-couche" class="table-of-contents__link toc-highlight">Formule de la normalisation de couche</a></li><li><a href="#rôle-dans-les-transformers" class="table-of-contents__link toc-highlight">Rôle dans les transformers</a></li></ul></li><li><a href="#modèle-causal-vs-bidirectionnel" class="table-of-contents__link toc-highlight">Modèle causal vs. bidirectionnel</a><ul><li><a href="#modèles-causaux" class="table-of-contents__link toc-highlight">Modèles causaux</a></li><li><a href="#modèles-bidirectionnels" class="table-of-contents__link toc-highlight">Modèles bidirectionnels</a></li><li><a href="#comparaison-et-implications" class="table-of-contents__link toc-highlight">Comparaison et implications</a></li></ul></li><li><a href="#forces-et-lacunes-dans-les-applications-nlp" class="table-of-contents__link toc-highlight">Forces et lacunes dans les applications NLP</a><ul><li><a href="#forces-des-transformers-en-nlp" class="table-of-contents__link toc-highlight">Forces des Transformers en NLP</a></li><li><a href="#lacunes-des-transformers-en-nlp" class="table-of-contents__link toc-highlight">Lacunes des Transformers en NLP</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#transformers---plongements-contextuels" class="table-of-contents__link toc-highlight">Transformers - Plongements contextuels</a></li><li><a href="#bert" class="table-of-contents__link toc-highlight">BERT</a><ul><li><a href="#transformers-encodeurs" class="table-of-contents__link toc-highlight">Transformers encodeurs</a></li><li><a href="#plongements-contextuels" class="table-of-contents__link toc-highlight">Plongements contextuels</a></li><li><a href="#liens-et-implications" class="table-of-contents__link toc-highlight">Liens et implications</a></li><li><a href="#bert-comme-modèle-de-langue-préentraîné" class="table-of-contents__link toc-highlight">BERT comme modèle de langue préentraîné</a></li><li><a href="#impact-de-bert-sur-les-modèles-de-langue-préentraînés" class="table-of-contents__link toc-highlight">Impact de BERT sur les modèles de langue préentraînés</a></li><li><a href="#implications-pour-le-domaine-du-nlp" class="table-of-contents__link toc-highlight">Implications pour le domaine du NLP</a></li></ul></li><li><a href="#quest-ce-que-mnli-ner-squad" class="table-of-contents__link toc-highlight">Qu&#39;est-ce que MNLI, NER, SQuAD?</a><ul><li><a href="#1-mnli-multi-genre-natural-language-inference" class="table-of-contents__link toc-highlight">1. MNLI (Multi-Genre Natural Language Inference)</a></li><li><a href="#2-ner-named-entity-recognition" class="table-of-contents__link toc-highlight">2. NER (Named Entity Recognition)</a></li><li><a href="#3-squad-stanford-question-answering-dataset" class="table-of-contents__link toc-highlight">3. SQuAD (Stanford Question Answering Dataset)</a></li></ul></li><li><a href="#bert-et-aspects-pratiques" class="table-of-contents__link toc-highlight">BERT et aspects pratiques</a><ul><li><a href="#tokenisation-wordpiece" class="table-of-contents__link toc-highlight">Tokenisation WordPiece</a></li><li><a href="#embeddings-431" class="table-of-contents__link toc-highlight">Embeddings (.431)</a></li><li><a href="#entraînement-et-fine-tuning" class="table-of-contents__link toc-highlight">Entraînement et fine-tuning</a></li><li><a href="#utilisation-pratique" class="table-of-contents__link toc-highlight">Utilisation pratique</a></li></ul></li><li><a href="#autres-modèles-dencodeurs" class="table-of-contents__link toc-highlight">Autres modèles d&#39;encodeurs</a><ul><li><a href="#1-roberta-robustly-optimized-bert-approach" class="table-of-contents__link toc-highlight">1. RoBERTa (Robustly Optimized BERT Approach)</a></li><li><a href="#2-distilbert" class="table-of-contents__link toc-highlight">2. DistilBERT</a></li><li><a href="#3-albert-a-lite-bert" class="table-of-contents__link toc-highlight">3. ALBERT (A Lite BERT)</a></li><li><a href="#4-transformer-xl" class="table-of-contents__link toc-highlight">4. Transformer-XL</a></li><li><a href="#5-ernie-enhanced-representation-through-knowledge-integration" class="table-of-contents__link toc-highlight">5. ERNIE (Enhanced Representation through kNowledge Integration)</a></li><li><a href="#6-electra" class="table-of-contents__link toc-highlight">6. ELECTRA</a></li><li><a href="#7-camembert" class="table-of-contents__link toc-highlight">7. CamemBERT</a></li><li><a href="#8-flaubert" class="table-of-contents__link toc-highlight">8. FlauBERT</a></li><li><a href="#9-xlm-cross-lingual-language-model" class="table-of-contents__link toc-highlight">9. XLM (Cross-lingual Language Model)</a></li><li><a href="#10-xlm-roberta-xlm-r" class="table-of-contents__link toc-highlight">10. XLM-Roberta (XLM-R)</a></li></ul></li><li><a href="#décodeur-vs-encodeur-pour-la-génération-de-textes" class="table-of-contents__link toc-highlight">Décodeur vs. encodeur pour la génération de textes</a><ul><li><a href="#encodeur" class="table-of-contents__link toc-highlight">Encodeur</a></li><li><a href="#décodeur" class="table-of-contents__link toc-highlight">Décodeur</a></li><li><a href="#génération-de-texte-avec-encodeurs-et-décodeurs" class="table-of-contents__link toc-highlight">Génération de texte avec encodeurs et décodeurs</a></li></ul></li><li><a href="#génération-autorégressive-sélection-des-mots-générés" class="table-of-contents__link toc-highlight">Génération autorégressive, sélection des mots générés</a><ul><li><a href="#fonctionnement-de-la-génération-autorégressive" class="table-of-contents__link toc-highlight">Fonctionnement de la génération autorégressive</a></li><li><a href="#sélection-des-mots-générés" class="table-of-contents__link toc-highlight">Sélection des mots générés</a></li><li><a href="#décodage-beam-search-page-175" class="table-of-contents__link toc-highlight">Décodage Beam Search (page 175)</a></li></ul></li></ul></li><li><a href="#traduction-automatique" class="table-of-contents__link toc-highlight">Traduction automatique</a><ul><li><a href="#caractéristiques-linguistiques" class="table-of-contents__link toc-highlight">Caractéristiques linguistiques</a><ul><li><a href="#typologie-de-lordre-des-mots-page-134" class="table-of-contents__link toc-highlight">Typologie de l&#39;ordre des mots (page 134)</a><ul><li><a href="#svo-sujet-verbe-objet" class="table-of-contents__link toc-highlight">SVO (Sujet-Verbe-Objet)</a></li><li><a href="#sov-sujet-objet-verbe" class="table-of-contents__link toc-highlight">SOV (Sujet-Objet-Verbe)</a></li><li><a href="#vso-verbe-sujet-objet" class="table-of-contents__link toc-highlight">VSO (Verbe-Sujet-Objet)</a></li><li><a href="#vos-verbe-objet-sujet" class="table-of-contents__link toc-highlight">VOS (Verbe-Objet-Sujet)</a></li><li><a href="#ovs-objet-verbe-sujet-et-osv-objet-sujet-verbe" class="table-of-contents__link toc-highlight">OVS (Objet-Verbe-Sujet) et OSV (Objet-Sujet-Verbe)</a></li></ul></li><li><a href="#position-des-mots-et-des-adjectifs" class="table-of-contents__link toc-highlight">Position des mots et des adjectifs</a></li><li><a href="#mots-polysémiques" class="table-of-contents__link toc-highlight">Mots polysémiques</a></li><li><a href="#cadrage-verbalsatellitaire" class="table-of-contents__link toc-highlight">Cadrage verbal/satellitaire</a><ul><li><a href="#cadrage-verbal-verb-framed" class="table-of-contents__link toc-highlight">Cadrage Verbal (Verb-framed)</a></li><li><a href="#cadrage-satellitaire-satellite-framed" class="table-of-contents__link toc-highlight">Cadrage Satellitaire (Satellite-framed)</a></li></ul></li></ul></li><li><a href="#nature-dune-bonne-traduction" class="table-of-contents__link toc-highlight">Nature d&#39;une bonne traduction</a></li><li><a href="#traduction-statistique-et-phrase-based" class="table-of-contents__link toc-highlight">Traduction statistique et phrase-based</a><ul><li><a href="#traduction-statistique-statistical-machine-translation-smt" class="table-of-contents__link toc-highlight">Traduction Statistique (Statistical Machine Translation, SMT)</a></li><li><a href="#traduction-basée-sur-des-phrases-phrase-based-translation" class="table-of-contents__link toc-highlight">Traduction basée sur des phrases (Phrase-Based Translation)</a></li></ul></li><li><a href="#modèles-de-traduction-dibm" class="table-of-contents__link toc-highlight">Modèles de traduction d&#39;IBM</a><ul><li><a href="#ibm-model-1" class="table-of-contents__link toc-highlight">IBM Model 1</a></li><li><a href="#ibm-model-2" class="table-of-contents__link toc-highlight">IBM Model 2</a></li><li><a href="#ibm-model-3" class="table-of-contents__link toc-highlight">IBM Model 3</a></li><li><a href="#ibm-model-4" class="table-of-contents__link toc-highlight">IBM Model 4</a></li><li><a href="#ibm-model-5" class="table-of-contents__link toc-highlight">IBM Model 5</a></li></ul></li><li><a href="#modèle-encodeur-décodeur-pour-la-traduction-page-144" class="table-of-contents__link toc-highlight">Modèle encodeur-décodeur pour la traduction (page 144)</a></li><li><a href="#encodeur-décodeur-rnn" class="table-of-contents__link toc-highlight">Encodeur-décodeur RNN</a><ul><li><a href="#modèle-de-base-page-154" class="table-of-contents__link toc-highlight">Modèle de base (page 154)</a><ul><li><a href="#encodeur-716" class="table-of-contents__link toc-highlight">Encodeur (.716)</a></li><li><a href="#décodeur-721" class="table-of-contents__link toc-highlight">Décodeur (.721)</a></li><li><a href="#processus-de-traduction" class="table-of-contents__link toc-highlight">Processus de traduction</a></li><li><a href="#fonction-de-perte-et-rétropropagation" class="table-of-contents__link toc-highlight">Fonction de perte et rétropropagation</a></li><li><a href="#goulot-détranglement" class="table-of-contents__link toc-highlight">Goulot d&#39;étranglement</a></li></ul></li><li><a href="#modèle-avec-attention" class="table-of-contents__link toc-highlight">Modèle avec attention</a><ul><li><a href="#fonctionnement-de-base" class="table-of-contents__link toc-highlight">Fonctionnement de Base</a></li><li><a href="#problème-du-goulot-détranglement" class="table-of-contents__link toc-highlight">Problème du goulot d’étranglement</a></li><li><a href="#mécanisme-dattention-1" class="table-of-contents__link toc-highlight">Mécanisme d&#39;Attention</a></li><li><a href="#avantages" class="table-of-contents__link toc-highlight">Avantages</a></li><li><a href="#formules-mathématiques" class="table-of-contents__link toc-highlight">Formules mathématiques</a></li></ul></li></ul></li></ul></li><li><a href="#évaluation-automatique" class="table-of-contents__link toc-highlight">Évaluation automatique</a><ul><li><a href="#bleu" class="table-of-contents__link toc-highlight">BLEU</a></li></ul></li><li><a href="#systèmes-question-réponse--recherche-dinformation-voir-page-204" class="table-of-contents__link toc-highlight">Systèmes question-réponse  (Recherche d&#39;information) Voir page 204</a></li><li><a href="#travail-pratique-2" class="table-of-contents__link toc-highlight">Travail pratique #2</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Alain Boisvert. Construit avec Docusaurus.</div></div></div></footer></div>
</body>
</html>