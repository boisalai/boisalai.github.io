<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-courses/university/ift-7022/week-08-partie-1" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">8 Deep NLP - Réseaux récurrents pour le traitement de séquences (RNN, GRU, LSTM) | Alain Boisvert</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://boisalai.github.io/docs/courses/university/ift-7022/week-08-partie-1"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="8 Deep NLP - Réseaux récurrents pour le traitement de séquences (RNN, GRU, LSTM) | Alain Boisvert"><meta data-rh="true" name="description" content="Références :"><meta data-rh="true" property="og:description" content="Références :"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://boisalai.github.io/docs/courses/university/ift-7022/week-08-partie-1"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/courses/university/ift-7022/week-08-partie-1" hreflang="en"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/courses/university/ift-7022/week-08-partie-1" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Alain Boisvert RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Alain Boisvert Atom Feed">



<link rel="stylesheet" href="/fonts/font-awesome/fontawesome.css">
<link rel="stylesheet" href="/fonts/font-awesome/solid.css">
<link rel="stylesheet" href="/fonts/font-awesome/regular.css">
<link rel="stylesheet" href="/fonts/font-awesome/brands.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.6bd097eb.css">
<script src="/assets/js/runtime~main.51788d0b.js" defer="defer"></script>
<script src="/assets/js/main.c00b94db.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Alain Boisvert</b></a><a class="navbar__item navbar__link" target="" href="/docs/courses/university/glo-7030">GLO-7030</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/boisalai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-github"> </a><a href="https://www.linkedin.com/in/alain-boisvert-98b058156/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-linkedin-in"> </a><a href="mailto:ay.boisvert@gmail.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-solid fa-envelope"> </a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Alain Boisvert</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/cv">Curriculum vitæ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/learning">Learning path</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/certificates">Certificates</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/courses/university/gif-7005">Courses</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/courses/university/gif-7005">Université Laval</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/courses/university/gif-7005">GIF-7005 Introduction à l&#x27;apprentissage automatique</a><button aria-label="Expand sidebar category &#x27;GIF-7005 Introduction à l&#x27;apprentissage automatique&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/docs/courses/university/ift-7022">IFT-7022 Traitement automatique de la langue naturelle</a><button aria-label="Collapse sidebar category &#x27;IFT-7022 Traitement automatique de la langue naturelle&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-01">1 Expressions régulières</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-02">2 Prétraitement de textes et distance minimale d&#x27;édition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-03">3 Modèles de langue N-grammes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-04">4 Classification de textes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-05">5 Sémantique vectorielle (représentation des mots)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-06">6 Deep NLP + Plongements de mots (word embeddings) + intro aux réseaux de neurones</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-07">7 Étiquetage de séquences - analyse grammaticale (Part of speech tagging) et reconnaissance d&#x27;entités nommées</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/courses/university/ift-7022/week-08-partie-1">8 Deep NLP - Réseaux récurrents pour le traitement de séquences (RNN, GRU, LSTM)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-08-partie-2">8 Notions de base pour les RNN, GRU et LSTM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-09">9 Deep NLP - Introduction aux modèles Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-10">10 Deep NLP - Plongements contextuels et modèles encodeurs pré-entraînés</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-11">11 Deep NLP - Traduction automatique et modèle encodeur-décodeur</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-12">12 Deep NLP - Systèmes question-réponse (QA)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/week-13">13 Deep NLP - Introduction aux LLMs, prompting et instruct tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-1">Travail pratique 1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-2">Travail pratique 2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/travail-3">Travail pratique 3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/exam-intra">Examen de mi-session</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/university/ift-7022/exam-final">Examen final</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/courses/university/glo-7030">GLO-7030 Apprentissage par réseaux de neurones profonds</a><button aria-label="Expand sidebar category &#x27;GLO-7030 Apprentissage par réseaux de neurones profonds&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/courses/deeplearning-ai/sp01-chatgpt-building-systems">DeepLearning.AI</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/courses/activeloop/rag-for-production">Activeloop</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/fine-tuning-llms">Training and fine-tuning LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/courses/python-gpu">Best practices CPU and GPU</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/models/revdisp">Interactive models</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/references/links">References</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Courses</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Université Laval</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/courses/university/ift-7022"><span itemprop="name">IFT-7022 Traitement automatique de la langue naturelle</span></a><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">8 Deep NLP - Réseaux récurrents pour le traitement de séquences (RNN, GRU, LSTM)</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>8 Deep NLP - Réseaux récurrents pour le traitement de séquences (RNN, GRU, LSTM)</h1>
<p>Références :</p>
<ul>
<li>Jurafsky &amp; Martin, 3e éd., chapitre 9.</li>
<li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener noreferrer">Understanding LSTM Networks</a></li>
<li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener noreferrer">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="applications">Applications<a href="#applications" class="hash-link" aria-label="Direct link to Applications" title="Direct link to Applications">​</a></h2>
<p>Type d&#x27;applications.</p>
<p><img loading="lazy" alt="s65" src="/assets/images/s65-4b6afa51586513643f5ae01e1886fe37.png" width="2216" height="1294" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="pos-tagging">POS tagging<a href="#pos-tagging" class="hash-link" aria-label="Direct link to POS tagging" title="Direct link to POS tagging">​</a></h2>
<ul>
<li>L&#x27;analyse grammaticale a quelques particularités intéressantes :<!-- -->
<ul>
<li>On doit choisir une étiquette pour chaque mot<!-- -->
<ul>
<li>Son étiquette grammaticale (POS tag)</li>
</ul>
</li>
<li>Le choix d&#x27;une étiquette dépend des mots et des choix précédents<!-- -->
<ul>
<li>On voudrait parfois élargir la fenêtre de contexte</li>
</ul>
</li>
<li>La longueur des phrases varie<!-- -->
<ul>
<li>Difficile de travailler avec une fenêtre de taille fixe</li>
</ul>
</li>
<li>L&#x27;ordre des mots est important</li>
</ul>
</li>
<li>On a donc besoin d&#x27;une architecture de réseau bien adaptée à ces caractéristiquee</li>
<li>Étiquetage de séquences avec des RN</li>
<li>Rappel du cours sur les CRF (MEMM)</li>
</ul>
<p><img loading="lazy" alt="s47" src="/assets/images/s47-a608742791b318d4535430abc2bcef8b.png" width="1784" height="944" class="img_ev3q"></p>
<ul>
<li>Rappel du cours sur les MEMMs et l&#x27;analyse grammaticale</li>
</ul>
<p><img loading="lazy" alt="s48" src="/assets/images/s48-80b784416c60c5db444a1c1a103f5cff.png" width="1292" height="448" class="img_ev3q"></p>
<ul>
<li>Lacunes des MLPs (MLP pour <em>Multilayer perceptron</em>) : On perd l&#x27;ordre des mots.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="réseau-récurrent---rnn">Réseau récurrent - RNN<a href="#réseau-récurrent---rnn" class="hash-link" aria-label="Direct link to Réseau récurrent - RNN" title="Direct link to Réseau récurrent - RNN">​</a></h2>
<ul>
<li><em>Recurrent Neural Network</em> (RNN)<!-- -->
<ul>
<li>On choisi une étiquette avec <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> pour chaque mot <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></li>
<li>Les <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> donnent les probabilités de chaque mot <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>.</li>
<li>Le choix dépend du mot <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> et du contexte précédent <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span></span></li>
<li>On veut garder l&#x27;ordre des mots.</li>
</ul>
</li>
<li>Post tagging et RNN</li>
<li>Dans la figure ci-dessous<!-- -->
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span></span> est une fonction d&#x27;activation.</li>
</ul>
</li>
</ul>
<p><img loading="lazy" alt="s49" src="/assets/images/s49-91ffb76f6c3fbf36ac76b64417e7d4bf.png" width="1826" height="1130" class="img_ev3q"></p>
<ul>
<li>Étiquetage (inférence) sur une phrase...<!-- -->
<ul>
<li>On copie les couches du réseau pour chaque mot d&#x27;une phrase...</li>
<li>Les poids U, W et V sont les mêmes à chaque étape. Les matrices sont répétées.</li>
</ul>
</li>
</ul>
<p><img loading="lazy" alt="s50" src="/assets/images/s50-8417aa4beba73637b3190e29dc0febaa.png" width="1420" height="806" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>RNN (Recurrent Neural Network)</div><div class="admonitionContent_BuS1"><p>Oui, je suis familier avec les réseaux de neurones récurrents (RNN). Les RNN sont une catégorie de réseaux de neurones artificiels spécialement conçus pour reconnaître des modèles dans des séquences de données, comme le texte, le son, ou les séries chronologiques. Ils sont particulièrement puissants pour les applications qui requièrent la prise en compte d&#x27;informations temporelles ou séquentielles.</p><p>Voici quelques points clés concernant les RNN :</p><ul>
<li><strong>1. Mémoire à court terme</strong> : Contrairement aux réseaux de neurones traditionnels, les RNN ont une sorte de mémoire. Ils sauvegardent des informations sur ce qu&#x27;ils ont vu précédemment en utilisant des boucles internes dans leurs couches cachées, ce qui leur permet d&#x27;intégrer des informations antérieures dans l&#x27;analyse de la donnée actuelle.</li>
<li><strong>2. Séquences</strong> : Cette caractéristique les rend très adaptés aux données séquentielles. Par exemple, ils peuvent être utilisés pour la prédiction de séries temporelles, la génération de texte, la reconnaissance vocale, et même pour la traduction automatique.</li>
<li><strong>3. Vanishing Gradient Problem</strong> : Un inconvénient majeur des RNNs traditionnels est le problème du gradient qui s&#x27;atténue (&quot;vanishing gradient&quot;), qui se produit lors de l&#x27;entraînement de réseaux de neurones profonds avec une rétropropagation à travers le temps. Les erreurs reculent de couche en couche et peuvent devenir extrêmement petites (c&#x27;est-à-dire, elles s&#x27;atténuent), ce qui rend le réseau incapable d&#x27;apprendre et de se souvenir d&#x27;informations à long terme.</li>
<li><strong>4. LSTM et GRU</strong> : Pour surmonter ce problème, des variantes de RNN ont été développées, telles que les réseaux à mémoire à long terme (Long Short-Term Memory, LSTM) et les unités récurrentes à porte (Gated Recurrent Units, GRU). Ces architectures utilisent des mécanismes de portes pour réguler les informations qui sont ajoutées ou retirées de l&#x27;état de la mémoire du réseau, permettant ainsi de préserver les dépendances à long terme dans les données.</li>
</ul><p>Les RNN, LSTM, et GRU continuent d&#x27;être des outils essentiels dans le domaine de l&#x27;apprentissage profond, en particulier pour traiter des séquences et des informations temporelles. Ils sont utilisés dans diverses applications du monde réel et continuent d&#x27;être un domaine de recherche actif.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="approches-de-pos-tagging">Approches de POS tagging<a href="#approches-de-pos-tagging" class="hash-link" aria-label="Direct link to Approches de POS tagging" title="Direct link to Approches de POS tagging">​</a></h2>
<ul>
<li>Approche 1 (vorace) : Choisir l&#x27;étiquette la plus probable à chaque étape</li>
<li>Approche 2 : Trouver la meilleure séquence d&#x27;étiquette avec Viterbi</li>
<li>Approche 3 : Réseau bidirectionnel</li>
<li>Approche 4 : Mot + séquence de caractères</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="approche-1--choisir-létiquette-la-plus-probable-à-chaque-étape">Approche 1 : Choisir l&#x27;étiquette la plus probable à chaque étape<a href="#approche-1--choisir-létiquette-la-plus-probable-à-chaque-étape" class="hash-link" aria-label="Direct link to Approche 1 : Choisir l&#x27;étiquette la plus probable à chaque étape" title="Direct link to Approche 1 : Choisir l&#x27;étiquette la plus probable à chaque étape">​</a></h2>
<ul>
<li>Utiliser un MLP (pas RNN).</li>
<li>Ne garantie pas la meilleure solution globale</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="approche-2--trouver-la-meilleure-séquence-détiquette-avec-viterbi">Approche 2 : Trouver la meilleure séquence d&#x27;étiquette avec Viterbi<a href="#approche-2--trouver-la-meilleure-séquence-détiquette-avec-viterbi" class="hash-link" aria-label="Direct link to Approche 2 : Trouver la meilleure séquence d&#x27;étiquette avec Viterbi" title="Direct link to Approche 2 : Trouver la meilleure séquence d&#x27;étiquette avec Viterbi">​</a></h2>
<ul>
<li>Viterbi...</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="pos-tagging-avec-rnn">POS tagging avec RNN<a href="#pos-tagging-avec-rnn" class="hash-link" aria-label="Direct link to POS tagging avec RNN" title="Direct link to POS tagging avec RNN">​</a></h2>
<ul>
<li>Lorsqu&#x27;on analyse un mot <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> avec un RNN :<!-- -->
<ul>
<li>État caché = contexte à gauche du mot.</li>
</ul>
</li>
</ul>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>h</mi><mi>t</mi><mrow><mi>f</mi><mi>o</mi><mi>r</mi><mi>w</mi><mi>a</mi><mi>r</mi><mi>d</mi></mrow></msubsup><mo>=</mo><mi>R</mi><mi>N</mi><msub><mi>N</mi><mrow><mi>f</mi><mi>o</mi><mi>r</mi><mi>w</mi><mi>a</mi><mi>r</mi><mi>d</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>:</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t^{forward} = RNN_{forward} (x_1:x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2128em;vertical-align:-0.2458em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.967em"><span style="top:-2.4542em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.1809em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em">f</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">or</span><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2458em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord mathnormal" style="margin-right:0.10903em">RN</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em">f</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">or</span><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>
<ul>
<li>Le contexte à droite pourrait être utile<!-- -->
<ul>
<li>Par ex. U.N. official Ekeus <strong>heads</strong> for Baghdad</li>
<li>On peut l&#x27;obtenir en traitant la séquence inverse</li>
</ul>
</li>
</ul>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>h</mi><mi>t</mi><mrow><mi>b</mi><mi>a</mi><mi>c</mi><mi>k</mi><mi>w</mi><mi>a</mi><mi>r</mi><mi>d</mi></mrow></msubsup><mo>=</mo><mi>R</mi><mi>N</mi><msub><mi>N</mi><mrow><mi>b</mi><mi>a</mi><mi>c</mi><mi>k</mi><mi>w</mi><mi>a</mi><mi>r</mi><mi>d</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>:</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t^{backward} = RNN_{backward} (x_n:x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1461em;vertical-align:-0.247em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ba</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10903em">RN</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ba</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>
<ul>
<li>L&#x27;idéal serait de combiner les deux contextes pour prendre nos décisions.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="étiquetage-avec-un-rnn">Étiquetage avec un RNN<a href="#étiquetage-avec-un-rnn" class="hash-link" aria-label="Direct link to Étiquetage avec un RNN" title="Direct link to Étiquetage avec un RNN">​</a></h2>
<p><img loading="lazy" alt="s66" src="/assets/images/s66-2a90b4731faa963a6c26bea65f221f03.png" width="2256" height="1292" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="approche-3--réseau-bidirectionnel">Approche 3 : Réseau bidirectionnel<a href="#approche-3--réseau-bidirectionnel" class="hash-link" aria-label="Direct link to Approche 3 : Réseau bidirectionnel" title="Direct link to Approche 3 : Réseau bidirectionnel">​</a></h2>
<p><img loading="lazy" alt="s51" src="/assets/images/s51-89b7289b60a2d7729fc20a14ef44eed2.png" width="1492" height="770" class="img_ev3q"></p>
<ul>
<li>Problèmes avec l&#x27;utilisation des mots<!-- -->
<ul>
<li><strong>Lexique volumineux</strong> : Des plongements pour tous les mots?</li>
<li><strong>Mots inconnus</strong> : Par ex. faute d&#x27;orthographe, noms propres, mots d&#x27;une autre langue, termes techniques...</li>
</ul>
</li>
<li>La séquence de caractères peut être utile pour certains applications<!-- -->
<ul>
<li>Par ex. identification de langue</li>
</ul>
</li>
<li>La <strong>séquence de caractères</strong> d&#x27;un mot peut être utile (comme un substitut) pour l&#x27;analyse grammaticale<!-- -->
<ul>
<li>Idée : Augmenter la représentation d&#x27;un mot avec les caractères qui le compose</li>
</ul>
</li>
</ul>
<p><img loading="lazy" alt="s67" src="/assets/images/s67-07949c8ca4e68732ff6ddd18afc90f78.png" width="2212" height="1386" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="étiquetage-avec-rnn-bidirectionnel">Étiquetage avec RNN bidirectionnel<a href="#étiquetage-avec-rnn-bidirectionnel" class="hash-link" aria-label="Direct link to Étiquetage avec RNN bidirectionnel" title="Direct link to Étiquetage avec RNN bidirectionnel">​</a></h2>
<p><img loading="lazy" alt="s68" src="/assets/images/s68-5b0b2b8c5f3965e8a74e052610a0be59.png" width="2220" height="1232" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="approche-4--mot--séquence-de-caractères">Approche 4 : Mot + séquence de caractères<a href="#approche-4--mot--séquence-de-caractères" class="hash-link" aria-label="Direct link to Approche 4 : Mot + séquence de caractères" title="Direct link to Approche 4 : Mot + séquence de caractères">​</a></h2>
<ul>
<li>On fait l&#x27;encodage de chacun des caractères.</li>
<li>On combine l&#x27;embedding du mot et l&#x27;encodage des caractères du mot.</li>
</ul>
<p><img loading="lazy" alt="s52" src="/assets/images/s52-b7307ad2a97aa2504d3235b370ac690d.png" width="1444" height="856" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="apprentissage---rnn">Apprentissage - RNN<a href="#apprentissage---rnn" class="hash-link" aria-label="Direct link to Apprentissage - RNN" title="Direct link to Apprentissage - RNN">​</a></h2>
<ul>
<li>Comment obtenir les poids d&#x27;un réseau récurrent (RNN)?</li>
<li>Tout simplement 2 étapes<!-- -->
<ul>
<li>On fait une propagation complète sur une séquence<!-- -->
<ul>
<li>Perte <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">t_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>, perte <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">t_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>, perte <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">t_3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>.</li>
</ul>
</li>
<li>On traite la séquence en direction inverse<!-- -->
<ul>
<li>On calcule les gradients et on conserve la portion à appliquer à l&#x27;étape précédente</li>
</ul>
</li>
</ul>
</li>
<li>Nous ne serons pas évalués sur les formulations mathématiques à l&#x27;examen.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="inférence">Inférence<a href="#inférence" class="hash-link" aria-label="Direct link to Inférence" title="Direct link to Inférence">​</a></h3>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>y</mi><mi>t</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>V</mi><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>h</mi><mi>t</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><mi>U</mi><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>W</mi><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
y_t &amp; = \text{softmax}(Vh_t) \\
h_t &amp; = g(Uh_{t-1} + W x_t)
\end{align*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.75em"><span style="top:-3.91em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.41em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.25em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.75em"><span style="top:-3.91em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-2.41em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.25em"><span></span></span></span></span></span></span></span></span></span></span></span>
<p><img loading="lazy" alt="s53" src="/assets/images/s53-ac53c19de55fba7d1eccfeab9e4d4462.png" width="1866" height="1122" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="modèle-de-langue-avec-rnn">Modèle de langue avec RNN<a href="#modèle-de-langue-avec-rnn" class="hash-link" aria-label="Direct link to Modèle de langue avec RNN" title="Direct link to Modèle de langue avec RNN">​</a></h2>
<ul>
<li>Estimation de probabilité ou génération</li>
</ul>
<p><img loading="lazy" alt="s54" src="/assets/images/s54-45d37831a5ebd348b638b25a792710df.png" width="1574" height="904" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="classification-de-textes---rnn">Classification de textes - RNN<a href="#classification-de-textes---rnn" class="hash-link" aria-label="Direct link to Classification de textes - RNN" title="Direct link to Classification de textes - RNN">​</a></h2>
<ul>
<li>Un texte est une séquence de mots</li>
<li>On peut utiliser un RNN<!-- -->
<ul>
<li>Pas de sortie intermédiaire</li>
<li>Encodage du texte en un seul vecteur</li>
<li>Classification du texte avec softmax</li>
</ul>
</li>
<li>Réseau bidirectionnel, pour éviter que la classification dépende trop des mots de fin de texte</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="génération-de-texte-avec-rnn">Génération de texte avec RNN<a href="#génération-de-texte-avec-rnn" class="hash-link" aria-label="Direct link to Génération de texte avec RNN" title="Direct link to Génération de texte avec RNN">​</a></h2>
<ul>
<li>On ne peut pas être bidirectionnel ici...</li>
</ul>
<p><img loading="lazy" alt="s69" src="/assets/images/s69-0282081e168d756dfa5959e287e9c0ef.png" width="2128" height="1208" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="entités-nommées---ner">Entités nommées - NER<a href="#entités-nommées---ner" class="hash-link" aria-label="Direct link to Entités nommées - NER" title="Direct link to Entités nommées - NER">​</a></h2>
<ul>
<li>Utilisation de modèles récurrents</li>
</ul>
<p><img loading="lazy" alt="s55" src="/assets/images/s55-96176c3b75103acf48a27bee284afbbb.png" width="1590" height="840" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="extension-des-réseaux-récurrents-rnns">Extension des réseaux récurrents (RNNs)<a href="#extension-des-réseaux-récurrents-rnns" class="hash-link" aria-label="Direct link to Extension des réseaux récurrents (RNNs)" title="Direct link to Extension des réseaux récurrents (RNNs)">​</a></h2>
<ul>
<li>Difficile d&#x27;entraîner des RNNs<!-- -->
<ul>
<li>Durant la rétro propagation, le signal d&#x27;erreur s&#x27;<strong>atténue rapidement</strong> : Gradient évanescent</li>
<li>On peut le perdre après quelques étapes</li>
</ul>
</li>
<li>L&#x27;information d&#x27;une unité tend à être <strong>locale</strong>
<ul>
<li>Basé principalement sur les entrées récentes</li>
</ul>
</li>
<li>Mais l&#x27;<strong>information distante</strong> est parfois importante pour certaines tâches<!-- -->
<ul>
<li>The <strong>flights</strong> the airline was cancelling <strong>were</strong> full.</li>
<li><strong>Il</strong> n&#x27;a pas tellement confiance en <strong>lui</strong>-même.</li>
</ul>
</li>
<li>Problème – les poids servent à 2 choses :<!-- -->
<ul>
<li>À prendre une décision basée sur le contexte actuel</li>
<li>À mettre à jour et à propager l&#x27;information utile pour les décisions futures</li>
</ul>
</li>
<li>Des architectures ont été proposées pour gérer ce problème<!-- -->
<ul>
<li><strong><em>Gated architecture</em></strong> (avec portes)</li>
</ul>
</li>
<li>Intuition<!-- -->
<ul>
<li>Le contexte joue le rôle d&#x27;une unité de mémoire</li>
<li>Maintenir de l&#x27;information contextuelle dans le temps</li>
<li>Gérer explicitement cette mémoire</li>
</ul>
</li>
<li>Dans la figure ci-dessous, les <em>gates</em> sont les <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="bold">z</mtext></mrow><annotation encoding="application/x-tex">\textbf{z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4444em"></span><span class="mord text"><span class="mord textbf">z</span></span></span></span></span>. Les valeurs peuvent être entre 0 et 1.</li>
</ul>
<p><img loading="lazy" alt="s56" src="/assets/images/s56-e8439834fc3d725282e74ee1d2f01392.png" width="1080" height="482" class="img_ev3q"></p>
<ul>
<li>Rappel - <em>Recurrent unit</em>
<ul>
<li>État <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> = inférence avec <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span></span> et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
<li>Les flèches de sortie :<!-- -->
<ul>
<li>vers le haut est le <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span> pos-tagging donnant la probabilité d&#x27;un mot.</li>
<li>vers la droite est le mot qui entre dans le réseau suivant.</li>
</ul>
</li>
</ul>
<p><img loading="lazy" alt="s57" src="/assets/images/s57-3e48e961d6e6dd1c047133d44aaa2ab8.png" width="1578" height="758" class="img_ev3q"></p>
<ul>
<li>L&#x27;idée est de faire une extension du réseau de neurone</li>
<li>La première extension est le <em>Gated-recurrent unit</em> (GRU)<!-- -->
<ul>
<li>L&#x27;état <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> provient en partie :<!-- -->
<ul>
<li>de l&#x27;état précédent <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span></span> (mémoire) et</li>
<li>de l&#x27;inférence au temps <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span> (input)</li>
</ul>
</li>
</ul>
</li>
<li>Les flèches de sortie :<!-- -->
<ul>
<li>vers le haut est le <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span> pos-tagging donnant la probabilité d&#x27;un mot.</li>
<li>vers la droite est le mot qui entre dans le réseau suivant.</li>
</ul>
</li>
</ul>
<p><img loading="lazy" alt="s58" src="/assets/images/s58-1e5f5bc37bef3ba6d3fcd14a917a4b20.png" width="1246" height="778" class="img_ev3q"></p>
<ul>
<li><em>Gated-recurrent unit</em> (GRU)<!-- -->
<ul>
<li>Les <em>gates</em> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span></span></span> et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span> sont :<!-- -->
<ul>
<li>Calculées dynamiquement en fonction de <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span></span> et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></li>
<li>Apprises en fonction du problème</li>
</ul>
</li>
</ul>
</li>
<li>Les avantages du GRU sont que nous avons :<!-- -->
<ul>
<li>seulement deux paramètres à déterminer <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span></span></span> et <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span> pour chacune des neurones</li>
<li>un seul signal <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> en sortie qui peut être utilisé pour prendre sa décision que comme contexte pour déterminer le mot suivant.</li>
</ul>
</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span></span> permet d&#x27;avoir des valeurs près de 0 ou 1.</li>
</ul>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>GRU (<em>Gated Recurrent Units</em>)</div><div class="admonitionContent_BuS1"><p>Bien sûr, je peux expliquer cela. Les GRU, ou &quot;Gated Recurrent Units&quot;, sont une autre forme de réseau de neurones récurrents, tout comme les LSTM, mais avec une structure différente, souvent considérée comme plus légère et plus simple.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="architecture-gru">Architecture GRU<a href="#architecture-gru" class="hash-link" aria-label="Direct link to Architecture GRU" title="Direct link to Architecture GRU">​</a></h3><ul>
<li><strong>Réinitialisation et mises à jour des portes</strong> : Au lieu des trois portes utilisées dans une cellule LSTM (entrée, oubli, et sortie), la GRU utilise seulement deux portes :<!-- -->
<ul>
<li><strong>Porte de mise à jour (Update Gate)</strong> : Cette porte décide de la quantité d&#x27;informations passées (de l&#x27;état précédent) à conserver. Elle fonctionne de manière similaire à la porte d&#x27;oubli et à la porte d&#x27;entrée dans un LSTM combinées.</li>
<li><strong>Porte de réinitialisation (Reset Gate)</strong> : Elle détermine combien de l&#x27;ancien état sera oublié (ou ignoré) pour calculer le contenu du nouvel état.</li>
</ul>
</li>
<li><strong>États cachés</strong> : Contrairement aux LSTM, qui maintiennent un état cellulaire séparé et un état caché, les GRU n&#x27;ont qu&#x27;un seul état caché. Cet état caché est une combinaison des informations de l&#x27;état précédent et des informations du pas de temps actuel, modulée par les portes de réinitialisation et de mise à jour.</li>
<li><strong>Pas d&#x27;état de cellule séparé</strong> : Les LSTM ont un mécanisme de cellule d&#x27;état qui leur permet de transporter des informations tout au long des étapes de traitement, ce qui manque dans les GRU. Les GRU, cependant, réussissent à fusionner l&#x27;état de la cellule et l&#x27;état caché en un seul concept, ce qui simplifie la structure du réseau.</li>
</ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="différences-entre-gru-et-lstm">Différences entre GRU et LSTM<a href="#différences-entre-gru-et-lstm" class="hash-link" aria-label="Direct link to Différences entre GRU et LSTM" title="Direct link to Différences entre GRU et LSTM">​</a></h3><ul>
<li><strong>Complexité</strong> : Les GRU sont généralement plus simples et donc plus rapides à entraîner que les LSTM. Cela est dû à l&#x27;absence d&#x27;état cellulaire et à l&#x27;utilisation de deux portes au lieu de trois. Cela peut les rendre plus efficaces en termes de ressources sur des problèmes où les LSTM seraient &quot;trop puissants&quot;.</li>
<li><strong>Performance</strong> : Les LSTM sont théoriquement capables de capturer des dépendances à long terme plus complexes grâce à leur structure de cellule d&#x27;état. Cependant, dans la pratique, les GRU peuvent souvent obtenir des performances comparables aux LSTM en termes de précision de la prédiction, surtout lorsque les ressources de calcul sont limitées ou que les dépendances des données sont moins complexes.</li>
<li><strong>Utilisation de la mémoire</strong> : Étant donné que les LSTM stockent plus d&#x27;informations pour chaque étape de temps grâce à leur état cellulaire séparé, ils peuvent consommer plus de mémoire, ce qui peut être problématique lors de la formation de modèles sur des appareils avec une mémoire limitée.</li>
<li><strong>Risque de surajustement</strong> : En raison de leur complexité relative, les LSTM peuvent être plus enclins au surajustement, en particulier lorsqu&#x27;ils sont appliqués à des ensembles de données plus petits. Les GRU peuvent être une meilleure option dans ces cas grâce à leur structure plus simplifiée.</li>
</ul><p>En résumé, le choix entre GRU et LSTM dépendra largement du problème spécifique que vous traitez, de la taille et de la complexité de votre jeu de données, et de vos contraintes de calcul. Dans certains cas, les avantages en termes de rapidité et d&#x27;utilisation de la mémoire des GRU peuvent l&#x27;emporter sur les capacités théoriquement supérieures des LSTM à modéliser des dépendances à long terme.</p></div></div>
<p><img loading="lazy" alt="s59" src="/assets/images/s59-57ed223ae29ffee55b31623eec5efc85.png" width="1806" height="1108" class="img_ev3q"></p>
<ul>
<li><strong><em>Long short-term memory</em></strong> (LSTM)<!-- -->
<ul>
<li>Modèle plus élaboré sur comment on départage la gestion du contexte.</li>
<li>Une mémoire long terme <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">c_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> qui agit comme un contexte.</li>
<li>3 <em>gates</em> : f (forget), i (input) et o (output)</li>
</ul>
</li>
</ul>
<p><img loading="lazy" alt="s60" src="/assets/images/s60-dae2ec40c3a0f813f3591c4ad4f64f0b.png" width="1620" height="792" class="img_ev3q"></p>
<p><img loading="lazy" alt="s63" src="/assets/images/s63-761b1af780a2a49239dc859f196f99bd.png" width="2548" height="1158" class="img_ev3q"></p>
<p><img loading="lazy" alt="s70" src="/assets/images/s70-f758cd1660fed2cace3301c29d2ca7e0.png" width="2006" height="1288" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>LSTM</div><div class="admonitionContent_BuS1"><p>Les LSTM (<em>Long short-term memory</em>) sont une forme avancée de réseaux de neurones récurrents (RNN) conçus pour éviter le
problème du gradient qui s&#x27;atténue et sont particulièrement efficaces pour traiter des séquences de données longues et
non segmentées avec des dépendances à long terme.</p><p>L&#x27;architecture LSTM introduit une structure appelée &quot;cellule LSTM&quot;, qui modifie la façon dont l&#x27;information circule à
travers le réseau. Voici les composants clés d&#x27;une cellule LSTM et leurs rôles:</p><ul>
<li><strong>Cellule d&#x27;état (State Cell)</strong>: Au cœur du LSTM se trouve la notion de cellule d&#x27;état. C&#x27;est un peu comme une voie de chemin de fer, permettant à l&#x27;information de s&#x27;écouler le long de la séquence, avec des points de contrôle pour les entrées et les sorties. Cette cellule d&#x27;état est responsable du maintien et de la mise à jour de l&#x27;état interne du réseau, ajoutant ou retirant des informations via des structures régulatrices appelées portes.</li>
<li><strong>Portes (Gates)</strong>: Les LSTM utilisent des mécanismes appelés &quot;portes&quot; pour réguler les informations qui sont retenues ou éliminées de la cellule d&#x27;état. Ces portes peuvent apprendre quels détails sont importants à retenir pour plus tard, et quels détails sont inutiles et peuvent être jetés. Les types de portes sont les suivants:<!-- -->
<ul>
<li><strong>Porte d&#x27;oubli (Forget Gate)</strong>: Décide de la quantité d&#x27;information à écarter de la cellule d&#x27;état. Elle regarde l&#x27;entrée actuelle et la sortie du pas de temps précédent, et renvoie un nombre entre 0 (éliminer complètement) et 1 (garder entièrement).</li>
<li><strong>Porte d&#x27;entrée (Input Gate)</strong>: Met à jour la cellule d&#x27;état avec de nouvelles informations. Elle modifie l&#x27;information de l&#x27;état cellulaire en ajoutant de nouvelles informations, qui sont régies par les activations de cette porte.</li>
<li><strong>Porte de sortie (Output Gate)</strong>: Décide quelle partie de la cellule d&#x27;état va constituer la sortie réelle de l&#x27;unité LSTM.</li>
</ul>
</li>
<li><strong>États cachés (Hidden States)</strong>: Ils représentent les sorties des étapes de temps du réseau. Ils sont utilisés pour la prédiction et sont également renvoyés dans le réseau pour les calculs des étapes de temps suivantes.</li>
</ul><p><img loading="lazy" alt="s62" src="/assets/images/s62-90f6f353a42ab90e720d76683ea179db.png" width="770" height="520" class="img_ev3q"></p><p>L&#x27;interaction de ces différents composants permet au LSTM de maintenir ou d&#x27;oublier des informations sur de longues périodes. Les portes régulent le flux d&#x27;informations en utilisant des opérations de multiplication de matrices et des fonctions d&#x27;activation telles que la sigmoïde, permettant au réseau de décider activement quelles informations sont pertinentes pour garder ou oublier pendant l&#x27;apprentissage. Cette régulation fine des informations, absente dans les RNN standard, est ce qui permet aux LSTM de surmonter les défis posés par les dépendances à long terme dans les séquences de données.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="rn-rnn-lstm-et-gru">RN, RNN, LSTM et GRU<a href="#rn-rnn-lstm-et-gru" class="hash-link" aria-label="Direct link to RN, RNN, LSTM et GRU" title="Direct link to RN, RNN, LSTM et GRU">​</a></h2>
<ul>
<li>Différentes unités de traitement</li>
<li>Les réseaux de neurones (RN) ne passent pas le mot précédent.</li>
<li>Les récurrents neural network (RNNs) ont de la difficulté à passer l&#x27;information sur les longues séquences de mots.</li>
<li>LSTM et GRU encapsulée des portes dans le neurone</li>
<li>LSTM est plus complexe car il comporte plus de portes.</li>
</ul>
<p><img loading="lazy" alt="s61" src="/assets/images/s61-931d2c16896cbe550e4806b2410b2396.png" width="1746" height="720" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>LSTM</div><div class="admonitionContent_BuS1"><p><strong>Expliquez comment les modèles sont utilisés pour faire la classification d’un texte.</strong></p><p>Les réseaux LSTM sont une forme spéciale de RNNs capables de capturer des dépendances à long terme
et de mémoriser des séquences sur de longues périodes. Ils sont particulièrement utiles pour traiter des séquences de données.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="classification-de-texte-avec-lstm">Classification de texte avec LSTM<a href="#classification-de-texte-avec-lstm" class="hash-link" aria-label="Direct link to Classification de texte avec LSTM" title="Direct link to Classification de texte avec LSTM">​</a></h3><p>Dans la classification de texte, l&#x27;objectif est d&#x27;attribuer une catégorie ou une étiquette à un texte donné.
Voici comment les LSTMs peuvent être utilisés pour cette tâche :</p><ul>
<li><strong>1. Prétraitement du texte</strong> : Le texte est d&#x27;abord nettoyé et transformé en une forme que le modèle peut comprendre,
généralement sous la forme de vecteurs (ce processus est connu sous le nom d&#x27;<em>embedding</em> de mots).</li>
<li><strong>2. Traitement séquentiel</strong> : Le LSTM reçoit une séquence de vecteurs (un pour chaque mot ou token) et traite cette séquence étape par étape.
À chaque étape, il met à jour son état interne avec des informations de la nouvelle entrée et de l&#x27;état précédent, capturant ainsi le contexte
des mots précédents.</li>
</ul><ol>
<li><strong>Classification</strong> : Après avoir traité la séquence entière, le LSTM produit un état final qui résume l&#x27;information de toute la séquence. Cet état est ensuite passé à travers une ou plusieurs couches (comme une couche entièrement connectée/Dense) pour effectuer la classification, souvent avec une couche softmax à la fin pour générer des probabilités de classe.</li>
</ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="unidirectionnel-vs-bidirectionnel">Unidirectionnel vs Bidirectionnel<a href="#unidirectionnel-vs-bidirectionnel" class="hash-link" aria-label="Direct link to Unidirectionnel vs Bidirectionnel" title="Direct link to Unidirectionnel vs Bidirectionnel">​</a></h3><ul>
<li>
<p><strong>LSTM Unidirectionnel</strong> : Dans un modèle unidirectionnel, le réseau parcourt la séquence dans un seul sens, de sorte que chaque état caché ne reflète que les informations des entrées précédentes. Pour la classification de texte, cela signifie que le modèle prend une décision en se basant uniquement sur le contexte précédent de chaque point de la séquence.</p>
</li>
<li>
<p><strong>LSTM Bidirectionnel</strong> : Les LSTM bidirectionnels (BiLSTM) étendent l&#x27;idée en parcourant la séquence dans les deux sens, ce qui permet à chaque état caché de refléter les informations des entrées passées et futures. Cela est accompli en utilisant deux LSTMs distincts : l&#x27;un parcourt la séquence de l&#x27;avant vers l&#x27;arrière, et l&#x27;autre de l&#x27;arrière vers l&#x27;avant. Leurs sorties sont ensuite combinées avant de passer à la couche de classification. Cette approche capture un contexte plus riche et peut conduire à de meilleures performances, en particulier sur des tâches où le contexte futur est aussi important que le contexte passé.</p>
</li>
</ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="application-à-la-classification-de-texte">Application à la classification de texte<a href="#application-à-la-classification-de-texte" class="hash-link" aria-label="Direct link to Application à la classification de texte" title="Direct link to Application à la classification de texte">​</a></h3><p>Pour la classification de texte, l&#x27;utilisation de BiLSTM peut être particulièrement avantageuse. Par exemple, dans la phrase &quot;Malgré le mauvais temps, la fête a été incroyable&quot;, comprendre que la fête a été &quot;incroyable&quot; nécessite de prendre en compte le contexte qui suit &quot;Malgré le mauvais temps&quot;. Un LSTM unidirectionnel, parcourant la phrase de gauche à droite, n&#x27;obtiendrait ce contexte qu&#x27;après avoir rencontré des mots moins pertinents. En revanche, un BiLSTM pourrait mieux saisir la tournure positive de la phrase en combinant le contexte des deux directions.</p><p>En somme, les LSTMs, en particulier les versions bidirectionnelles, sont puissants pour la classification de texte car ils peuvent saisir le sens nuancé en considérant le contexte à la fois avant et après chaque point dans la séquence de texte.</p></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/university/ift-7022/week-08-partie-1.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/courses/university/ift-7022/week-07"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">7 Étiquetage de séquences - analyse grammaticale (Part of speech tagging) et reconnaissance d&#x27;entités nommées</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/courses/university/ift-7022/week-08-partie-2"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">8 Notions de base pour les RNN, GRU et LSTM</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#applications" class="table-of-contents__link toc-highlight">Applications</a></li><li><a href="#pos-tagging" class="table-of-contents__link toc-highlight">POS tagging</a></li><li><a href="#réseau-récurrent---rnn" class="table-of-contents__link toc-highlight">Réseau récurrent - RNN</a></li><li><a href="#approches-de-pos-tagging" class="table-of-contents__link toc-highlight">Approches de POS tagging</a></li><li><a href="#approche-1--choisir-létiquette-la-plus-probable-à-chaque-étape" class="table-of-contents__link toc-highlight">Approche 1 : Choisir l&#39;étiquette la plus probable à chaque étape</a></li><li><a href="#approche-2--trouver-la-meilleure-séquence-détiquette-avec-viterbi" class="table-of-contents__link toc-highlight">Approche 2 : Trouver la meilleure séquence d&#39;étiquette avec Viterbi</a></li><li><a href="#pos-tagging-avec-rnn" class="table-of-contents__link toc-highlight">POS tagging avec RNN</a></li><li><a href="#étiquetage-avec-un-rnn" class="table-of-contents__link toc-highlight">Étiquetage avec un RNN</a></li><li><a href="#approche-3--réseau-bidirectionnel" class="table-of-contents__link toc-highlight">Approche 3 : Réseau bidirectionnel</a></li><li><a href="#étiquetage-avec-rnn-bidirectionnel" class="table-of-contents__link toc-highlight">Étiquetage avec RNN bidirectionnel</a></li><li><a href="#approche-4--mot--séquence-de-caractères" class="table-of-contents__link toc-highlight">Approche 4 : Mot + séquence de caractères</a></li><li><a href="#apprentissage---rnn" class="table-of-contents__link toc-highlight">Apprentissage - RNN</a><ul><li><a href="#inférence" class="table-of-contents__link toc-highlight">Inférence</a></li></ul></li><li><a href="#modèle-de-langue-avec-rnn" class="table-of-contents__link toc-highlight">Modèle de langue avec RNN</a></li><li><a href="#classification-de-textes---rnn" class="table-of-contents__link toc-highlight">Classification de textes - RNN</a></li><li><a href="#génération-de-texte-avec-rnn" class="table-of-contents__link toc-highlight">Génération de texte avec RNN</a></li><li><a href="#entités-nommées---ner" class="table-of-contents__link toc-highlight">Entités nommées - NER</a></li><li><a href="#extension-des-réseaux-récurrents-rnns" class="table-of-contents__link toc-highlight">Extension des réseaux récurrents (RNNs)</a><ul><li><a href="#architecture-gru" class="table-of-contents__link toc-highlight">Architecture GRU</a></li><li><a href="#différences-entre-gru-et-lstm" class="table-of-contents__link toc-highlight">Différences entre GRU et LSTM</a></li></ul></li><li><a href="#rn-rnn-lstm-et-gru" class="table-of-contents__link toc-highlight">RN, RNN, LSTM et GRU</a><ul><li><a href="#classification-de-texte-avec-lstm" class="table-of-contents__link toc-highlight">Classification de texte avec LSTM</a></li><li><a href="#unidirectionnel-vs-bidirectionnel" class="table-of-contents__link toc-highlight">Unidirectionnel vs Bidirectionnel</a></li><li><a href="#application-à-la-classification-de-texte" class="table-of-contents__link toc-highlight">Application à la classification de texte</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Alain Boisvert. Construit avec Docusaurus.</div></div></div></footer></div>
</body>
</html>