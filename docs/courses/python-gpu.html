<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-courses/python-gpu/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">Best practices CPU and GPU | Alain Boisvert</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://boisalai.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://boisalai.github.io/docs/courses/python-gpu"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Best practices CPU and GPU | Alain Boisvert"><meta data-rh="true" name="description" content="&lt;!--"><meta data-rh="true" property="og:description" content="&lt;!--"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://boisalai.github.io/docs/courses/python-gpu"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/courses/python-gpu" hreflang="en"><link data-rh="true" rel="alternate" href="https://boisalai.github.io/docs/courses/python-gpu" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Alain Boisvert RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Alain Boisvert Atom Feed">



<link rel="stylesheet" href="/fonts/font-awesome/fontawesome.css">
<link rel="stylesheet" href="/fonts/font-awesome/solid.css">
<link rel="stylesheet" href="/fonts/font-awesome/regular.css">
<link rel="stylesheet" href="/fonts/font-awesome/brands.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.e0a8b603.css">
<script src="/assets/js/runtime~main.9f2d5a51.js" defer="defer"></script>
<script src="/assets/js/main.ae67d273.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Alain Boisvert</b></a><a class="navbar__item navbar__link" target="" href="/docs/courses/university/glo-7030">GLO-7030</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/boisalai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-github"> </a><a href="https://www.linkedin.com/in/alain-boisvert-98b058156/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-brands fa-linkedin-in"> </a><a href="mailto:ay.boisvert@gmail.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-solid fa-envelope"> </a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Alain Boisvert</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/cv">Curriculum vitæ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/learning">Learning path</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/certificates">Certificates</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/courses/university/gif-7005">Courses</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/courses/university/gif-7005">Université Laval</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/courses/deeplearning-ai/sp01-chatgpt-building-systems">DeepLearning.AI</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/courses/activeloop/rag-for-production">Activeloop</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden"><a class="menu__link" tabindex="0" href="/docs/courses/fine-tuning-llms">Training and fine-tuning LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/courses/python-gpu">Best practices CPU and GPU</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/models/revdisp">Interactive models</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/references/links">References</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Courses</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Best practices CPU and GPU</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Best practices CPU and GPU</h1>
<p>Training given by Calcul Québec on November 30, 2023.</p>
<p>Over the last decade, GPUs have become an integral part of the High-Performance Computing (HPC) world. Originally designed to efficiently render images on a screen, it turns out that GPUs can also be used to speed up scientific computing workloads, sometimes by orders of magnitude!</p>
<p>They are not, however, a silver bullet, in the sense that they will not magically accelerate just <em>anything</em> that you throw at them. In many cases, using a GPU may even <em>slow down</em> your workload. In other cases, a GPU may achieve pretty unimpressive performance gains over a CPU, sometimes no gain at all. Given that GPUs are <strong>much</strong> more expensive, and much less numerous than CPUs on the Digital Research Alliance of Canada&#x27;s HPC Clusters, you would be better off using one or more CPUs instead in all of those cases.</p>
<p>In general, users should be mindful of not wasting resources on the clusters. Whenever your code requests a GPUs, but does not make a reasonable use of them, you are <strong>unnecessarily using up your group&#x27;s priority</strong>, which will impact your colleagues&#x27; ability to run their jobs! You might also be preventing other users, whose jobs actually require a GPU, from acessing these much needed resources!</p>
<p>To avoid all these negative impacts, this workshop will introduce you to a set of best practices you can follow when deciding whether or not you need a GPU. There will be many factors that you will need to consider and we will provide concrete steps for you to reason about them.</p>
<p>First, let&#x27;s spend some time looking into the inner-workings of a GPU. Knowing how they work at a very high level will help you make sense out of our first set of recommendations.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-gpu-and-when-should-i-use-one">What is a GPU and when should I use one?<a href="#what-is-a-gpu-and-when-should-i-use-one" class="hash-link" aria-label="Direct link to What is a GPU and when should I use one?" title="Direct link to What is a GPU and when should I use one?">​</a></h2>
<p>In this section, we will use Nvidia&#x27;s <a href="https://en.wikipedia.org/wiki/Ampere_(microarchitecture)" target="_blank" rel="noopener noreferrer">Ampere GPU architecture</a>  as a reference to introduce key concepts that will help you reason about what kinds of workload are best suited to run on GPUs. In general, most of these concepts will also apply more-or-less to other Nvidia GPU architectures and to GPUs designed by other vendors than Nvidia.</p>
<p>So what is a GPU anyway?</p>
<p><img loading="lazy" alt="cpu vs gpu" src="/assets/images/cpu-vs-gpu.001-0531136527a3b7b26b70f82672d3260f.png" width="1920" height="1080" class="img_ev3q"></p>
<p>Simply put, a GPU is a <a href="https://en.wikipedia.org/wiki/Massively_parallel" target="_blank" rel="noopener noreferrer">Massively Parallel Processor (MPP)</a>. While a CPU in an HPC node will typically have between 16 and 32 <strong>compute cores</strong>, a modern GPU has thousands of them. On both CPUs and GPUs, each one of these cores is a part of the chip that has the ability to perform a number of floating-point operation (Flop) per clock cycle. On modern CPUs, each core may be able to perform up to 16 Flops at a time. On GPUs, each core can perform one Flop at a time (two if we count fused operations).</p>
<p>In the diagram above, the right-hand side represents a portion of a GPU component called a <strong>Streaming Multiprocessor (SM)</strong>. Each SM is made up of 4 of these. A modern A100 GPU has <strong>108 SMs</strong>, boasting a total of 8192 cores.</p>
<p>So can we say that GPUs are <em>always faster</em> than CPUs because they can perform several thousands of Flops per clock cycle against a CPU&#x27;s up to a few hundred?</p>
<p>No. It is not that simple.</p>
<p>It turns out that CPUs and GPUs are not designed for the same kinds of tasks. Each has a set of key characteristics that makes it best suited for different computational workloads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cpu">CPU<a href="#cpu" class="hash-link" aria-label="Direct link to CPU" title="Direct link to CPU">​</a></h3>
<ul>
<li>Key Charcteristics<!-- -->
<ul>
<li>Higher Clock Frequency</li>
<li>Low count of general purpose cores</li>
<li>Lower memory bandwidth</li>
<li>Designed to minimize memory access latency</li>
<li>Branching prediction</li>
</ul>
</li>
<li>They make CPUs good at:<!-- -->
<ul>
<li>Executing sequences of instructions that require access to different areas of memory at each step</li>
<li>Switching into different tasks that require different sets of instructions fast</li>
<li>Pipelining unordered sequences of instructions over small amounts of data (Out-of-order Execution)</li>
<li>Executing sequences that branch out into different instructions depending on the input data (If-Else)</li>
<li>Parallelizing individual instructions over very small batches of data at a time (SIMD)</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpu">GPU<a href="#gpu" class="hash-link" aria-label="Direct link to GPU" title="Direct link to GPU">​</a></h3>
<ul>
<li>Key Charcteristics<!-- -->
<ul>
<li>Lower clock frequency</li>
<li>High count of specialized cores</li>
<li>Higher memory bandwidth</li>
<li>Designed to maximize amount of data fetched per memory access</li>
<li>No branching prediction</li>
</ul>
</li>
<li>They make GPUs good at:<!-- -->
<ul>
<li>Executing sequences of instructions that re-use the same area of memory at each step</li>
<li>Executing different tasks, that require the same sets of instructions, at the same time.</li>
<li>Pipelining ordered sequences of instructions over large amounts of data (In-order Execution)</li>
<li>Executing sequences that do not branch out depending on the input data.</li>
<li>Parallelizing individual instructions over relatively large batches of data at a time (SIMT)</li>
</ul>
</li>
</ul>
<p><img loading="lazy" alt="cpu-vs-cpu-bandwitdh" src="/assets/images/cpu-vs-gpu-bdwidth-5fc1d7419134b79c84e0823fab2afcf8.png" width="1153" height="499" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="first-set-of-best-practices---when-to-use-a-gpu">First set of best practices - when to use a GPU<a href="#first-set-of-best-practices---when-to-use-a-gpu" class="hash-link" aria-label="Direct link to First set of best practices - when to use a GPU" title="Direct link to First set of best practices - when to use a GPU">​</a></h3>
<p>GPUs are the best option when:</p>
<ul>
<li>
<p>What you are doing requires large volumes of data</p>
</li>
<li>
<p>What you are doing can be parallelized over thousands of &quot;workers&quot;</p>
</li>
<li>
<p>Your program does not branch out depending on its inputs</p>
</li>
<li>
<p>Your program re-uses large chunks of data often</p>
</li>
</ul>
<p>Now let&#x27;s look at a few pseudo-code examples that illustrate these ideas.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="pseudo-code-examples">Pseudo-Code Examples<a href="#pseudo-code-examples" class="hash-link" aria-label="Direct link to Pseudo-Code Examples" title="Direct link to Pseudo-Code Examples">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="example-1---embarassingly-parallel-problem">Example 1 - Embarassingly parallel problem<a href="#example-1---embarassingly-parallel-problem" class="hash-link" aria-label="Direct link to Example 1 - Embarassingly parallel problem" title="Direct link to Example 1 - Embarassingly parallel problem">​</a></h4>
<p>Show/Hide</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> i </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">range</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> constant</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Is this code best suited to run on a CPU or a GPU?</p>
<p>Answer:</p>
<p>It depends on <code>data.size</code>.</p>
<p>Assuming both <code>data</code> and <code>constant</code> are <strong>available in the GPU&#x27;s memory</strong> when this program runs, then:</p>
<ol>
<li>CPU wins for <strong>very small</strong> arrays</li>
<li>GPU wins for any thing larger than a few thousands of elements</li>
<li>Performance gap increases <strong>very quickly</strong> as array size grows.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="example-2---serial--branching-problem">Example 2 - Serial &amp; branching problem<a href="#example-2---serial--branching-problem" class="hash-link" aria-label="Direct link to Example 2 - Serial &amp; branching problem" title="Direct link to Example 2 - Serial &amp; branching problem">​</a></h4>
<p>Show/Hide</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> i </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">range</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> other_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token operator">%</span><span class="token number">2</span><span class="token plain"> </span><span class="token operator">==</span><span class="token plain"> </span><span class="token number">0</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">else</span><span class="token plain"> another_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Is this code best suited to run on a CPU or a GPU?</p>
<p>Answer:</p>
<p>The short answer here is <strong>CPU</strong>.</p>
<p>Why does the CPU win?</p>
<ol>
<li>Each step depends on the value of the previous step. This is a serial loop.</li>
<li>Assuming <code>other_data</code> and <code>another_data</code> are completely disjoint, each step fetches values from different memory regions.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="example-3---communication-bound-parallel-problem">Example 3 - Communication-bound parallel problem<a href="#example-3---communication-bound-parallel-problem" class="hash-link" aria-label="Direct link to Example 3 - Communication-bound parallel problem" title="Direct link to Example 3 - Communication-bound parallel problem">​</a></h4>
<p>Show/Hide</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> i </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">range</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> constant</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    other_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> other_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    more_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> more_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> other_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output </span><span class="token operator">=</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">sum</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">data </span><span class="token operator">+</span><span class="token plain"> other_data </span><span class="token operator">+</span><span class="token plain"> another_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Is this code best suited to run on a CPU or a GPU?</p>
<p>As in the first example, it depends on <code>data.size</code>.</p>
<p>Assuming both <code>data</code>, <code>other_data</code> and <code>more_data</code> are <strong>available in the GPU&#x27;s memory</strong> when this program runs, then:</p>
<ul>
<li>CPU wins with <strong>very small</strong> arrays</li>
<li>GPU wins for anything larger than a few thousands of elements.</li>
<li>Performance gap increases <strong>very quickly</strong> as array size grows.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="gpus-and-data-loading">GPUs and Data Loading<a href="#gpus-and-data-loading" class="hash-link" aria-label="Direct link to GPUs and Data Loading" title="Direct link to GPUs and Data Loading">​</a></h2>
<p>As previously mentioned, we are using Nvidia&#x27;s <a href="https://en.wikipedia.org/wiki/Ampere_(microarchitecture)" target="_blank" rel="noopener noreferrer">Ampere GPU architecture</a> as a stand-in for generic GPUs in this material. In this architecture, as well as in older Nvidia architectures and comptemporary architectures from other vendors, GPUs and CPUs <strong>do not share memory</strong>. That is to say, each has its own memory and one cannot access data that is stored on the other&#x27;s memory. It is also often the case in systems equipped with such GPUs that data cannot be loaded directly from disk to GPU memory. Data must first be loaded via CPU into the system&#x27;s memory, and then from there it can be loaded into the GPU&#x27;s memory - an approach known as a <em>bounce buffer</em>:</p>
<p><img loading="lazy" alt="cpu-vs-cpu-bandwitdh" src="/assets/images/bounce_buffer-7e19713f9d381783430ab02a97e0c5a7.png" width="559" height="289" class="img_ev3q"></p>
<p>As it turns out, using a bounce buffer comes with a series of negative consequences in terms of performance, which include:</p>
<ul>
<li>Latency due to multiple read/write operations, leaving the GPU&#x27;s Streaming Multiprocessors idle while waiting for data from disk.</li>
<li>CPU might multi-task between data loading and running other processes.</li>
<li>Amount of system memory must be taken into consideration.</li>
</ul>
<p>This makes <strong>loading data</strong> into the GPU&#x27;s memory a critical factor that influences the performance of a GPU program. Also worth noticing is the fact that the negative consequences apply the other way around too! If you need to copy data from GPU memory to system memory, you will also experience these performance setbacks.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="second-set-of-best-practices---data-loading">Second set of best practices - Data Loading<a href="#second-set-of-best-practices---data-loading" class="hash-link" aria-label="Direct link to Second set of best practices - Data Loading" title="Direct link to Second set of best practices - Data Loading">​</a></h3>
<p>When designing pipelines to load data into GPU memory, always try to:</p>
<ul>
<li>Load all the data you will need in one go, if possible, before starting your computations.</li>
<li>Load the maximum amount of data you can get away with if loading the whole dataset is not an option.</li>
<li>Use one or more additional CPUs to perform data loading if you must load data iteratively in batches. Avoid using the main process to load data.</li>
<li>Avoid moving data back and forth between GPU and CPU!</li>
<li>Generate random values directly on the GPU if your computatations require using them.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring--profiling---how-do-i-know-if-im-using-a-gpu-correctly">Monitoring &amp; Profiling - How do I know if I&#x27;m using a GPU correctly?<a href="#monitoring--profiling---how-do-i-know-if-im-using-a-gpu-correctly" class="hash-link" aria-label="Direct link to Monitoring &amp; Profiling - How do I know if I&#x27;m using a GPU correctly?" title="Direct link to Monitoring &amp; Profiling - How do I know if I&#x27;m using a GPU correctly?">​</a></h2>
<p>So far we have seen two sets of best practices to keep in mind when reasoning about using GPUs in your computations. They are good pointers in theory, but even if you follow them to the letter it is still possible that things will not go as expected. Thus, the best way to make sure you are getting maximum performance out of a GPU is to monitor its usage and to profile your code.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nvidia-smi">Nvidia-smi<a href="#nvidia-smi" class="hash-link" aria-label="Direct link to Nvidia-smi" title="Direct link to Nvidia-smi">​</a></h3>
<p>We will start with monitoring tools. The first thing you should always verify is whether your code is using the GPU at all. The simplest way to answer this question is with the command <code>nvidia-smi</code>:</p>
<p><img loading="lazy" alt="cpu-vs-cpu-bandwitdh" src="/assets/images/nvidia-smi-bb2d4e72a28842df6ff3b30f41a155b0.png" width="778" height="359" class="img_ev3q"></p>
<p>As you can see above, <code>nvidia-smi</code> will tell you whether or not any processes are currently using the GPU and how much GPU memory they have allocated.</p>
<p>A common way of misreading the <code>utilisation %</code> field is to take it as the amount of compute capacity currently being used. That is <strong>not</strong> what that number is. Rather, <code>utilisation %</code> is the proportion of time where there was <em>at least one kernel</em> running on the GPU. A low % means that the GPU is either idle, or busy doing other things than running kernels, such as managing memory or scheduling tasks.</p>
<p>Ideally, this % stays high for the duration of your job, but <strong>that is not sufficient</strong> to conclude that your code is not wasting GPU capacity. You could after all have a single kernel running 100% of the time using just a few cores, thereby massively wasting GPU capacity.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nvtop">Nvtop<a href="#nvtop" class="hash-link" aria-label="Direct link to Nvtop" title="Direct link to Nvtop">​</a></h3>
<p>Another way of checking whether or not your code is actually using the GPU is the command <code>nvtop</code>. This tool will display the same usage statistics as <code>nvidia-smi</code> as a line plot that changes over time. You will also get a breakdown of <code>utilisation %</code> per process if you have multiple processes sharing the same GPU:</p>
<p><img loading="lazy" alt="cpu-vs-cpu-bandwitdh" src="/assets/images/nvtop-2e1bea267464707ea292db96321596af.png" width="1285" height="402" class="img_ev3q"></p>
<p>The line plot is useful to identify peaks and cliffs in <code>utilisation %</code> as well as any changes in memory allocation during the execution of your code. This can provide helpful clues for you to optimize your code to minimize idle time for example.</p>
<p>Another interesting number provided by this tool is the <strong>CPU % utilisation</strong>. In general, CPU usage should be low in a GPU program. A high CPU utilisation % might indicate your program is not using the GPU optimally as a lot of work is being done by the CPU.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cluster-portals">Cluster Portals<a href="#cluster-portals" class="hash-link" aria-label="Direct link to Cluster Portals" title="Direct link to Cluster Portals">​</a></h3>
<p>Next we turn to a tool that stradles the line between monitoring and profiling: the Digital Research Alliance of Canada&#x27;s cluster portals:</p>
<p><img loading="lazy" alt="cpu-vs-cpu-bandwitdh" src="/assets/images/portal-7526d8307c3e5fc8015ff90824a0d69b.png" width="997" height="682" class="img_ev3q"></p>
<p>Above you can see a different set of statistics pertaining to the same code example used to illustrate <code>nvidia-smi</code> <code>nvtop</code>:</p>
<ul>
<li><strong>SM Activity:</strong> This is the average % of time over all SMs where there was at least one set of concurrent operations active, no matter how many kernels that happens to be. Note that <em>active</em> does not mean <em>computing</em>. Other activities such as waiting for data and acessing memory also count here. Ideally this number should stay over 80% for the duration of your job, but that <strong>does not necessarily mean</strong> you are using 80% of the GPUs compute capacity. It means that either 80% of the time your were using 100% of the capcity and stayed idle 20% of the time, or that you used 80% capacity 100% of the time with 20% of vacant capacity.</li>
<li><strong>SM Occupancy:</strong>  This is the average, over time, of the ratio of active concurrent operations and the maximum number of concurrent operations supported by the GPU. A high % also <strong>does not necessarily mean</strong> you are using the GPU effectively. For our purposes however, we will say that this number being high is a strong indicator of good usage.</li>
</ul>
<p>The other metrics we see above - <code>Tensor</code>, <code>FP64</code>, <code>FP32</code> and <code>FP16</code> - indicate what type of GPU core is being used and what % of time these specific parts of the card were active. Of note here is the <code>Tensor</code> metric. Tensor cores are specialized cores purpose built for speeding-up n-dimensional tensor operations like multiplications and convolutions. If your code makes heavy use of tensor operations, the Cuda compiler can automatically take advantage of these specialized cores if your program satisfies one of these conditions:</p>
<ul>
<li>You use <a href="https://en.wikipedia.org/wiki/Mixed-precision_arithmetic" target="_blank" rel="noopener noreferrer">mixed precision</a> in your program.</li>
<li>You explicitly use <a href="https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/" target="_blank" rel="noopener noreferrer">Nvidia&#x27;s TF32 data type</a> in your program.</li>
</ul>
<p>And additionally:</p>
<ul>
<li>In a matrix multiplication, all your matrices&#x27; dimensions are multiples of 8.</li>
<li>In a convolution, the number of channels and the width are all multiples of 8.</li>
</ul>
<p>A high % usage of Tensor cores should correspond to good speed-ups relative to other GPU core types. One caveat is that the first two conditions require sacrificing precision in computations, so you have to be mindful of whether or not high precision is important for your application.</p>
<p>Apart from the GPU metrics depicted above, you can also monitor a number of other job statistics on the Alliance&#x27;s cluster portals such as: CPU and system memory usage; network metrics; IO metrics, disk usage and more.</p>
<p>The alliance currently maintains cluster portals for the following clusters:</p>
<ul>
<li><a href="https://portail.beluga.calculquebec.ca" target="_blank" rel="noopener noreferrer">Béluga</a></li>
<li><a href="https://portail.narval.calculquebec.ca" target="_blank" rel="noopener noreferrer">Narval</a></li>
<li><a href="https://dashboard.graham.sharcnet.ca" target="_blank" rel="noopener noreferrer">Graham</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="third-set-of-best-practices---monitoring-and-profiling">Third set of best practices - Monitoring and Profiling<a href="#third-set-of-best-practices---monitoring-and-profiling" class="hash-link" aria-label="Direct link to Third set of best practices - Monitoring and Profiling" title="Direct link to Third set of best practices - Monitoring and Profiling">​</a></h3>
<ul>
<li>Always do a dry run of your job before requesting a GPU for long durations.</li>
<li>Make sure your program is even using a GPU in the first place.</li>
<li>Make sure your program is able to keep the GPU busy for most of the duration of your job.</li>
<li>Keep <strong>SM Activity</strong> above 80% most of the time.</li>
<li>Keep <strong>SM Occupancy</strong> above 50% most of the time.</li>
<li>Use Tensor cores when applicable.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="when-should-i-use-multiple-cpus-instead-of-a-gpu">When should I use multiple CPUs instead of a GPU?<a href="#when-should-i-use-multiple-cpus-instead-of-a-gpu" class="hash-link" aria-label="Direct link to When should I use multiple CPUs instead of a GPU?" title="Direct link to When should I use multiple CPUs instead of a GPU?">​</a></h2>
<p>Let&#x27;s assume you have a program that fits the bill of the first and second sets of best practices, but you can&#x27;t for the love of all that is good get it to keep the GPU consistently busy. Maybe you have bursts of high usage followed by long periods of no usage at all. Maybe you can&#x27;t get activity and/or occupancy above 50%.</p>
<p>In all these cases, a GPU still beats a single CPU hands down... but how does it compare to multiple CPUs?</p>
<p>While CPUs are not purpose-built to handle the type and scale of operations that a GPU does, they are still pretty good at it - especially when you can throw lots of them at the problem. In many cases, like linear algebra operators and fourier transforms, highly optimized parallel implementations of these operations for multiple CPUs have been around for ages.</p>
<p>Furthermore, unless you are writing Cuda code directly, it is often possible to run the exact same code without any changes on a GPU or one/many CPUs.</p>
<p>This, coupled with the fact that there are <strong>many, many, many</strong> more CPUs available in the Alliance&#x27;s clusters than there are GPUs, make using multiple CPUs a great alternative.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="narval">Narval<a href="#narval" class="hash-link" aria-label="Direct link to Narval" title="Direct link to Narval">​</a></h3>
<ul>
<li>CPUs: 75584</li>
<li>GPUs: 636</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="béluga">Béluga<a href="#béluga" class="hash-link" aria-label="Direct link to Béluga" title="Direct link to Béluga">​</a></h3>
<ul>
<li>CPUs: 32080</li>
<li>GPUs: 688</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cedar">Cedar<a href="#cedar" class="hash-link" aria-label="Direct link to Cedar" title="Direct link to Cedar">​</a></h3>
<ul>
<li>CPUs: 90752</li>
<li>GPUs: 1352</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="graham">Graham<a href="#graham" class="hash-link" aria-label="Direct link to Graham" title="Direct link to Graham">​</a></h3>
<ul>
<li>CPUs: 37664</li>
<li>GPUs: 536</li>
</ul>
<p>So even in cases where you <strong>can</strong> make an effective usage of a GPU, you might get, say, 16 CPUs allocated to you much faster and possibly beat the GPU&#x27;s performance depending on the nature of your program!</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="fourth-set-of-best-practices---use-multiple-cpus-instead-of-a-gpu">Fourth set of best practices - Use multiple CPUs instead of a GPU<a href="#fourth-set-of-best-practices---use-multiple-cpus-instead-of-a-gpu" class="hash-link" aria-label="Direct link to Fourth set of best practices - Use multiple CPUs instead of a GPU" title="Direct link to Fourth set of best practices - Use multiple CPUs instead of a GPU">​</a></h3>
<ul>
<li>When dry-running your GPU code, dry run it with multiple CPUs too. See if there is really an advantage to using a GPU.</li>
</ul>
<p>Now let&#x27;s go through some hands-on example to put into practice what we&#x27;ve learned so far!</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="example-1---solving-linear-systems">Example 1 - Solving Linear Systems<a href="#example-1---solving-linear-systems" class="hash-link" aria-label="Direct link to Example 1 - Solving Linear Systems" title="Direct link to Example 1 - Solving Linear Systems">​</a></h2>
<p>In this example, you will simultaneously solve N systems of equations of the type <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>x</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">Ax=b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span> is a <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>x</mi><mi>M</mi></mrow><annotation encoding="application/x-tex">MxM</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span> matrix and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span> is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">Mx1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span></p>
<p>You will be given two functions: one to generate random matrices and one to solve the systems of equations. You will use the magic function <code>%timeit</code> to compare the running times for these functions with different combinations of the following parameters:</p>
<ul>
<li>Using CPU or GPU</li>
<li>The number of CPUs to use</li>
<li>The number and size of the systems of equations to be solved</li>
</ul>
<p>You should be able to see how these parameters affect performance when using CPUs or a GPU to solve the systems.</p>
<p>For the CPU part of teh example, you will use <code>numpy</code> - a Python library that contains optimized implementations of a myriad of numerical operations. For the GPU portion, you will use <code>cupy</code> - a library that aims to be an analogous of <code>numpy</code> but for GPUs.</p>
<p>On the first cell, you will import all libraries needed for this example and you will choose the number of CPUs that should be used for your computations. The environment variable <code>OMP_NUM_THREADS</code> controls how many parallel threads should be spawned by routines written using OpenMP - an API for parallel programming. As it turns out, the low level C code that <code>numpy</code> calls to solve linear systems of equations and to generate random numbers are both examples of routines written using OpenMP!</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> timeit</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">N_CPUS</span><span class="token operator">=</span><span class="token number">10</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">os</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;OMP_NUM_THREADS&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token operator">=</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">str</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">N_CPUS</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> numpy </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> np</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> cupy </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> cp</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Next you will define functions to generate random data and to solve linear systems:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">generate_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">n_systems</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_rows</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_cols</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">device</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    lib </span><span class="token operator">=</span><span class="token plain"> cp </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> device </span><span class="token operator">==</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;gpu&quot;</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">else</span><span class="token plain"> np</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rmg </span><span class="token operator">=</span><span class="token plain"> lib</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">random</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">rand </span><span class="token comment" style="color:rgb(98, 114, 164)">#random matrix generator</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    a </span><span class="token operator">=</span><span class="token plain"> rmg</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">n_systems</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_rows</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_cols</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    b </span><span class="token operator">=</span><span class="token plain"> rmg</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">n_systems</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_rows</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">b</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">multiple_solve</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">b</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    lib </span><span class="token operator">=</span><span class="token plain"> cp </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">type</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token operator">==</span><span class="token plain"> cp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ndarray </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">else</span><span class="token plain"> np</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    results </span><span class="token operator">=</span><span class="token plain"> lib</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">linalg</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">solve</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">b</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> results</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Now try changing both the number and size of systems, as well as the device used to generate the data. The magic function <code>%timeit</code> will return the average run time of <code>generate_data</code> over several repetitions:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token operator">%</span><span class="token plain">timeit generate_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">n_systems</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> n_rows</span><span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> n_cols</span><span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> device</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;gpu&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># 3.63 ms ± 12.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token operator">%</span><span class="token plain">timeit generate_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">n_systems</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> n_rows</span><span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> n_cols</span><span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> device</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;cpu&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># 884 ms ± 21.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You have now got a sense of the time it takes to generate random systems of equations on both CPU and GPU. Now let&#x27;s store some random data in a variable so we can use it to time the execution time of our second function: <code>multiple_solve</code>. Try it out with different numbers and sizes of systems on both the CPU and the GPU. Notice how the difference in performance changes as the size of the problem grows.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">b </span><span class="token operator">=</span><span class="token plain"> generate_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">10000</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">10000</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;gpu&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token operator">%</span><span class="token plain">timeit multiple_solve</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">b</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># 257 ms ± 44.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">b </span><span class="token operator">=</span><span class="token plain"> generate_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">10000</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">10000</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;cpu&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token operator">%</span><span class="token plain">timeit multiple_solve</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">b</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># 3.71 s ± 578 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Last, but not least, let&#x27;s see what happens when we generate data using the CPU, then copy the data to GPU before calling <code>multiple solve</code>. This should illustrate the benefits of generating random numbers directly on the GPU, instead of using a <em>bounce buffer</em> to load numbers generated on the CPU:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token operator">%</span><span class="token operator">%</span><span class="token plain">timeit</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">c</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">d </span><span class="token operator">=</span><span class="token plain"> generate_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;cpu&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">c_gpu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> d_gpu </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">cp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">asarray</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">c</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">cp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">asarray</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">d</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)"># This will load c and d into the GPU&#x27;s memory</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">multiple_solve</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">c_gpu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">d_gpu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">compute_batched</span><span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="example-2---linear-regression">Example 2 - Linear Regression<a href="#example-2---linear-regression" class="hash-link" aria-label="Direct link to Example 2 - Linear Regression" title="Direct link to Example 2 - Linear Regression">​</a></h2>
<p>In this next example, you will use <code>JAX</code> to solve a linear regression problem. <code>JAX</code> is a numerical computing library that, similarly to <code>cupy</code>, aims to bring the <code>numpy</code> user experience to the world of GPUs. Unlike <code>cupy</code> however, JAX is geared towards Machine Learning research, and so it has many useful built-in features to fit models at scale - most notably <a href="https://en.wikipedia.org/wiki/Automatic_differentiation" target="_blank" rel="noopener noreferrer">automatic differentiation</a>.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Load cuda/11.4 to use JAX</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> jax</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">numpy </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> jnp</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> numpy </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> np</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> matplotlib </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> pyplot </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> plt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>First, you will define a function to generate random data for our linear regression problem. The function below takes in a mathematical function - preferably a linear function - and the number of desired samples as arguments. It then generates the desired number of samples by adding random noise to randomly chosen points along the line drawn by the mathematical function.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">generate_regression_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">function</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_samples</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    samples </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">function</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">random</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">normal</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token number">0.1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> x </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">sort</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">random</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">uniform</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_samples</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y </span><span class="token operator">=</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">zip</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token operator">*</span><span class="token plain">samples</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    X </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">array</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">dtype</span><span class="token operator">=</span><span class="token plain">jnp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)">#Change the numpy array&#x27;s data type into a JAX data type</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    X </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">c_</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ones</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">shape</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    y </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">array</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">dtype</span><span class="token operator">=</span><span class="token plain">jnp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">y</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Let&#x27;s start by generating 50 noisy samples from the function <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>2</mn><mi>x</mi><mo>+</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">y = 2x + 5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">5</span></span></span></span>:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">f </span><span class="token operator">=</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">lambda</span><span class="token plain"> x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> x </span><span class="token operator">+</span><span class="token plain"> </span><span class="token number">5</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">N_SAMPLES</span><span class="token operator">=</span><span class="token number">50</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">y </span><span class="token operator">=</span><span class="token plain"> generate_regression_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">f</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">N_SAMPLES</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In our linear regression problem, you will be given only the blue dots and will attempt to recover the red line. In other words, given some data points (the blue dots), you will run an algorithm to <strong>learn</strong> the function that best describes the data (the red line):</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">X_target </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">linspace</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">50</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">reshape</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">plt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">scatter</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">label</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;Samples&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">plt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">plot</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X_target</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">f</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X_target</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">color</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;r&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">label</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;Target&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">plt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">legend</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">loc</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;lower right&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img loading="lazy" alt="s01" src="/assets/images/s01-c34cdee85863f5ea42be88f596d30e66.png" width="1136" height="826" class="img_ev3q"></p>
<p>You will solve the linear regression using the iterative <a href="https://en.wikipedia.org/wiki/Least_squares" target="_blank" rel="noopener noreferrer">least-squares approach</a>. In the cell below, you are given a set of functions that implement this approach. In a nutshell - you will use <a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener noreferrer">gradient descent</a> for <code>n_iterations</code> in an attempt to minimize the square of the distance between the blue dots and a linear function, that is initialized as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn><mi>x</mi><mo>+</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y = 0x + 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">0</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span>.</p>
<p>Notice how this is re-using the same dataset and how it is applying the same operations during each iteration over and over again? And think about the mathematical operations used in the function below - can they be computed in a parallelized manner?</p>
<ul>
<li>Matrix multiplication of X and beta</li>
<li>Element-wise square of the element-wise difference between two vectors</li>
<li>Mean of a vector</li>
<li>A vector minus another vector multiplied by a constant</li>
</ul>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">beta</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X @ beta</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">T</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">loss</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">beta</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> jnp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">mean</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">beta</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token decorator annotation punctuation" style="color:rgb(248, 248, 242)">@jax</span><span class="token decorator annotation punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token decorator annotation punctuation" style="color:rgb(248, 248, 242)">jit</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">update</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">beta</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> learning_rate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> beta </span><span class="token operator">-</span><span class="token plain"> learning_rate </span><span class="token operator">*</span><span class="token plain"> jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">grad</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">loss</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">beta</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">fit_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> n_iterations</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> learning_rate</span><span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    beta </span><span class="token operator">=</span><span class="token plain"> jnp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">zeros</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">shape</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ndim </span><span class="token operator">&gt;</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        y </span><span class="token operator">=</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">reshape</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> i </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">range</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">n_iterations</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        beta </span><span class="token operator">=</span><span class="token plain"> update</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">beta</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> learning_rate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> beta</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>After <code>n_iterations</code> you should see the coefficients <code>beta</code> of the linear function converge to 2 and 5 - the true values of these coefficients that you had set when you first generated this random dataset:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">beta_fit </span><span class="token operator">=</span><span class="token plain"> fit_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">10000</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">print</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">f&quot;Estimated f(x) = </span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation">beta_fit</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string-interpolation interpolation number">0</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)"> * X + </span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation">beta_fit</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string-interpolation interpolation number">1</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If you plot this <strong>learned function</strong>, you will see that it is pretty close to that red line we had on the plot a couple cells above:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">plt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">scatter</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">label</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;Samples&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">plt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">plot</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X_target</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">dot</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">c_</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">X_target</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ones</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">shape</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">beta_fit</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> color</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;g&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">label</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Linear Fit&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">plt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">legend</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">loc</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;lower right&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img loading="lazy" alt="s02" src="/assets/images/s02-6e17dae2382037c97bbeea620b17452c.png" width="1104" height="836" class="img_ev3q"></p>
<p>Now let&#x27;s see what is the effect on performance when we increase the number of samples. First you will time the execution of <code>fit_model()</code> using the CPU. What happens to the execution time when the number of samples grow?</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">N_SAMPLES</span><span class="token operator">=</span><span class="token number">10000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">y </span><span class="token operator">=</span><span class="token plain"> generate_regression_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">f</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">N_SAMPLES</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)">#Try this with different values of N_SAMPLES like: 10000, 50000, 100000, 1000000...</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">default_device</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">devices</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;cpu&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token operator">%</span><span class="token plain">timeit fit_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> N_SAMPLES</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">block_until_ready</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-txt codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-txt codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CPU</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> 1000: 16.6 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">10000: 591 ms ± 27.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">GPU</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> 1000: 161 ms ± 16.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> 2.02 s ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Now let&#x27;s see how long it takes to load data into the GPU using <code>JAX</code>:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token operator">%</span><span class="token plain">timeit jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">device_put</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token operator">%</span><span class="token plain">timeit jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">device_put</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Finally, let&#x27;s compare the execution time we obtained for the CPU with the execution time of the same function on the GPU. What can you say about the difference in performance as the number of samples gorw?</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">N_SAMPLES</span><span class="token operator">=</span><span class="token number">10000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">y </span><span class="token operator">=</span><span class="token plain"> generate_regression_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">f</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">N_SAMPLES</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)">#Try this with different values of N_SAMPLES like: 10000, 50000, 100000, 1000000...</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">default_device</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">devices</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;gpu&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    X_gpu </span><span class="token operator">=</span><span class="token plain"> jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">device_put</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    y_gpu </span><span class="token operator">=</span><span class="token plain"> jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">device_put</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token operator">%</span><span class="token plain">timeit fit_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X_gpu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y_gpu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> N_SAMPLES</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">block_until_ready</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># CPU: 3.05 s ± 149 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># GPU: 796 ms ± 81.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>So far you have solved a single-variable linear regression problem, that is, one where the matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span> has dimensions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi>x</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">Nx2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal">x</span><span class="mord">2</span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span> is the number of samples and the second column of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span> is filled with 1&#x27;s. As it turns out, the one operation where you use <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span> in <code>fit_model</code> is a matrix multiplication, an operation that GPUs are able to massively parallelize. What do you think will happen if you add more columns to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>? In other words, what will be the difference in performance between GPU an CPU in a <strong>multivariate linear regression problem</strong>?</p>
<p>First you will use a slightly altered function to generate multivariate data (i.e, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span> with more than 2 columns). This time, the coefficients <code>beta</code> of the linear function used to generate the data will be chosen randomly:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">generate_data_multi_regression</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">n_samples</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_variables</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    beta </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">random</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">randint</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">n_variables </span><span class="token operator">+</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f </span><span class="token operator">=</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">lambda</span><span class="token plain"> x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">dot</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">beta</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    X </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">array</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">random</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">rand</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">n_samples</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_variables</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    X </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">c_</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ones</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">shape</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    y </span><span class="token operator">=</span><span class="token plain"> f</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">random</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">normal</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">n_samples</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    X </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">array</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">dtype</span><span class="token operator">=</span><span class="token plain">jnp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    y </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">array</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">dtype</span><span class="token operator">=</span><span class="token plain">jnp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">float32</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">y</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">N_SAMPLES</span><span class="token operator">=</span><span class="token number">10000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">N_VARS</span><span class="token operator">=</span><span class="token number">50</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">y </span><span class="token operator">=</span><span class="token plain"> generate_data_multi_regression</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">N_SAMPLES</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> N_VARS</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token comment" style="color:rgb(98, 114, 164)">#Try this with different values of N_SAMPLES and N_VARS</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">default_device</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">devices</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;cpu&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token operator">%</span><span class="token plain">timeit fit_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> N_SAMPLES</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> learning_rate</span><span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">block_until_ready</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># CPU: 2.13 s ± 91.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">default_device</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">devices</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;gpu&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    X_gpu </span><span class="token operator">=</span><span class="token plain"> jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">device_put</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    y_gpu </span><span class="token operator">=</span><span class="token plain"> jax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">device_put</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token operator">%</span><span class="token plain">timeit fit_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">X_gpu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y_gpu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> N_SAMPLES</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> learning_rate</span><span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">block_until_ready</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># GPU: 970 ms ± 64.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="example-3---deep-learning">Example 3 - Deep Learning<a href="#example-3---deep-learning" class="hash-link" aria-label="Direct link to Example 3 - Deep Learning" title="Direct link to Example 3 - Deep Learning">​</a></h2>
<p>In this final example you will scale up the ideas from the previous example to fit a more complicated model to a dataset. This time you will use <code>PyTorch</code> to fit a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_blank" rel="noopener noreferrer">Convolutional Neural Network</a> to the CIFAR10 dataset - a collection of 60000 images representing 10 different animals and transportation methods.</p>
<p>In the linear regression problem, the model being fit to the data was a linear function, which entailed performing two matrix multiplications per iteration of the least squares method. The convolutional neural network you will work with in this example consists of a sequence of two convolutions, followed by 400 concurrent matrix multiplications, then 120 concurrent matrix multiplications and finally 10 concurrent matrix multiplications. Both convolutions and matrix multiplications are operations that can be parallelized. There are also other operations in between each of these convolution and matrix multiplciation <em>layers</em>, which can all be parallelized:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">nn </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> nn</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">nn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">functional </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> F</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">optim </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> optim</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> torchvision</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">transforms </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> transforms</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> torchvision</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">datasets </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> CIFAR10</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> torch</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">utils</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">data </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> DataLoader</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">torch</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">set_num_threads</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">class</span><span class="token plain"> </span><span class="token class-name">Net</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">nn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Module</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">__init__</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token builtin" style="color:rgb(189, 147, 249)">super</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">Net</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">conv1 </span><span class="token operator">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">6</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">pool </span><span class="token operator">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">MaxPool2d</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">conv2 </span><span class="token operator">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">6</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">16</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">fc1 </span><span class="token operator">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Linear</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">16</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token number">5</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token number">5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">120</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">fc2 </span><span class="token operator">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Linear</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">120</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">84</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">fc3 </span><span class="token operator">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Linear</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">84</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">forward</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        x </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">pool</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">F</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">relu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">conv1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        x </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">pool</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">F</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">relu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">conv2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        x </span><span class="token operator">=</span><span class="token plain"> x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">view</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">16</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token number">5</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token number">5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        x </span><span class="token operator">=</span><span class="token plain"> F</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">relu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">fc1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        x </span><span class="token operator">=</span><span class="token plain"> F</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">relu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">fc2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        x </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">fc3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> x</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">net </span><span class="token operator">=</span><span class="token plain"> Net</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)"># Load model on the GPU</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">criterion </span><span class="token operator">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">CrossEntropyLoss</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)"># Load the loss function on the GPU</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">optimizer </span><span class="token operator">=</span><span class="token plain"> optim</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">SGD</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">net</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">parameters</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> lr</span><span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">transform_train </span><span class="token operator">=</span><span class="token plain"> transforms</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Compose</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">transforms</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ToTensor</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">transforms</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Normalize</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0.5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0.5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0.5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0.5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0.5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">0.5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Purely from the perspective of parallelizing a large number of computations, this looks like something that could benefit from a GPU. But, as you&#x27;ve seen in the previous example, another important part of deciding whether or not this should run on a GPU is the amount of data used in these computations.</p>
<p>In Deep Learning, it is a common practice to use a variant of the gradient descent method called <a href="https://stats.stackexchange.com/questions/488017/understanding-mini-batch-gradient-descent" target="_blank" rel="noopener noreferrer">mini-batch gradient descent</a>. Instead of using the entire matrix of inputs <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span> at each iteration of the algorithm, you use a small subset of the rows of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>. The amount of rows that make up this subset, called the <strong>batch size</strong>, is an important factor that affects whether or not as well as how quickly this approach converges.</p>
<p>Next, you will use mini-batch gradient descent to fit our convolutional neural network to the CIFAR10 data. You will use the code below to load one batch of images at a time from disk, then time the execution of 3 iterations of this training algorithm. Try it out with different combinations of the three parameters below and see what the impact is on execution time and on GPU usage:</p>
<ol>
<li><strong>BATCH_SIZE</strong>: this is the mini-batch gradient descent batch size, i.e., how many images should be loaded from disk and used by the trainig algorith at a time.</li>
<li><strong>NUM_WORKERS</strong>: this controls how many CPUs will be used to load batches of images <em>in parallel</em>. The idea is to use multiple processes, with one CPU each, to streamline loading images from disk <em>without</em> blocking the main process.</li>
<li><strong>PRE_FETCH</strong>: this controls how many batches of images should be loaded from disk <em>before</em> they are needed by the training algorithm. These batches will be stored in the system&#x27;s memory and loaded on the GPU once its their turn to be used.</li>
</ol>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">BATCH_SIZE </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">4096</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">NUM_WORKERS </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">10</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">PRE_FETCH </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">datadir </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;./data&quot;</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)">#f&quot;{os.getenv(&#x27;SLURM_TMPDIR&#x27;)}/data&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">dataset_train </span><span class="token operator">=</span><span class="token plain"> CIFAR10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">root</span><span class="token operator">=</span><span class="token plain">datadir</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> train</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> download</span><span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> transform</span><span class="token operator">=</span><span class="token plain">transform_train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">train_loader </span><span class="token operator">=</span><span class="token plain"> DataLoader</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">dataset_train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> batch_size</span><span class="token operator">=</span><span class="token plain">BATCH_SIZE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> num_workers</span><span class="token operator">=</span><span class="token plain">NUM_WORKERS</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> prefetch_factor</span><span class="token operator">=</span><span class="token plain">PRE_FETCH</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> n_iterations</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> iteration </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">range</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">n_iterations</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> batch_idx</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">inputs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> targets</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            inputs </span><span class="token operator">=</span><span class="token plain"> inputs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            targets </span><span class="token operator">=</span><span class="token plain"> targets</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            outputs </span><span class="token operator">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">inputs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            loss </span><span class="token operator">=</span><span class="token plain"> criterion</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">outputs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> targets</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            optimizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">zero_grad</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            loss</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">backward</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            optimizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">step</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token operator">%</span><span class="token plain">timeit train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">net</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_iterations</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> data</span><span class="token operator">=</span><span class="token builtin" style="color:rgb(189, 147, 249)">enumerate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">train_loader</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">#     2.24 s ± 84.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In the example above, we used the <code>DataLoader</code> object to iretatively read bacthes of images from disk, which means 2 sets of read/write operations: <code>disk -&gt; system memory</code> and <code>system memory -&gt; GPU memory</code>. The <code>disk -&gt; system memory</code> step is prone to causing bottlenecks, as reading from disk is usually much slower than reading from memory. What would the difference in performance be if we instead loaded the whole dataset to the system memory before starting our computations?</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cifar10 </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">batch_idx</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">inputs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">targets</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> batch_idx</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">inputs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> targets</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">enumerate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">train_loader</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token operator">%</span><span class="token plain">timeit train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">net</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_iterations</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> data</span><span class="token operator">=</span><span class="token plain">cifar10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The convolution neural network you used above was, relatively speaking, not that big. If you look at how much GPU memory is used when that model is loaded, you will see it is somewhere between 1 and 2 GB. There is more than enough space left to load the CIFAR10 dataset. Could we have sped-up the training algorithm even more simply by loading all images in GPU memory before calling <code>train()</code>?</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cifar10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token operator">%</span><span class="token plain">timeit train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">net</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_iterations</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> data</span><span class="token operator">=</span><span class="token plain">cifar10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">#     358 ms ± 825 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Now let&#x27;s try this out with a much larger convolutional neural network - the ResNet152 model, which takes up about 20GB in memory and could thus justify keeping the dataset on disk instead of loading it on the GPU:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> torchvision</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">models </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> resnet152</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">torch</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">empty_cache</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">resnet </span><span class="token operator">=</span><span class="token plain"> resnet152</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">criterion </span><span class="token operator">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">CrossEntropyLoss</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)"># Load the loss function on the GPU</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">optimizer </span><span class="token operator">=</span><span class="token plain"> optim</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">SGD</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">net</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">parameters</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> lr</span><span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token operator">%</span><span class="token plain">timeit train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">resnet</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">n_iterations</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">data</span><span class="token operator">=</span><span class="token builtin" style="color:rgb(189, 147, 249)">enumerate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">train_loader</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">#     7.15 s ± 68.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="submitting-jobs">Submitting Jobs<a href="#submitting-jobs" class="hash-link" aria-label="Direct link to Submitting Jobs" title="Direct link to Submitting Jobs">​</a></h2>
<p>So far we&#x27;ve been executing our code examples inside a Jupyter notebook. In real life, we encourage you to do so as a means of verifying that your code is able to use a GPU efficiently. In other words, Jupyter notebooks are a great option to <strong>test your code</strong> and keep an eye on resource usage as it runs.</p>
<p>Using Jupyter notebooks, however, is <strong>not recommended</strong> on the Alliance&#x27;s clusters if you do not need to run your code interactively. If your plan is to run a large workload that will run for a long duration without any intervention on your part, you should <strong>submit a batch job</strong> instead.</p>
<p>Here is an example of a <strong>job submission script</strong> for a job that requires a GPU:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">#!/bin/bash</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH ntasks=1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH gres=gpu:1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH cpus-per-task=4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH mem=8000M</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH account=def-myself_gpu</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">## YOUR JOB&#x27;S COMMANDS GO HERE ##</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>And here is an example of a job that does not requires multiple CPUs, but no GPU:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">#!/bin/bash</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH ntasks=1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH cpus-per-task=16</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH mem=32000M</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH account=def-myself_cpu</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">## YOUR JOB&#x27;S COMMANDS GO HERE ##</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>Important note:</strong> the parameter <code>--mem</code> controls the amount of <strong>system memory</strong> your job is requesting. The amount of GPU memory you will get is fixed, ad depends on the type of GPU allocated to your job:</p>
<p><img loading="lazy" alt="GPU Types" src="/assets/images/gpu_types-e1b4672714ecb8276fb939c0aa7d6158.png" width="1198" height="383" class="img_ev3q"></p>
<p>On Clusters where there&#x27;s more than one type of GPU available, you can specify the one you want by adding the &quot;Slurm type specifier&quot; (third column on the image above) to the parameter <code>--gres=gpu</code>.</p>
<p>For example, to specify that you want one V100 GPU with 32GB of memory on Cedar, the parameter should be set as <code>--gres=gpu:v100l:1</code></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="using-multiple-cpus">Using Multiple CPUs<a href="#using-multiple-cpus" class="hash-link" aria-label="Direct link to Using Multiple CPUs" title="Direct link to Using Multiple CPUs">​</a></h2>
<p>In all code examples in this notebook, you experimented with using multiple CPUs for different purposes. On the first two cases, we compared execution performance on GPU versus an increasing number of CPUs. On the third, we used the <code>num_workers</code> parameter of the <code>DataLoader</code> object to tell our program to use multiple CPUs and multiple processes to load data from disk. In all these cases, we had a variable in our code where we &quot;hardcoded&quot; the desired number of CPUs.</p>
<p><strong>That is not the recommended way to this when submitting jobs.</strong></p>
<p>Instead, you should use a special environment variable that the cluster scheduler creates for you when you submit a job: <code>SLURM_CPUS_PER_TASK</code>. This variable will be set to the same value you pass to the parameter <code>--cpus-per-task</code>.</p>
<p>On the first example, setting the variable <code>OMP_NUM_THREADS</code> would look like this:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">N_CPUS </span><span class="token operator">=</span><span class="token plain"> os</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;SLURM_CPUS_PER_TASK&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">os</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;OMP_NUM_THREADS&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> N_CPUS</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>On the third example, we would limit the number of threads that <code>torch</code> can spawn with:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">N_CPUS </span><span class="token operator">=</span><span class="token plain"> os</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;SLURM_CPUS_PER_TASK&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">torch</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">set_num_threads</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">N_CPUS</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Why is this important?</p>
<p>As it turns out, many widely used Python libraries, including <code>torch</code> and <code>jax</code>, <code>tensorflow</code> will by default try to spawn a number of threads equal to the number of CPUs physically installed on the machine where the code is running. As we&#x27;ve seen above, that number will be somwehere between 32 and 64 depending on the cluster you choose.</p>
<p>That means that even if you set <code>--cpus-per-task=4</code>, for example, these libraries would still try to spawn 32 - 64 threads. This would result in a scenario called <strong>core oversubscription</strong>, or, in other words, too many threads per CPU. This typically causes code execution to <strong>slow down</strong> significantly, sometimes even freezing it completely. It is thus important to have exactly one thread per CPU.</p>
<p>This wraps up our discussion of the best practices when reasoning about using CPUs or a GPU. We hope this material will prove useful to you in designing your workloads in the future!</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="see-also">See also<a href="#see-also" class="hash-link" aria-label="Direct link to See also" title="Direct link to See also">​</a></h2>
<ul>
<li><a href="https://github.com/ComputeCanada/magic_castle" target="_blank" rel="noopener noreferrer">Magic Castle</a> de Compute Canada</li>
<li><a href="https://github.com/calculquebec/" target="_blank" rel="noopener noreferrer">GutHub</a> de Calcul Québec</li>
<li><a href="https://calculquebec.github.io/cip201-serveurs-calcul/2-ressources.html" target="_blank" rel="noopener noreferrer">Bien choisir les ressources</a></li>
<li><a href="https://www.calculquebec.ca/services-aux-chercheurs/formation/" target="_blank" rel="noopener noreferrer">Formations</a> de Calcul Québec</li>
<li><a href="https://calculquebec.eventbrite.ca/" target="_blank" rel="noopener noreferrer">Activités</a> de Calcul Québec sur eventbrite</li>
<li>Portails utilisateur de <a href="https://portail.narval.calculquebec.ca/" target="_blank" rel="noopener noreferrer">Narval</a> et <a href="https://portail.beluga.calculquebec.ca/" target="_blank" rel="noopener noreferrer">Beluga</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format" target="_blank" rel="noopener noreferrer">bfloat16 floating-point format</a></li>
<li><a href="https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/" target="_blank" rel="noopener noreferrer">TensorFloat-32 in the A100 GPU Accelerates AI Training, HPC up to 20x</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/courses/python-gpu/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/courses/fine-tuning-llms"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Training and fine-tuning LLMs</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/models/revdisp"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Disposable Income Calculator</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-a-gpu-and-when-should-i-use-one" class="table-of-contents__link toc-highlight">What is a GPU and when should I use one?</a><ul><li><a href="#cpu" class="table-of-contents__link toc-highlight">CPU</a></li><li><a href="#gpu" class="table-of-contents__link toc-highlight">GPU</a></li><li><a href="#first-set-of-best-practices---when-to-use-a-gpu" class="table-of-contents__link toc-highlight">First set of best practices - when to use a GPU</a></li><li><a href="#pseudo-code-examples" class="table-of-contents__link toc-highlight">Pseudo-Code Examples</a></li></ul></li><li><a href="#gpus-and-data-loading" class="table-of-contents__link toc-highlight">GPUs and Data Loading</a><ul><li><a href="#second-set-of-best-practices---data-loading" class="table-of-contents__link toc-highlight">Second set of best practices - Data Loading</a></li></ul></li><li><a href="#monitoring--profiling---how-do-i-know-if-im-using-a-gpu-correctly" class="table-of-contents__link toc-highlight">Monitoring &amp; Profiling - How do I know if I&#39;m using a GPU correctly?</a><ul><li><a href="#nvidia-smi" class="table-of-contents__link toc-highlight">Nvidia-smi</a></li><li><a href="#nvtop" class="table-of-contents__link toc-highlight">Nvtop</a></li><li><a href="#cluster-portals" class="table-of-contents__link toc-highlight">Cluster Portals</a></li><li><a href="#third-set-of-best-practices---monitoring-and-profiling" class="table-of-contents__link toc-highlight">Third set of best practices - Monitoring and Profiling</a></li></ul></li><li><a href="#when-should-i-use-multiple-cpus-instead-of-a-gpu" class="table-of-contents__link toc-highlight">When should I use multiple CPUs instead of a GPU?</a><ul><li><a href="#narval" class="table-of-contents__link toc-highlight">Narval</a></li><li><a href="#béluga" class="table-of-contents__link toc-highlight">Béluga</a></li><li><a href="#cedar" class="table-of-contents__link toc-highlight">Cedar</a></li><li><a href="#graham" class="table-of-contents__link toc-highlight">Graham</a></li><li><a href="#fourth-set-of-best-practices---use-multiple-cpus-instead-of-a-gpu" class="table-of-contents__link toc-highlight">Fourth set of best practices - Use multiple CPUs instead of a GPU</a></li></ul></li><li><a href="#example-1---solving-linear-systems" class="table-of-contents__link toc-highlight">Example 1 - Solving Linear Systems</a></li><li><a href="#example-2---linear-regression" class="table-of-contents__link toc-highlight">Example 2 - Linear Regression</a></li><li><a href="#example-3---deep-learning" class="table-of-contents__link toc-highlight">Example 3 - Deep Learning</a></li><li><a href="#submitting-jobs" class="table-of-contents__link toc-highlight">Submitting Jobs</a></li><li><a href="#using-multiple-cpus" class="table-of-contents__link toc-highlight">Using Multiple CPUs</a></li><li><a href="#see-also" class="table-of-contents__link toc-highlight">See also</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Alain Boisvert. Construit avec Docusaurus.</div></div></div></footer></div>
</body>
</html>